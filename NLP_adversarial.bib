
@inproceedings{gan_improving_2019,
	address = {Florence, Italy},
	title = {Improving the {Robustness} of {Question} {Answering} {Systems} to {Question} {Paraphrasing}},
	url = {https://www.aclweb.org/anthology/P19-1610},
	doi = {10.18653/v1/P19-1610},
	abstract = {Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models' over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Gan, Wee Chung and Ng, Hwee Tou},
	month = jul,
	year = {2019},
	pages = {6065--6075},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/6I83LU6Z/Gan and Ng - 2019 - Improving the Robustness of Question Answering Sys.pdf:application/pdf},
}

@article{le_malcom_2020,
	title = {{MALCOM}: {Generating} {Malicious} {Comments} to {Attack} {Neural} {Fake} {News} {Detection} {Models}},
	shorttitle = {{MALCOM}},
	url = {http://arxiv.org/abs/2009.01048},
	abstract = {In recent years, the proliferation of so-called "fake news" has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art models to auto-detect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask "what if adversaries attempt to attack such detection models?" and investigate related issues by (i) proposing a novel threat model against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead fake news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94\% and 93.5\% of the time on average MALCOM can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, MALCOM can also fool black box fake news detectors to always output real news labels 90\% of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness.},
	urldate = {2021-04-29},
	journal = {arXiv:2009.01048 [cs, stat]},
	author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.01048},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at the 20th IEEE International Conference on Data Mining (ICDM 2020)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/KUWS2I5U/Le et al. - 2020 - MALCOM Generating Malicious Comments to Attack Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/I72NSGR9/2009.html:text/html},
}

@inproceedings{han_adversarial_2020,
	address = {Online},
	title = {Adversarial {Attack} and {Defense} of {Structured} {Prediction} {Models}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.182},
	doi = {10.18653/v1/2020.emnlp-main.182},
	abstract = {Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Han, Wenjuan and Zhang, Liwen and Jiang, Yong and Tu, Kewei},
	month = nov,
	year = {2020},
	pages = {2327--2338},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/YP2RN4DW/Han et al. - 2020 - Adversarial Attack and Defense of Structured Predi.pdf:application/pdf},
}

@inproceedings{wang_cat-gen_2020,
	address = {Online},
	title = {{CAT}-{Gen}: {Improving} {Robustness} in {NLP} {Models} via {Controlled} {Adversarial} {Text} {Generation}},
	shorttitle = {{CAT}-{Gen}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.417},
	doi = {10.18653/v1/2020.emnlp-main.417},
	abstract = {NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Tianlu and Wang, Xuezhi and Qin, Yao and Packer, Ben and Li, Kang and Chen, Jilin and Beutel, Alex and Chi, Ed},
	month = nov,
	year = {2020},
	pages = {5141--5146},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/UITXTNI7/Wang et al. - 2020 - CAT-Gen Improving Robustness in NLP Models via Co.pdf:application/pdf},
}

@inproceedings{brown_adversarial_2020,
	title = {The {Adversarial} {UFP}/{UFN} {Attack}: {A} {New} {Threat} to {ML}-based {Fake} {News} {Detection} {Systems}?},
	shorttitle = {The {Adversarial} {UFP}/{UFN} {Attack}},
	doi = {10.1109/SSCI47803.2020.9308298},
	abstract = {In this paper, we propose two new attacks: the Adversarial Universal False Positive (UFP) Attack and the Adversarial Universal False Negative (UFN) Attack. The objective of this research is to introduce a new class of attack using only feature vector information. The results show the potential weaknesses of five machine learning (ML) classifiers. These classifiers include k-Nearest Neighbor (KNN), Naive Bayes (NB), Random Forrest (RF), a Support Vector Machine (SVM) with a Radial Basis Function (RBF) Kernel, and XGBoost (XGB).},
	booktitle = {2020 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Brown, Brandon and Richardson, Alexicia and Smith, Marcellus and Dozier, Gerry and King, Michael C.},
	month = dec,
	year = {2020},
	keywords = {Computer science, Fake News Detection Systems, Feature extraction, Radio frequency, Social networking (online), Software engineering, Support vector machines, Universal False Negative, Universal False Positive, Voting},
	pages = {1523--1527},
	file = {IEEE Xplore Abstract Record:/Users/teetusaini/Zotero/storage/I8TRWY5V/9308298.html:text/html},
}

@article{das_automated_2019,
	title = {Automated email {Generation} for {Targeted} {Attacks} using {Natural} {Language}},
	url = {http://arxiv.org/abs/1908.06893},
	abstract = {With an increasing number of malicious attacks, the number of people and organizations falling prey to social engineering attacks is proliferating. Despite considerable research in mitigation systems, attackers continually improve their modus operandi by using sophisticated machine learning, natural language processing techniques with an intent to launch successful targeted attacks aimed at deceiving detection mechanisms as well as the victims. We propose a system for advanced email masquerading attacks using Natural Language Generation (NLG) techniques. Using legitimate as well as an influx of varying malicious content, the proposed deep learning system generates {\textbackslash}textit\{fake\} emails with malicious content, customized depending on the attacker's intent. The system leverages Recurrent Neural Networks (RNNs) for automated text generation. We also focus on the performance of the generated emails in defeating statistical detectors, and compare and analyze the emails using a proposed baseline.},
	urldate = {2021-04-29},
	journal = {arXiv:1908.06893 [cs]},
	author = {Das, Avisha and Verma, Rakesh},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06893},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	annote = {Comment: 8 pages, Workshop on Text Analytics for Cybersecurity and Online Safety 2018 (Co-located with Language Resources and Evaluation Conference 2018)},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/EC7T9S4U/1908.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/8Q6YQYDI/Das and Verma - 2019 - Automated email Generation for Targeted Attacks us.pdf:application/pdf},
}

@article{wang_natural_2020,
	title = {Natural {Language} {Adversarial} {Attacks} and {Defenses} in {Word} {Level}},
	url = {http://arxiv.org/abs/1909.06723},
	abstract = {In recent years, inspired by a mass of researches on adversarial examples for computer vision, there has been a growing interest in designing adversarial attacks for Natural Language Processing (NLP) tasks, followed by very few works of adversarial defenses for NLP. To our knowledge, there exists no defense method against the successful synonym substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to be perceived by humans. We contribute to fill this gap and propose a novel adversarial defense method called {\textbackslash}textit\{Synonym Encoding Method\} (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with little decay on the accuracy for benign examples. To better evaluate SEM, we also design a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic for synonym substitution based attacks. Compared with the first genetic based adversarial attack proposed in 2018, IGA can achieve higher attack success rate with lower word substitution rate, at the same time maintain the transferability of adversarial examples.},
	urldate = {2021-04-29},
	journal = {arXiv:1909.06723 [cs]},
	author = {Wang, Xiaosen and Jin, Hao and He, Kun},
	month = apr,
	year = {2020},
	note = {arXiv: 1909.06723},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 16 pages, 4 figures, 7 tables},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/G2TQYJ57/1909.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/ME2TYR5G/Wang et al. - 2020 - Natural Language Adversarial Attacks and Defenses .pdf:application/pdf},
}

@article{maheshwary_context_2020,
	title = {A {Context} {Aware} {Approach} for {Generating} {Natural} {Language} {Attacks}},
	url = {http://arxiv.org/abs/2012.13339},
	abstract = {We study an important task of attacking natural language processing models in a black box setting. We propose an attack strategy that crafts semantically similar adversarial examples on text classification and entailment tasks. Our proposed attack finds candidate words by considering the information of both the original word and its surrounding context. It jointly leverages masked language modelling and next sentence prediction for context understanding. In comparison to attacks proposed in prior literature, we are able to generate high quality adversarial examples that do significantly better both in terms of success rate and word perturbation percentage.},
	urldate = {2021-04-29},
	journal = {arXiv:2012.13339 [cs]},
	author = {Maheshwary, Rishabh and Maheshwary, Saket and Pudi, Vikram},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.13339},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted as Student Poster at AAAI 2021},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/NCMZS4MN/2012.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/BF7VYMQQ/Maheshwary et al. - 2020 - A Context Aware Approach for Generating Natural La.pdf:application/pdf},
}

@article{jin_is_2020,
	title = {Is {BERT} {Really} {Robust}? {A} {Strong} {Baseline} for {Natural} {Language} {Attack} on {Text} {Classification} and {Entailment}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Is {BERT} {Really} {Robust}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
	doi = {10.1609/aaai.v34i05.6311},
	language = {en},
	number = {05},
	urldate = {2021-04-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {8018--8025},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/5V8MIJXJ/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/K4TFYJ5B/6311.html:text/html},
}

@inproceedings{li_universal_2019,
	title = {Universal {Rules} for {Fooling} {Deep} {Neural} {Networks} based {Text} {Classification}},
	doi = {10.1109/CEC.2019.8790213},
	abstract = {Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses.},
	booktitle = {2019 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Li, Di and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = jun,
	year = {2019},
	keywords = {Adversarial machine learning, Deep learning, Natural language processing, Natural Language processing, Observers, Optimization, Perturbation methods, Text misclassification, Training},
	pages = {2221--2228},
	file = {IEEE Xplore Abstract Record:/Users/teetusaini/Zotero/storage/Z2EQYYMU/8790213.html:text/html;Submitted Version:/Users/teetusaini/Zotero/storage/Z59DVF25/Li et al. - 2019 - Universal Rules for Fooling Deep Neural Networks b.pdf:application/pdf},
}

@article{belinkov_analysis_2019,
	title = {Analysis {Methods} in {Neural} {Language} {Processing}: {A} {Survey}},
	volume = {7},
	shorttitle = {Analysis {Methods} in {Neural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/Q19-1004},
	doi = {10.1162/tacl_a_00254},
	abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
	urldate = {2021-04-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Glass, James},
	month = mar,
	year = {2019},
	pages = {49--72},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/BFYG2X5X/Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf:application/pdf},
}

@misc{noauthor_thunlpopenattack_2021,
	title = {thunlp/{OpenAttack}},
	copyright = {MIT License         ,                 MIT License},
	url = {https://github.com/thunlp/OpenAttack},
	abstract = {An Open-Source Package for Textual Adversarial Attack.},
	urldate = {2021-04-29},
	publisher = {THUNLP},
	month = apr,
	year = {2021},
	note = {original-date: 2020-02-29T09:42:52Z},
	keywords = {adversarial-attacks, adversarial-example, nlp},
}

@article{roth_token-modification_2021,
	title = {Token-{Modification} {Adversarial} {Attacks} for {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Token-{Modification} {Adversarial} {Attacks} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2103.00676},
	abstract = {There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a {\textbackslash}textit\{token-modification\} attack. Each token-modification attack is defined by a specific combination of fundamental {\textbackslash}textit\{components\}, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. We hope this survey will guide new researchers to this field and spark further research into the individual attack components.},
	urldate = {2021-04-29},
	journal = {arXiv:2103.00676 [cs]},
	author = {Roth, Tom and Gao, Yansong and Abuadbba, Alsharif and Nepal, Surya and Liu, Wei},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00676},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 8 pages, 1 figure},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/2VXCSR8E/2103.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/LZ6QTFH4/Roth et al. - 2021 - Token-Modification Adversarial Attacks for Natural.pdf:application/pdf},
}

@inproceedings{magassouba_multimodal_2020,
	title = {Multimodal {Attention} {Branch} {Network} for {Perspective}-{Free} {Sentence} {Generation}},
	url = {http://proceedings.mlr.press/v100/magassouba20a.html},
	abstract = {In this paper, we address the automatic sentence generation of fetching instructions for domestic service robots. Typical fetching commands such as “bring me the yellow toy from the upper part of t...},
	language = {en},
	urldate = {2021-04-17},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Magassouba, Aly and Sugiura, Komei and Kawai, Hisashi},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {76--85},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/UM3MX3K4/magassouba20a.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/QIX5A9NR/Magassouba et al. - 2020 - Multimodal Attention Branch Network for Perspectiv.pdf:application/pdf},
}

@inproceedings{fukui_attention_2019,
	title = {Attention {Branch} {Network}: {Learning} of {Attention} {Mechanism} for {Visual} {Explanation}},
	shorttitle = {Attention {Branch} {Network}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.html},
	urldate = {2021-04-17},
	author = {Fukui, Hiroshi and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	year = {2019},
	pages = {10705--10714},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/ZXM4DJXT/Fukui et al. - 2019 - Attention Branch Network Learning of Attention Me.pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/SWA4E28K/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019.html:text/html},
}

@article{chen_robustness_2019,
	title = {Robustness {Verification} of {Tree}-based {Models}},
	url = {http://arxiv.org/abs/1906.03849},
	abstract = {We study the robustness verification problem for tree-based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles, the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on the robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. OnRF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than the previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.},
	urldate = {2021-04-17},
	journal = {arXiv:1906.03849 [cs, stat]},
	author = {Chen, Hongge and Zhang, Huan and Si, Si and Li, Yang and Boning, Duane and Hsieh, Cho-Jui},
	month = dec,
	year = {2019},
	note = {arXiv: 1906.03849},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Hongge Chen and Huan Zhang contributed equally},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/UZS4KWBJ/1906.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/RR25BMEX/Chen et al. - 2019 - Robustness Verification of Tree-based Models.pdf:application/pdf},
}

@article{guan_robustness_2020,
	title = {Robustness {Verification} of {Quantum} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2008.07230},
	abstract = {Several important models of machine learning algorithms have been successfully generalized to the quantum world, with potential applications to data analytics in quantum physics that can be implemented on the near future quantum computers. However, noise and decoherence are two major obstacles to the practical implementation of quantum machine learning. In this work, we introduce a general framework for the robustness analysis of quantum machine learning algorithms against noise and decoherence. We argue that fidelity is the only pick of measuring the robustness. A robust bound is derived and an algorithm is developed to check whether or not a quantum machine learning algorithm is robust with respect to the training data. In particular, this algorithm can help to defense attacks and improve the accuracy as it can identify useful new training data during checking. The effectiveness of our robust bound and algorithm is confirmed by the case study of quantum phase recognition. Furthermore, this experiment demonstrates a trade-off between the accuracy of quantum machine learning algorithms and their robustness.},
	urldate = {2021-04-17},
	journal = {arXiv:2008.07230 [quant-ph]},
	author = {Guan, Ji and Fang, Wang and Ying, Mingsheng},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07230},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/AJISHQKH/2008.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/NTUGYAB9/Guan et al. - 2020 - Robustness Verification of Quantum Machine Learnin.pdf:application/pdf},
}

@article{nicolae_adversarial_2019,
	title = {Adversarial {Robustness} {Toolbox} v1.0.0},
	url = {http://arxiv.org/abs/1807.01069},
	abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
	urldate = {2021-04-17},
	journal = {arXiv:1807.01069 [cs, stat]},
	author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
	month = nov,
	year = {2019},
	note = {arXiv: 1807.01069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 34 pages},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/I9FMDFWC/1807.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/2RZSIXT4/Nicolae et al. - 2019 - Adversarial Robustness Toolbox v1.0.0.pdf:application/pdf},
}

@article{hitaj_capture_2020,
	title = {Capture the {Bot}: {Using} {Adversarial} {Examples} to {Improve} {CAPTCHA} {Robustness} to {Bot} {Attacks}},
	shorttitle = {Capture the {Bot}},
	url = {http://arxiv.org/abs/2010.16204},
	abstract = {To this date, CAPTCHAs have served as the first line of defense preventing unauthorized access by (malicious) bots to web-based services, while at the same time maintaining a trouble-free experience for human visitors. However, recent work in the literature has provided evidence of sophisticated bots that make use of advancements in machine learning (ML) to easily bypass existing CAPTCHA-based defenses. In this work, we take the first step to address this problem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial examples. While typically adversarial examples are used to lead an ML model astray, with CAPTURE, we attempt to make a "good use" of such mechanisms. Our empirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to solve by humans while at the same time, effectively thwarting ML-based bot solvers.},
	urldate = {2021-04-16},
	journal = {arXiv:2010.16204 [cs]},
	author = {Hitaj, Dorjan and Hitaj, Briland and Jajodia, Sushil and Mancini, Luigi V.},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.16204},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 17 pages, 4 figures. Accepted for publication on IEEE Intelligent Systems magazine},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/JSX8YKM9/2010.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/FH2XDY22/Hitaj et al. - 2020 - Capture the Bot Using Adversarial Examples to Imp.pdf:application/pdf},
}

@article{xiong_towards_2021,
	title = {Towards a {Robust} and {Trustworthy} {Machine} {Learning} {System} {Development}},
	url = {http://arxiv.org/abs/2101.03042},
	abstract = {Machine Learning (ML) technologies have been widely adopted in many mission critical fields, such as cyber security, autonomous vehicle control, healthcare, etc. to support intelligent decision-making. While ML has demonstrated impressive performance over conventional methods in these applications, concerns arose with respect to system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, firstly we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness technologies from a security engineering perspective, which covers all aspects of secure ML system development including threat modeling, common offensive and defensive technologies, privacy-preserving machine learning, user trust in the context of machine learning, and empirical evaluation for ML model robustness. Secondly, we then push our studies forward above and beyond a survey by describing a metamodel we created that represents the body of knowledge in a standard and visualized way for ML practitioners. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process in a context of generic ML system development, which extends and scales up the classic process. Thirdly, we propose future research directions motivated by our findings to advance the development of robust and trustworthy ML systems. Our work differs from existing surveys in this area in that, to the best of our knowledge, it is the first of its kind of engineering effort to (i) explore the fundamental principles and best practices to support robust and trustworthy ML system development; and (ii) study the interplay of robustness and user trust in the context of ML systems.},
	urldate = {2021-04-16},
	journal = {arXiv:2101.03042 [cs]},
	author = {Xiong, Pulei and Buffett, Scott and Iqbal, Shahrear and Lamontagne, Philippe and Mamun, Mohammad and Molyneaux, Heather},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.03042},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, D.2, H.1, I.2},
	annote = {Comment: 40 pages, 7 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/NMGBQLW3/2101.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/I9F999K6/Xiong et al. - 2021 - Towards a Robust and Trustworthy Machine Learning .pdf:application/pdf},
}

@article{henriksson_performance_2021,
	title = {Performance {Analysis} of {Out}-of-{Distribution} {Detection} on {Various} {Trained} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2103.15580},
	abstract = {Several areas have been improved with Deep Learning during the past years. For non-safety related products adoption of AI and ML is not an issue, whereas in safety critical applications, robustness of such approaches is still an issue. A common challenge for Deep Neural Networks (DNN) occur when exposed to out-of-distribution samples that are previously unseen, where DNNs can yield high confidence predictions despite no prior knowledge of the input. In this paper we analyse two supervisors on two well-known DNNs with varied setups of training and find that the outlier detection performance improves with the quality of the training procedure. We analyse the performance of the supervisor after each epoch during the training cycle, to investigate supervisor performance as the accuracy converges. Understanding the relationship between training results and supervisor performance is valuable to improve robustness of the model and indicates where more work has to be done to create generalized models for safety critical applications.},
	urldate = {2021-04-16},
	journal = {arXiv:2103.15580 [cs]},
	author = {Henriksson, Jens and Berger, Christian and Borg, Markus and Tornberg, Lars and Sathyamoorthy, Sankar Raman and Englund, Cristofer},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15580},
	keywords = {Computer Science - Machine Learning, D.2.5},
	annote = {Comment: 8 pages, 7 figures, presented at SEAA 2019},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/2WGLHH3J/2103.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/VNFVQYTU/Henriksson et al. - 2021 - Performance Analysis of Out-of-Distribution Detect.pdf:application/pdf},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {TextBugger}: {Generating} {Adversarial} {Text} {Against} {Real}-world {Applications} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/TextBugger%3A-Generating-Adversarial-Text-Against-Li-Ji/1e7ea2465753c5186e5fcc9e1a64cdc522cf4de4},
	urldate = {2021-04-15},
	file = {[PDF] TextBugger\: Generating Adversarial Text Against Real-world Applications | Semantic Scholar:/Users/teetusaini/Zotero/storage/QQ524299/1e7ea2465753c5186e5fcc9e1a64cdc522cf4de4.html:text/html},
}

@article{wahle_identifying_2021,
	title = {Identifying {Machine}-{Paraphrased} {Plagiarism}},
	abstract = {Employing paraphrasing tools to conceal plagiarized text is a severe threat to academic integrity. To enable the detection of machine-paraphrased text, we evaluate the effectiveness of five pre-trained word embedding models combined with machine learning classifiers and state-of-the-art neural language models. We analyze preprints of research papers, graduation theses, and Wikipedia articles, which we paraphrased using different configurations of the tools SpinBot and SpinnerChief. The best performing technique, Longformer, achieved an average F1 score of 80.99\% (F1=99.68\% for SpinBot and F1=71.64\% for SpinnerChief cases), while human evaluators achieved F1=78.4\% for SpinBot and F1=65.6\% for SpinnerChief cases. We show that the automated classification alleviates shortcomings of widelyused text-matching systems, such as Turnitin and PlagScan. To facilitate future research, all data, code, and two web applications showcasing our contributions are openly available.},
	journal = {ArXiv},
	author = {Wahle, Jan Philip and Ruas, Terry and Folt'ynek, Tom'avs and Meuschke, Norman and Gipp, Bela},
	year = {2021},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/69SQWXCC/Wahle et al. - 2021 - Identifying Machine-Paraphrased Plagiarism.pdf:application/pdf},
}

@inproceedings{merono-penuela_can_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Can a {Transformer} {Assist} in {Scientific} {Writing}? {Generating} {Semantic} {Web} {Paper} {Snippets} with {GPT}-2},
	isbn = {978-3-030-62327-2},
	shorttitle = {Can a {Transformer} {Assist} in {Scientific} {Writing}?},
	doi = {10.1007/978-3-030-62327-2_27},
	abstract = {The Semantic Web community has produced a large body of literature that is becoming increasingly difficult to manage, browse, and use. Recent work on attention-based, sequence-to-sequence Transformer neural architecture has produced language models that generate surprisingly convincing synthetic conditional text samples. In this demonstration, we re-train the GPT-2 architecture using the complete corpus of proceedings of the International Semantic Web Conference since 2002 until 2019. We use user-provided sentences to conditionally sample paper snippets, therefore illustrating cases where this model can help at addressing challenges in scientific paper writing, such as navigating extensive literature, explaining the Semantic Web core concepts, providing definitions, and even inspiring new research ideas.},
	language = {en},
	booktitle = {The {Semantic} {Web}: {ESWC} 2020 {Satellite} {Events}},
	publisher = {Springer International Publishing},
	author = {Meroño-Peñuela, Albert and Spagnuelo, Dayana},
	editor = {Harth, Andreas and Presutti, Valentina and Troncy, Raphaël and Acosta, Maribel and Polleres, Axel and Fernández, Javier D. and Xavier Parreira, Josiane and Hartig, Olaf and Hose, Katja and Cochez, Michael},
	year = {2020},
	keywords = {Natural language generation, Scholarly communication, Semantic Web papers},
	pages = {158--163},
	file = {Springer Full Text PDF:/Users/teetusaini/Zotero/storage/E8RM23G3/Meroño-Peñuela and Spagnuelo - 2020 - Can a Transformer Assist in Scientific Writing Ge.pdf:application/pdf},
}

@inproceedings{croce_provable_2019,
	title = {Provable {Robustness} of {ReLU} networks via {Maximization} of {Linear} {Regions}},
	url = {http://proceedings.mlr.press/v89/croce19a.html},
	abstract = {It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networ...},
	language = {en},
	urldate = {2021-04-12},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2057--2066},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/7AKZ2WRE/Croce et al. - 2019 - Provable Robustness of ReLU networks via Maximizat.pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/UBABYME2/croce19a.html:text/html},
}

@article{madry_towards_2019,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	urldate = {2021-04-12},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = sep,
	year = {2019},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICLR'18},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/KRI98EUP/1706.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/A7NTWK8C/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf},
}

@article{raghunathan_certified_2020,
	title = {Certified {Defenses} against {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1801.09344},
	abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most {\textbackslash}epsilon = 0.1 can cause more than 35\% test error.},
	urldate = {2021-04-12},
	journal = {arXiv:1801.09344 [cs]},
	author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
	month = oct,
	year = {2020},
	note = {arXiv: 1801.09344},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published at the International Conference on Learning Representations (ICLR) 2018},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/FQIHBKUX/1801.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/AGMPMEIJ/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf:application/pdf},
}

@inproceedings{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Network}},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularis...},
	language = {en},
	urldate = {2021-04-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1613--1622},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/SA5AQ6CJ/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf:application/pdf},
}

@article{sensoy_evidential_2018,
	title = {Evidential {Deep} {Learning} to {Quantify} {Classification} {Uncertainty}},
	url = {http://arxiv.org/abs/1806.01768},
	abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.},
	urldate = {2021-04-12},
	journal = {arXiv:1806.01768 [cs, stat]},
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01768},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/TDDLKP6D/1806.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/N96Y7WG5/Sensoy et al. - 2018 - Evidential Deep Learning to Quantify Classificatio.pdf:application/pdf},
}

@article{ovadia_can_2019,
	title = {Can {You} {Trust} {Your} {Model}'s {Uncertainty}? {Evaluating} {Predictive} {Uncertainty} {Under} {Dataset} {Shift}},
	shorttitle = {Can {You} {Trust} {Your} {Model}'s {Uncertainty}?},
	url = {http://arxiv.org/abs/1906.02530},
	abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive \{{\textbackslash}em uncertainty\}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
	urldate = {2021-04-12},
	journal = {arXiv:1906.02530 [cs, stat]},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
	month = dec,
	year = {2019},
	note = {arXiv: 1906.02530},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Advances in Neural Information Processing Systems, 2019},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/5F3LB7TG/1906.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/Y239PSZ3/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf:application/pdf},
}

@article{luo_local_2021,
	title = {Local {Interpretations} for {Explainable} {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Local {Interpretations} for {Explainable} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2103.11072},
	abstract = {As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term {\textbackslash}textit\{interpretability\} and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model's predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.},
	urldate = {2021-04-12},
	journal = {arXiv:2103.11072 [cs]},
	author = {Luo, Siwen and Ivison, Hamish and Han, Caren and Poon, Josiah},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.11072},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: This work is an initial draft},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/QZ4DYA4V/2103.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/9PRNLNNM/Luo et al. - 2021 - Local Interpretations for Explainable Natural Lang.pdf:application/pdf},
}

@article{ayoub_combat_2021,
	title = {Combat {COVID}-19 infodemic using explainable natural language processing models},
	volume = {58},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457321000704},
	doi = {10.1016/j.ipm.2021.102569},
	abstract = {Misinformation of COVID-19 is prevalent on social media as the pandemic unfolds, and the associated risks are extremely high. Thus, it is critical to detect and combat such misinformation. Recently, deep learning models using natural language processing techniques, such as BERT (Bidirectional Encoder Representations from Transformers), have achieved great successes in detecting misinformation. In this paper, we proposed an explainable natural language processing model based on DistilBERT and SHAP (Shapley Additive exPlanations) to combat misinformation about COVID-19 due to their efficiency and effectiveness. First, we collected a dataset of 984 claims about COVID-19 with fact-checking. By augmenting the data using back-translation, we doubled the sample size of the dataset and the DistilBERT model was able to obtain good performance (accuracy: 0.972; areas under the curve: 0.993) in detecting misinformation about COVID-19. Our model was also tested on a larger dataset for AAAI2021 — COVID-19 Fake News Detection Shared Task and obtained good performance (accuracy: 0.938; areas under the curve: 0.985). The performance on both datasets was better than traditional machine learning models. Second, in order to boost public trust in model prediction, we employed SHAP to improve model explainability, which was further evaluated using a between-subjects experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE), and text+SHAP explanation+source and evidence (TSESE). The participants were significantly more likely to trust and share information related to COVID-19 in the TSE and TSESE conditions than in the T condition. Our results provided good implications for detecting misinformation about COVID-19 and improving public trust.},
	language = {en},
	number = {4},
	urldate = {2021-04-12},
	journal = {Information Processing \& Management},
	author = {Ayoub, Jackie and Yang, X. Jessie and Zhou, Feng},
	month = jul,
	year = {2021},
	keywords = {BERT, COVID-19, DistilBERT, Misinformation detection, SHAP, Trust},
	pages = {102569},
	file = {ScienceDirect Snapshot:/Users/teetusaini/Zotero/storage/86GIHKPN/S0306457321000704.html:text/html;ScienceDirect Full Text PDF:/Users/teetusaini/Zotero/storage/EG6WKN76/Ayoub et al. - 2021 - Combat COVID-19 infodemic using explainable natura.pdf:application/pdf},
}

@article{danilevsky_survey_2020,
	title = {A {Survey} of the {State} of {Explainable} {AI} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2010.00711},
	abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
	urldate = {2021-04-12},
	journal = {arXiv:2010.00711 [cs]},
	author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.00711},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.7},
	annote = {Comment: To appear in AACL-IJCNLP 2020},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/JA76ZWKN/2010.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/GCTX2IHE/Danilevsky et al. - 2020 - A Survey of the State of Explainable AI for Natura.pdf:application/pdf},
}

@article{wu_novel_2021,
	title = {A novel framework for detecting social bots with deep neural networks and active learning},
	doi = {10.1016/j.knosys.2020.106525},
	abstract = {Abstract Microblogging is a popular online social network (OSN), which facilitates users to obtain and share news and information. Nevertheless, it is filled with a huge number of social bots that significantly disrupt the normal order of OSNs. Sina Weibo, one of the most popular Chinese OSNs in the world, is also seriously affected by social bots. With the growing development of social bots in Sina Weibo, they are increasingly indistinguishable from normal users, which presents more huge challenges in detecting social bots. Firstly, it is difficult to extract the features of social bots completely. Secondly, large-scale data collection and labeling of user data are extremely hard. Thirdly, the performance of classical classification approaches applied to social bot detection is not good enough. Therefore, this paper proposes a novel framework for detecting social bots in Sina Weibo based on deep neural networks and active learning (DABot). Specifically, 30 features from four categories, namely metadata-based, interaction-based, content-based, and timing-based are extracted to distinguish between social bots and normal users. Nine of these features are completely new features proposed in this paper. Moreover, active learning is employed to efficiently expand the labeled data. Then, a new deep neural network model called RGA is built to implement the detection of social bots, which makes use of a residual network (ResNet), a bidirectional gated recurrent unit (BiGRU), and an attention mechanism. After performance evaluation, the results show that DABot is more effective than the state-of-the-art baselines with the accuracy of 0.9887.},
	journal = {Knowl. Based Syst.},
	author = {Wu, Yuhao and Fang, Yuzhou and Shang, Shuaikang and Jin, Jing and Wei, Lai and Wang, Haizhou},
	year = {2021},
}

@article{wahle_are_2021,
	title = {Are {Neural} {Language} {Models} {Good} {Plagiarists}? {A} {Benchmark} for {Neural} {Paraphrase} {Detection}},
	shorttitle = {Are {Neural} {Language} {Models} {Good} {Plagiarists}?},
	url = {http://arxiv.org/abs/2103.12450},
	abstract = {The rise of language models such as BERT allows for high-quality text paraphrasing. This is a problem to academic integrity, as it is difficult to differentiate between original and machine-generated content. We propose a benchmark consisting of paraphrased articles using recent language models relying on the Transformer architecture. Our contribution fosters future research of paraphrase detection systems as it offers a large collection of aligned original and paraphrased documents, a study regarding its structure, classification experiments with state-of-the-art systems, and we make our findings publicly available.},
	urldate = {2021-04-10},
	journal = {arXiv:2103.12450 [cs]},
	author = {Wahle, Jan Philip and Ruas, Terry and Meuschke, Norman and Gipp, Bela},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.12450},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Digital Libraries},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/4RLI7B3D/2103.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/C7UWFU37/Wahle et al. - 2021 - Are Neural Language Models Good Plagiarists A Ben.pdf:application/pdf},
}

@article{harrag_bert_2021,
	title = {{BERT} {Transformer} model for {Detecting} {Arabic} {GPT2} {Auto}-{Generated} {Tweets}},
	url = {http://arxiv.org/abs/2101.09345},
	abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
	urldate = {2021-04-10},
	journal = {arXiv:2101.09345 [cs]},
	author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09345},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/S8PQI464/2101.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/LRM5P6TE/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf:application/pdf},
}

@misc{noauthor_maifshapash_2021,
	title = {{MAIF}/shapash},
	copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/MAIF/shapash},
	abstract = {Shapash makes Machine Learning models transparent and understandable by everyone},
	urldate = {2021-04-07},
	publisher = {MAIF},
	month = apr,
	year = {2021},
	note = {original-date: 2020-04-29T07:34:23Z},
	keywords = {ethical-artificial-intelligence, explainability, explainable-ml, lime, machine-learning, python, shap, transparency},
}

@article{morris_textattack_2020,
	title = {{TextAttack}: {A} {Framework} for {Adversarial} {Attacks}, {Data} {Augmentation}, and {Adversarial} {Training} in {NLP}},
	shorttitle = {{TextAttack}},
	url = {http://arxiv.org/abs/2005.05909},
	abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
	urldate = {2021-04-06},
	journal = {arXiv:2005.05909 [cs]},
	author = {Morris, John X. and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.05909},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 6 pages. More details are shared at https://github.com/QData/TextAttack},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/IJBNNZEH/2005.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/2LDHB8U7/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf:application/pdf},
}

@article{narang_deepnetdevanagari_2021,
	title = {{DeepNetDevanagari}: a deep learning model for {Devanagari} ancient character recognition},
	issn = {1573-7721},
	shorttitle = {{DeepNetDevanagari}},
	url = {https://doi.org/10.1007/s11042-021-10775-6},
	doi = {10.1007/s11042-021-10775-6},
	abstract = {Devanagari script is the most widely used script in India and other Asian countries. There is a rich collection of ancient Devanagari manuscripts, which is a wealth of knowledge. To make these manuscripts available to people, efforts are being done to digitize these documents. Optical Character Recognition (OCR) plays an important role in recognizing these documents. Convolutional Neural Network (CNN) is a powerful model that is giving very promising results in the field of character recognition, pattern recognition etc. CNN has never been used for the recognition of the Devanagari ancient manuscripts. Our aim in the proposed work is to use the power of CNN for extracting the wealth of knowledge from Devanagari handwritten ancient manuscripts. In addition, we aim is to experiment with various design options like number of layes, stride size, number of filters, kenel size and different functions in various layers and to select the best of these. In this paper, the authors have proposed to use deep learning model as a feature extractor as well as a classifier for the recognition of 33 classes of basic characters of Devanagari ancient manuscripts. A dataset containing 5484 characters has been used for the experimental work. Various experiments show that the accuracy achieved using CNN as a feature extractor is better than other state-of-the-art techniques. The recognition accuracy of 93.73\% has been achieved by using the model proposed in this paper for Devanagari ancient character recognition.},
	language = {en},
	urldate = {2021-04-05},
	journal = {Multimedia Tools and Applications},
	author = {Narang, Sonika Rani and Kumar, Munish and Jindal, M. K.},
	month = mar,
	year = {2021},
	file = {Springer Full Text PDF:/Users/teetusaini/Zotero/storage/BEXTT63Z/Narang et al. - 2021 - DeepNetDevanagari a deep learning model for Devan.pdf:application/pdf},
}

@misc{noauthor_connected_nodate,
	title = {Connected {Papers} {\textbar} {Find} and explore academic papers},
	url = {https://www.connectedpapers.com/main/32c8884a95101ae9b854399b69284b28dd062fce/Machine-Learning-Threatens-5G-Security/prior},
	abstract = {A unique, visual tool to help researchers and applied scientists find and explore papers relevant to their field of work.},
	language = {en},
	urldate = {2021-04-04},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/3XHAAWEH/prior.html:text/html},
}

@misc{paul_very_nodate,
	title = {A very tiny alteration can help deepfakes escape detection},
	url = {https://techxplore.com/news/2020-10-tiny-deepfakes.html},
	abstract = {Last month, Sophie Wilmès, the prime minister of Belgium, appeared in an online video to tell her audience that the COVID-19 pandemic was linked to the "exploitation and destruction by humans of our natural environment." Whether or not these two existential crises are connected, the fact is that Wilmès said no such thing. Produced by an organization of climate change activists, the video was actually a deepfake, or a form of fake media created using deep learning. Deepfakes are yet another way to spread misinformation—as if there wasn't enough fake news about the pandemic already.},
	language = {en},
	urldate = {2021-04-04},
	author = {Paul, Ben and California, University of Southern},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/WCBUKGF7/2020-10-tiny-deepfakes.html:text/html},
}

@article{harrag_bert_2021-1,
	title = {{BERT} {Transformer} model for {Detecting} {Arabic} {GPT2} {Auto}-{Generated} {Tweets}},
	url = {http://arxiv.org/abs/2101.09345},
	abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
	urldate = {2021-04-04},
	journal = {arXiv:2101.09345 [cs]},
	author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09345},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/GIY6CWB7/2101.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/H89SQSFV/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf:application/pdf},
}

@inproceedings{li_bert-attack_2020,
	title = {{BERT}-{ATTACK}: {Adversarial} {Attack} {Against} {BERT} {Using} {BERT}},
	shorttitle = {{BERT}-{ATTACK}},
	doi = {10.18653/v1/2020.emnlp-main.500},
	abstract = {Adversarial attacks for discrete data (such as text) has been proved significantly more challenging than continuous data (such as image), since it is difficult to generate adversarial samples with gradient-based methods. Currently, the successful attack methods for text usually adopt heuristic replacement strategies on character or word level, which remains challenging to find the optimal solution in the massive space of possible combination of replacements, while preserving semantic consistency and language fluency. In this paper, we propose {\textbackslash}textbf\{BERT-Attack\}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models for downstream tasks. Our method successfully misleads the target models to predict incorrectly, outperforming state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations.},
	booktitle = {{EMNLP}},
	author = {Li, L. and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
	year = {2020},
	file = {Full Text:/Users/teetusaini/Zotero/storage/H7LCX3UC/Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf:application/pdf},
}

@article{dong_towards_2021,
	title = {{TOWARDS} {ROBUSTNESS} {AGAINST} {NATURAL} {LANGUAGE} {WORD} {SUBSTITUTIONS}},
	language = {en},
	author = {Dong, Xinshuai and Luu, Anh Tuan and Ji, Rongrong and Liu, Hong},
	year = {2021},
	pages = {14},
	file = {Dong et al. - 2021 - TOWARDS ROBUSTNESS AGAINST NATURAL LANGUAGE WORD S.pdf:/Users/teetusaini/Zotero/storage/G53HXVLC/Dong et al. - 2021 - TOWARDS ROBUSTNESS AGAINST NATURAL LANGUAGE WORD S.pdf:application/pdf},
}

@article{lee-thorp_fnet_2021,
	title = {{FNet}: {Mixing} {Tokens} with {Fourier} {Transforms}},
	shorttitle = {{FNet}},
	url = {https://arxiv.org/abs/2105.03824v1},
	abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate "efficient" Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
	language = {en},
	urldate = {2021-05-22},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	month = may,
	year = {2021},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/EBHWQTHL/2105.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/UZB33DL3/Lee-Thorp et al. - 2021 - FNet Mixing Tokens with Fourier Transforms.pdf:application/pdf},
}

@article{harrag_bert_2021-2,
	title = {{BERT} {Transformer} model for {Detecting} {Arabic} {GPT2} {Auto}-{Generated} {Tweets}},
	url = {http://arxiv.org/abs/2101.09345},
	abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
	urldate = {2021-05-20},
	journal = {arXiv:2101.09345 [cs]},
	author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09345},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/KKLSASUP/2101.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/D3IAG7C6/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf:application/pdf},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	pages = {12},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/teetusaini/Zotero/storage/E9B7IXYG/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2021-05-19},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = mar,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/3Y7BLYWL/1802.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/43V945MF/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf},
}

@article{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2021-05-19},
	journal = {arXiv:1801.06146 [cs, stat]},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = may,
	year = {2018},
	note = {arXiv: 1801.06146},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ACL 2018, fixed denominator in Equation 3, line 3},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/7AK3LRJS/1801.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/4FZGA7LD/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf:application/pdf},
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2021-05-18},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/P8DMYQT5/1409.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MYZ7NFA6/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@article{vaswani_tensor2tensor_2018,
	title = {{Tensor2Tensor} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1803.07416},
	abstract = {Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.},
	urldate = {2021-05-18},
	journal = {arXiv:1803.07416 [cs, stat]},
	author = {Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N. and Gouws, Stephan and Jones, Llion and Kaiser, Łukasz and Kalchbrenner, Nal and Parmar, Niki and Sepassi, Ryan and Shazeer, Noam and Uszkoreit, Jakob},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.07416},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1706.03762},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/ADUJV7D5/1803.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/JB9IUDTR/Vaswani et al. - 2018 - Tensor2Tensor for Neural Machine Translation.pdf:application/pdf},
}

@article{wang_towards_2021,
	title = {Towards a {Robust} {Deep} {Neural} {Network} in {Texts}: {A} {Survey}},
	shorttitle = {Towards a {Robust} {Deep} {Neural} {Network} in {Texts}},
	url = {http://arxiv.org/abs/1902.07285},
	abstract = {Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques. We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.},
	urldate = {2021-05-18},
	journal = {arXiv:1902.07285 [cs]},
	author = {Wang, Wenqi and Wang, Run and Wang, Lina and Wang, Zhibo and Ye, Aoshuang},
	month = apr,
	year = {2021},
	note = {arXiv: 1902.07285},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/9B5382I5/1902.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/CQ727D7D/Wang et al. - 2021 - Towards a Robust Deep Neural Network in Texts A S.pdf:application/pdf},
}

@article{wang_natural_2020-1,
	title = {Natural {Language} {Adversarial} {Attacks} and {Defenses} in {Word} {Level}},
	url = {http://arxiv.org/abs/1909.06723},
	abstract = {In recent years, inspired by a mass of researches on adversarial examples for computer vision, there has been a growing interest in designing adversarial attacks for Natural Language Processing (NLP) tasks, followed by very few works of adversarial defenses for NLP. To our knowledge, there exists no defense method against the successful synonym substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to be perceived by humans. We contribute to fill this gap and propose a novel adversarial defense method called {\textbackslash}textit\{Synonym Encoding Method\} (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with little decay on the accuracy for benign examples. To better evaluate SEM, we also design a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic for synonym substitution based attacks. Compared with the first genetic based adversarial attack proposed in 2018, IGA can achieve higher attack success rate with lower word substitution rate, at the same time maintain the transferability of adversarial examples.},
	urldate = {2021-05-18},
	journal = {arXiv:1909.06723 [cs]},
	author = {Wang, Xiaosen and Jin, Hao and He, Kun},
	month = apr,
	year = {2020},
	note = {arXiv: 1909.06723},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 16 pages, 4 figures, 7 tables},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/RUKARYWE/1909.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6I52DNUL/Wang et al. - 2020 - Natural Language Adversarial Attacks and Defenses .pdf:application/pdf},
}

@article{wang_survey_2019,
	title = {A survey on {Adversarial} {Attacks} and {Defenses} in {Text}},
	url = {https://arxiv.org/abs/1902.07285v2},
	abstract = {Deep neural networks (DNNs) have shown an inherent vulnerability to adversarial examples which are maliciously crafted on real examples by attackers, aiming at making target DNNs misbehave. The threats of adversarial examples are widely existed in image, voice, speech, and text recognition and classification. Inspired by the previous work, researches on adversarial attacks and defenses in text domain develop rapidly. In order to make people have a general understanding about the field, this article presents a comprehensive review on adversarial examples in text. We analyze the advantages and shortcomings of recent adversarial examples generation methods and elaborate the efficiency and limitations on countermeasures. Finally, we discuss the challenges in adversarial texts and provide a research direction of this aspect.},
	language = {en},
	urldate = {2021-05-18},
	author = {Wang, Wenqi and Wang, Lina and Tang, Benxiao and Wang, Run and Ye, Aoshuang},
	month = feb,
	year = {2019},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/MYFQEL2L/1902.html:text/html;Full Text:/Users/teetusaini/Zotero/storage/Q2VBU8LL/Wang et al. - 2019 - A survey on Adversarial Attacks and Defenses in Te.pdf:application/pdf},
}

@article{wang_towards_2021-1,
	title = {Towards a {Robust} {Deep} {Neural} {Network} in {Texts}: {A} {Survey}},
	shorttitle = {Towards a {Robust} {Deep} {Neural} {Network} in {Texts}},
	url = {http://arxiv.org/abs/1902.07285},
	abstract = {Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques. We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.},
	urldate = {2021-05-18},
	journal = {arXiv:1902.07285 [cs]},
	author = {Wang, Wenqi and Wang, Run and Wang, Lina and Wang, Zhibo and Ye, Aoshuang},
	month = apr,
	year = {2021},
	note = {arXiv: 1902.07285},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/PP3DSKQC/1902.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/P2NHSKAZ/Wang et al. - 2021 - Towards a Robust Deep Neural Network in Texts A S.pdf:application/pdf},
}

@article{patwa_fighting_2021,
	title = {Fighting an {Infodemic}: {COVID}-19 {Fake} {News} {Dataset}},
	shorttitle = {Fighting an {Infodemic}},
	url = {http://arxiv.org/abs/2011.03327},
	abstract = {Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46{\textbackslash}\% F1-score with SVM. The data and code is available at: https://github.com/parthpatwa/covid19-fake-news-dectection},
	urldate = {2021-05-18},
	journal = {arXiv:2011.03327 [cs]},
	author = {Patwa, Parth and Sharma, Shivam and Pykl, Srinivas and Guptha, Vineeth and Kumari, Gitanjali and Akhtar, Md Shad and Ekbal, Asif and Das, Amitava and Chakraborty, Tanmoy},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.03327},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	annote = {Comment: Accepted at CONSTRAINT-2021, Collocated with AAAI-2021},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/D45KXKU8/2011.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/D2R6R9G4/Patwa et al. - 2021 - Fighting an Infodemic COVID-19 Fake News Dataset.pdf:application/pdf},
}

@article{yuan_adversarial_2018,
	title = {Adversarial {Examples}: {Attacks} and {Defenses} for {Deep} {Learning}},
	shorttitle = {Adversarial {Examples}},
	url = {http://arxiv.org/abs/1712.07107},
	abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.},
	urldate = {2021-05-18},
	journal = {arXiv:1712.07107 [cs, stat]},
	author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	month = jul,
	year = {2018},
	note = {arXiv: 1712.07107},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Github: https://github.com/chbrian/awesome-adversarial-examples-dl},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/8XZTLDVI/1712.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/WH2XII8L/Yuan et al. - 2018 - Adversarial Examples Attacks and Defenses for Dee.pdf:application/pdf},
}

@article{akhtar_threat_2018,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1801.00553},
	abstract = {Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.},
	urldate = {2021-05-18},
	journal = {arXiv:1801.00553 [cs]},
	author = {Akhtar, Naveed and Mian, Ajmal},
	month = feb,
	year = {2018},
	note = {arXiv: 1801.00553},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Incorporates feedback provided by multiple researchers},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/H7YLBURW/1801.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/9IGBAP98/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:application/pdf},
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial {Attacks} and {Defences}: {A} {Survey}},
	shorttitle = {Adversarial {Attacks} and {Defences}},
	url = {http://arxiv.org/abs/1810.00069},
	abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
	urldate = {2021-05-18},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/BQXYNW59/1810.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/WKQNEES3/Chakraborty et al. - 2018 - Adversarial Attacks and Defences A Survey.pdf:application/pdf},
}

@inproceedings{chang_bias_2019,
	title = {Bias and {Fairness} in {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/D19-2004},
	abstract = {Kai-Wei Chang, Vinod Prabhakaran, Vicente Ordonez. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts. 2019.},
	language = {en-us},
	urldate = {2021-05-15},
	author = {Chang, Kai-Wei and Prabhakaran, Vinod and Ordonez, Vicente},
	month = nov,
	year = {2019},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/P8MAFGVT/D19-2004.html:text/html},
}

@article{miyato_adversarial_2017,
	title = {Adversarial {Training} {Methods} for {Semi}-{Supervised} {Text} {Classification}},
	url = {http://arxiv.org/abs/1605.07725},
	abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
	urldate = {2021-05-13},
	journal = {arXiv:1605.07725 [cs, stat]},
	author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
	month = may,
	year = {2017},
	note = {arXiv: 1605.07725},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/BI9FJVAE/1605.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/JS2XZ39B/Miyato et al. - 2017 - Adversarial Training Methods for Semi-Supervised T.pdf:application/pdf},
}

@article{miyato_virtual_2018,
	title = {Virtual {Adversarial} {Training}: {A} {Regularization} {Method} for {Supervised} and {Semi}-{Supervised} {Learning}},
	shorttitle = {Virtual {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1704.03976},
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
	urldate = {2021-05-13},
	journal = {arXiv:1704.03976 [cs, stat]},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	month = jun,
	year = {2018},
	note = {arXiv: 1704.03976},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To be appeared in IEEE Transactions on Pattern Analysis and Machine Intelligence},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/7PTPI7YX/1704.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/W9ETVEI8/Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf:application/pdf},
}

@article{liu_adversarial_2020,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2004.08994},
	abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
	urldate = {2021-05-13},
	journal = {arXiv:2004.08994 [cs]},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.08994},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 13 pages, 9 tables, 2 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/862WBUPA/2004.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/Z8SXZTEF/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf:application/pdf},
}

@inproceedings{du_adversarial_2020,
	address = {Online},
	title = {Adversarial and {Domain}-{Aware} {BERT} for {Cross}-{Domain} {Sentiment} {Analysis}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.370},
	doi = {10.18653/v1/2020.acl-main.370},
	abstract = {Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Chunning and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
	month = jul,
	year = {2020},
	pages = {4019--4028},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/9BX4SLA2/Du et al. - 2020 - Adversarial and Domain-Aware BERT for Cross-Domain.pdf:application/pdf},
}

@article{zhu_at-bert_2021,
	title = {{AT}-{BERT}: {Adversarial} {Training} {BERT} for {Acronym} {Identification} {Winning} {Solution} for {SDU}@{AAAI}-21},
	shorttitle = {{AT}-{BERT}},
	url = {http://arxiv.org/abs/2101.03700},
	abstract = {Acronym identification focuses on finding the acronyms and the phrases that have been abbreviated, which is crucial for scientific document understanding tasks. However, the limited size of manually annotated datasets hinders further improvement for the problem. Recent breakthroughs of language models pre-trained on large corpora clearly show that unsupervised pre-training can vastly improve the performance of downstream tasks. In this paper, we present an Adversarial Training BERT method named AT-BERT, our winning solution to acronym identification task for Scientific Document Understanding (SDU) Challenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to capture better semantic representation. Then we incorporate the FGM adversarial training strategy into the fine-tuning of BERT, which makes the model more robust and generalized. Furthermore, an ensemble mechanism is devised to involve the representations learned from multiple BERT variants. Assembling all these components together, the experimental results on the SciAI dataset show that our proposed approach outperforms all other competitive state-of-the-art methods.},
	urldate = {2021-05-13},
	journal = {arXiv:2101.03700 [cs]},
	author = {Zhu, Danqing and Lin, Wangli and Zhang, Yang and Zhong, Qiwei and Zeng, Guanxiong and Wu, Weilin and Tang, Jiayu},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.03700},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to SDU @ AAAI 2021, 8 pages, 3 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/I3B3CFFX/2101.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/FJ4YLD9I/Zhu et al. - 2021 - AT-BERT Adversarial Training BERT for Acronym Ide.pdf:application/pdf},
}

@article{zang_word-level_2019,
	title = {Word-level {Textual} {Adversarial} {Attacking} as {Combinatorial} {Optimization}},
	url = {https://arxiv.org/abs/1910.12196v4},
	doi = {10.18653/v1/2020.acl-main.540},
	abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
	language = {en},
	urldate = {2021-05-12},
	author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
	month = oct,
	year = {2019},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/2VYWRSDW/Zang et al. - 2019 - Word-level Textual Adversarial Attacking as Combin.pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/WG9GYFZW/1910.html:text/html},
}

@article{tarvainen_mean_2018,
	title = {Mean teachers are better role models: {Weight}-averaged consistency targets improve semi-supervised deep learning results},
	shorttitle = {Mean teachers are better role models},
	url = {http://arxiv.org/abs/1703.01780},
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
	urldate = {2021-05-11},
	journal = {arXiv:1703.01780 [cs, stat]},
	author = {Tarvainen, Antti and Valpola, Harri},
	month = apr,
	year = {2018},
	note = {arXiv: 1703.01780},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: In this version: Corrected hyperparameters of the 4000-label CIFAR-10 ResNet experiment. Changed Antti's contact info, Advances in Neural Information Processing Systems 30 (NIPS 2017) pre-proceedings},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/VJB9JEHF/1703.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/QHT4R9TD/Tarvainen and Valpola - 2018 - Mean teachers are better role models Weight-avera.pdf:application/pdf},
}

@article{rogers_primer_2021,
	title = {A {Primer} in {BERTology}: {What} {We} {Know} {About} {How} {BERT} {Works}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {A {Primer} in {BERTology}},
	url = {https://doi.org/10.1162/tacl_a_00349},
	doi = {10.1162/tacl_a_00349},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
	urldate = {2021-05-11},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = jan,
	year = {2021},
	pages = {842--866},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/5R3XIMLI/A-Primer-in-BERTology-What-We-Know-About-How-BERT.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/DBHDPUQ8/Rogers et al. - 2021 - A Primer in BERTology What We Know About How BERT.pdf:application/pdf},
}

@article{zhang_generating_2020,
	title = {Generating {Fluent} {Adversarial} {Examples} for {Natural} {Languages}},
	url = {http://arxiv.org/abs/2007.06174},
	abstract = {Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms the baseline model on attacking capability. Adversarial training with MAH also leads to better robustness and performance.},
	urldate = {2021-05-11},
	journal = {arXiv:2007.06174 [cs]},
	author = {Zhang, Huangzhao and Zhou, Hao and Miao, Ning and Li, Lei},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.06174},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2019},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/2WLPTGUH/2007.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/WXR4EEMZ/Zhang et al. - 2020 - Generating Fluent Adversarial Examples for Natural.pdf:application/pdf},
}

@article{sun_adv-bert_2020,
	title = {Adv-{BERT}: {BERT} is not robust on misspellings! {Generating} nature adversarial samples on {BERT}},
	shorttitle = {Adv-{BERT}},
	url = {http://arxiv.org/abs/2003.04985},
	abstract = {There is an increasing amount of literature that claims the brittleness of deep neural networks in dealing with adversarial examples that are created maliciously. It is unclear, however, how the models will perform in realistic scenarios where {\textbackslash}textit\{natural rather than malicious\} adversarial instances often exist. This work systematically explores the robustness of BERT, the state-of-the-art Transformer-style model in NLP, in dealing with noisy data, particularly mistakes in typing the keyboard, that occur inadvertently. Intensive experiments on sentiment analysis and question answering benchmarks indicate that: (i) Typos in various words of a sentence do not influence equally. The typos in informative words make severer damages; (ii) Mistype is the most damaging factor, compared with inserting, deleting, etc.; (iii) Humans and machines have different focuses on recognizing adversarial attacks.},
	urldate = {2021-05-11},
	journal = {arXiv:2003.04985 [cs]},
	author = {Sun, Lichao and Hashimoto, Kazuma and Yin, Wenpeng and Asai, Akari and Li, Jia and Yu, Philip and Xiong, Caiming},
	month = feb,
	year = {2020},
	note = {arXiv: 2003.04985},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/ETS5A5G9/2003.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/TDW6TXYR/Sun et al. - 2020 - Adv-BERT BERT is not robust on misspellings! Gene.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-05-09},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/ISN3SHEF/1706.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/AHTHPI6X/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2021-05-09},
	journal = {arXiv:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.01108},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/43KMCF84/1910.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6R55MCXP/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@article{lee_patentbert_2019,
	title = {{PatentBERT}: {Patent} {Classification} with {Fine}-{Tuning} a pre-trained {BERT} {Model}},
	shorttitle = {{PatentBERT}},
	url = {http://arxiv.org/abs/1906.02124},
	abstract = {In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.},
	urldate = {2021-05-09},
	journal = {arXiv:1906.02124 [cs, stat]},
	author = {Lee, Jieh-Sheng and Hsiang, Jieh},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02124},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/H6D8D737/1906.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/ADDVF3PF/Lee and Hsiang - 2019 - PatentBERT Patent Classification with Fine-Tuning.pdf:application/pdf},
}

@article{hao_visualizing_2019,
	title = {Visualizing and {Understanding} the {Effectiveness} of {BERT}},
	url = {http://arxiv.org/abs/1908.05620},
	abstract = {Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.},
	urldate = {2021-05-09},
	journal = {arXiv:1908.05620 [cs]},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05620},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted by EMNLP-19},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/2I4SDZ7S/1908.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/QNNLHL3A/Hao et al. - 2019 - Visualizing and Understanding the Effectiveness of.pdf:application/pdf},
}

@article{zhang_revisiting_2021,
	title = {Revisiting {Few}-sample {BERT} {Fine}-tuning},
	url = {http://arxiv.org/abs/2006.05987},
	abstract = {This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.},
	urldate = {2021-05-09},
	journal = {arXiv:2006.05987 [cs]},
	author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.05987},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Code available at https://github.com/asappresearch/revisit-bert-finetuning},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/63J5XAQY/2006.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MUZ83PHM/Zhang et al. - 2021 - Revisiting Few-sample BERT Fine-tuning.pdf:application/pdf},
}

@article{wang_structbert_2019,
	title = {{StructBERT}: {Incorporating} {Language} {Structures} into {Pre}-training for {Deep} {Language} {Understanding}},
	shorttitle = {{StructBERT}},
	url = {http://arxiv.org/abs/1908.04577},
	abstract = {Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.},
	urldate = {2021-05-09},
	journal = {arXiv:1908.04577 [cs]},
	author = {Wang, Wei and Bi, Bin and Yan, Ming and Wu, Chen and Bao, Zuyi and Xia, Jiangnan and Peng, Liwei and Si, Luo},
	month = sep,
	year = {2019},
	note = {arXiv: 1908.04577},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 10 Pages},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/NK9HBTPP/1908.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/YNBN3HYL/Wang et al. - 2019 - StructBERT Incorporating Language Structures into.pdf:application/pdf},
}

@article{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s {Transformers}: {State}-of-the-art {Natural} {Language} {Processing}},
	shorttitle = {{HuggingFace}'s {Transformers}},
	url = {http://arxiv.org/abs/1910.03771},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	urldate = {2021-05-09},
	journal = {arXiv:1910.03771 [cs]},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = jul,
	year = {2020},
	note = {arXiv: 1910.03771},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 4 figures, more details at https://github.com/huggingface/transformers},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/TDJ7DQMM/1910.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/Y3WVKMCG/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf},
}

@article{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2021-05-09},
	journal = {arXiv:1906.08237 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv: 1906.08237},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Pretrained models and code are available at https://github.com/zihangdai/xlnet},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/RHNQZGPW/1906.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/ISS323V8/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and {\textbackslash}squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	urldate = {2021-05-09},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11942},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/KQ53JPPS/1909.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/2NNW9ILA/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2021-05-09},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/NYSNL8D2/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/J5M3NDMQ/1907.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-05-09},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/REGEI929/1810.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/LZZMJ6UV/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{noever_virus-mnist_2021,
	title = {Virus-{MNIST}: {A} {Benchmark} {Malware} {Dataset}},
	shorttitle = {Virus-{MNIST}},
	url = {http://arxiv.org/abs/2103.00602},
	abstract = {The short note presents an image classification dataset consisting of 10 executable code varieties and approximately 50,000 virus examples. The malicious classes include 9 families of computer viruses and one benign set. The image formatting for the first 1024 bytes of the Portable Executable (PE) mirrors the familiar MNIST handwriting dataset, such that most of the previously explored algorithmic methods can transfer with minor modifications. The designation of 9 virus families for malware derives from unsupervised learning of class labels; we discover the families with KMeans clustering that excludes the non-malicious examples. As a benchmark using deep learning methods (MobileNetV2), we find an overall 80\% accuracy for virus identification by families when beneware is included. We also find that once a positive malware detection occurs (by signature or heuristics), the projection of the first 1024 bytes into a thumbnail image can classify with 87\% accuracy the type of virus. The work generalizes what other malware investigators have demonstrated as promising convolutional neural networks originally developed to solve image problems but applied to a new abstract domain in pixel bytes from executable files. The dataset is available on Kaggle and Github.},
	urldate = {2021-05-04},
	journal = {arXiv:2103.00602 [cs]},
	author = {Noever, David and Noever, Samantha E. Miller},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00602},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/8GSKJRE5/2103.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6EQMU4W9/Noever and Noever - 2021 - Virus-MNIST A Benchmark Malware Dataset.pdf:application/pdf},
}

@article{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2021-05-03},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/6PBELWQN/1312.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/M2CHLT8E/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf:application/pdf},
}

@article{wang_cat-gen_2020-1,
	title = {{CAT}-{Gen}: {Improving} {Robustness} in {NLP} {Models} via {Controlled} {Adversarial} {Text} {Generation}},
	shorttitle = {{CAT}-{Gen}},
	url = {http://arxiv.org/abs/2010.02338},
	abstract = {NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.},
	urldate = {2021-05-01},
	journal = {arXiv:2010.02338 [cs]},
	author = {Wang, Tianlu and Wang, Xuezhi and Qin, Yao and Packer, Ben and Li, Kang and Chen, Jilin and Beutel, Alex and Chi, Ed},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.02338},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 6 pages, accepted to EMNLP 2020},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/LFKDN9QU/2010.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/LR5BEYI6/Wang et al. - 2020 - CAT-Gen Improving Robustness in NLP Models via Co.pdf:application/pdf},
}

@article{goodman_fastwordbug_2020,
	title = {{FastWordBug}: {A} {Fast} {Method} {To} {Generate} {Adversarial} {Text} {Against} {NLP} {Applications}},
	shorttitle = {{FastWordBug}},
	url = {http://arxiv.org/abs/2002.00760},
	abstract = {In this paper, we present a novel algorithm, FastWordBug, to efficiently generate small text perturbations in a black-box setting that forces a sentiment analysis or text classification mode to make an incorrect prediction. By combining the part of speech attributes of words, we propose a scoring method that can quickly identify important words that affect text classification. We evaluate FastWordBug on three real-world text datasets and two state-of-the-art machine learning models under black-box setting. The results show that our method can significantly reduce the accuracy of the model, and at the same time, we can call the model as little as possible, with the highest attack efficiency. We also attack two popular real-world cloud services of NLP, and the results show that our method works as well.},
	urldate = {2021-05-01},
	journal = {arXiv:2002.00760 [cs]},
	author = {Goodman, Dou and Zhonghou, Lv and minghua, Wang},
	month = jan,
	year = {2020},
	note = {arXiv: 2002.00760},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/VXNLG57V/2002.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6IC8SB4M/Goodman et al. - 2020 - FastWordBug A Fast Method To Generate Adversarial.pdf:application/pdf},
}

@article{zhou_fake_2019,
	title = {Fake {News} {Detection} via {NLP} is {Vulnerable} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1901.09657},
	doi = {10.5220/0007566307940800},
	abstract = {News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.},
	urldate = {2021-05-01},
	journal = {Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
	author = {Zhou, Zhixuan and Guan, Huankang and Bhat, Meghana Moorthy and Hsu, Justin},
	year = {2019},
	note = {arXiv: 1901.09657},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	pages = {794--800},
	annote = {Comment: 11th International Conference on Agents and Artificial Intelligence (ICAART 2019)},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/UWJ6Z4RD/1901.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/WJSEKDHH/Zhou et al. - 2019 - Fake News Detection via NLP is Vulnerable to Adver.pdf:application/pdf},
}

@inproceedings{zhang_paws_2019,
	address = {Minneapolis, Minnesota},
	title = {{PAWS}: {Paraphrase} {Adversaries} from {Word} {Scrambling}},
	shorttitle = {{PAWS}},
	url = {https://www.aclweb.org/anthology/N19-1131},
	doi = {10.18653/v1/N19-1131},
	abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS ({\textbackslash}textless40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
	urldate = {2021-04-30},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
	month = jun,
	year = {2019},
	pages = {1298--1308},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/9845A3YK/Zhang et al. - 2019 - PAWS Paraphrase Adversaries from Word Scrambling.pdf:application/pdf},
}

@misc{noauthor_pdf_nodate-1,
	title = {[{PDF}] {HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/HotFlip%3A-White-Box-Adversarial-Examples-for-Text-Ebrahimi-Rao/514e7fb769950dbe96eb519c88ca17e04dc829f6},
	urldate = {2021-04-30},
	file = {[PDF] HotFlip\: White-Box Adversarial Examples for Text Classification | Semantic Scholar:/Users/teetusaini/Zotero/storage/I5XETQ6L/514e7fb769950dbe96eb519c88ca17e04dc829f6.html:text/html},
}

@article{jia_certified_2019,
	title = {Certified {Robustness} to {Adversarial} {Word} {Substitutions}},
	url = {http://arxiv.org/abs/1909.00986},
	abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75{\textbackslash}\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8{\textbackslash}\%\$ and \$35{\textbackslash}\%\$, respectively.},
	urldate = {2021-04-30},
	journal = {arXiv:1909.00986 [cs]},
	author = {Jia, Robin and Raghunathan, Aditi and Göksel, Kerem and Liang, Percy},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.00986},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: EMNLP 2019},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/68BPL5HA/1909.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/CWP6P4NX/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf:application/pdf},
}

@article{saxena_textdecepter_2020,
	title = {{TextDecepter}: {Hard} {Label} {Black} {Box} {Attack} on {Text} {Classifiers}},
	shorttitle = {{TextDecepter}},
	url = {http://arxiv.org/abs/2008.06860},
	abstract = {Machine learning has been proven to be susceptible to carefully crafted samples, known as adversarial examples. The generation of these adversarial examples helps to make the models more robust and gives us an insight into the underlying decision-making of these models. Over the years, researchers have successfully attacked image classifiers in both, white and black-box settings. However, these methods are not directly applicable to texts as text data is discrete. In recent years, research on crafting adversarial examples against textual applications has been on the rise. In this paper, we present a novel approach for hard-label black-box attacks against Natural Language Processing (NLP) classifiers, where no model information is disclosed, and an attacker can only query the model to get a final decision of the classifier, without confidence scores of the classes involved. Such an attack scenario applies to real-world black-box models being used for security-sensitive applications such as sentiment analysis and toxic content detection.},
	urldate = {2021-04-30},
	journal = {arXiv:2008.06860 [cs]},
	author = {Saxena, Sachin},
	month = dec,
	year = {2020},
	note = {arXiv: 2008.06860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 10 pages, 11 tables},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/6T34RZIQ/2008.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/EVZ9AF6W/Saxena - 2020 - TextDecepter Hard Label Black Box Attack on Text .pdf:application/pdf},
}

@article{hossam_explain2attack_2021,
	title = {{Explain2Attack}: {Text} {Adversarial} {Attacks} via {Cross}-{Domain} {Interpretability}},
	shorttitle = {{Explain2Attack}},
	url = {http://arxiv.org/abs/2010.06812},
	abstract = {Training robust deep learning models for down-stream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.},
	urldate = {2021-04-30},
	journal = {arXiv:2010.06812 [cs]},
	author = {Hossam, Mahmoud and Le, Trung and Zhao, He and Phung, Dinh},
	month = jan,
	year = {2021},
	note = {arXiv: 2010.06812
version: 4},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, I.2.0, I.5.0},
	annote = {Comment: Preprint for accepted paper at 25th International Conference on Pattern Recognition (ICPR 2020)},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/TXKSDKZU/2010.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6YRBXH46/Hossam et al. - 2021 - Explain2Attack Text Adversarial Attacks via Cross.pdf:application/pdf},
}

@article{wang_textfirewall_2021,
	title = {{TextFirewall}: {Omni}-{Defending} {Against} {Adversarial} {Texts} in {Sentiment} {Classification}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{TextFirewall}},
	doi = {10.1109/ACCESS.2021.3058278},
	abstract = {Sentiment classification has been broadly applied in real life, such as product recommendation and opinion-oriented analysis. Unfortunately, the widely employed sentiment classification systems based on deep neural networks (DNNs) are susceptible to adversarial attacks with imperceptible perturbations into the legitimate texts (also called adversarial texts). Adversarial texts could cause erroneous outputs even without access to the target model, bringing security concerns to systems deployed in safety-critical applications. However, studies on defending against adversarial texts are still in the early stage and not ready for tackling the emerging threats, especially in dealing with unknown attacks. Investigating the minor differences between adversarial texts and legitimate texts and enhancing the robustness of target models are two mainstream ideas for defending against adversarial texts. However, both of them suffer the generalization issue in dealing with unknown adversarial attacks. In this paper, we proposed a general method, called TextFirewall, for defending against adversarial texts crafted by various adversarial attacks, which shows the potential in identifying new developed adversarial attacks in the future. Given a piece of text, our TextFirewall identifies the adversarial text by investigating the inconsistency between the target model's output and the impact value calculated by important words in the text. TextFirewall could be deployed as a third-party tool without modifying the target model and agnostic to the specific type of adversarial texts. Experimental results demonstrate that our proposed TextFirewall effectively identifies adversarial texts generated by the three state-of-the-art (SOTA) attacks and outperforms previous defense techniques. Specifically, TextFirewall achieves an average accuracy of 90.7\% on IMDB and 96.9\% on Yelp in defending the three SOTA attacks.},
	journal = {IEEE Access},
	author = {Wang, Wenqi and Wang, Run and Ke, Jianpeng and Wang, Lina},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Perturbation methods, Training, Adversarial texts, Genetic algorithms, Motion pictures, Robustness, Sentiment analysis, sentiment classification, sentiment polarity, Task analysis, valuable words},
	pages = {27467--27475},
	file = {IEEE Xplore Abstract Record:/Users/teetusaini/Zotero/storage/SAIFIXX6/9350600.html:text/html;IEEE Xplore Full Text PDF:/Users/teetusaini/Zotero/storage/H3EKU2J9/Wang et al. - 2021 - TextFirewall Omni-Defending Against Adversarial T.pdf:application/pdf},
}

@article{zeng_openattack_2020,
	title = {{OpenAttack}: {An} {Open}-source {Textual} {Adversarial} {Attack} {Toolkit}},
	shorttitle = {{OpenAttack}},
	url = {http://arxiv.org/abs/2009.09191},
	abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and apt comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack. It currently builds in 12 typical attack models that cover all the attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a victim model, assisting in developing new attack models, and adversarial training. Source code, built-in models and documentation can be obtained at https://github.com/thunlp/OpenAttack.},
	urldate = {2021-04-30},
	journal = {arXiv:2009.09191 [cs]},
	author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.09191},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence},
	annote = {Comment: Work in progress, 8 pages, 3 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/BIWTTJC5/2009.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/5ZUEU54S/Zeng et al. - 2020 - OpenAttack An Open-source Textual Adversarial Att.pdf:application/pdf},
}

@article{morris_second-order_2020,
	title = {Second-{Order} {NLP} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/2010.01770},
	abstract = {Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at https://github.com/jxmorris12/second-order-adversarial-examples.},
	urldate = {2021-04-30},
	journal = {arXiv:2010.01770 [cs]},
	author = {Morris, John X.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.01770},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/B3A85FRF/2010.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/M6ZMBBWB/Morris - 2020 - Second-Order NLP Adversarial Examples.pdf:application/pdf},
}

@article{guo_towards_2021,
	title = {Towards {Variable}-{Length} {Textual} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2104.08139},
	abstract = {Adversarial attacks have shown the vulnerability of machine learning models, however, it is non-trivial to conduct textual adversarial attacks on natural language processing tasks due to the discreteness of data. Most previous approaches conduct attacks with the atomic {\textbackslash}textit\{replacement\} operation, which usually leads to fixed-length adversarial examples and therefore limits the exploration on the decision space. In this paper, we propose variable-length textual adversarial attacks{\textasciitilde}(VL-Attack) and integrate three atomic operations, namely {\textbackslash}textit\{insertion\}, {\textbackslash}textit\{deletion\} and {\textbackslash}textit\{replacement\}, into a unified framework, by introducing and manipulating a special {\textbackslash}textit\{blank\} token while attacking. In this way, our approach is able to more comprehensively find adversarial examples around the decision boundary and effectively conduct adversarial attacks. Specifically, our method drops the accuracy of IMDB classification by \$96{\textbackslash}\%\$ with only editing \$1.3{\textbackslash}\%\$ tokens while attacking a pre-trained BERT model. In addition, fine-tuning the victim model with generated adversarial samples can improve the robustness of the model without hurting the performance, especially for length-sensitive models. On the task of non-autoregressive machine translation, our method can achieve \$33.18\$ BLEU score on IWSLT14 German-English translation, achieving an improvement of \$1.47\$ over the baseline model.},
	urldate = {2021-04-30},
	journal = {arXiv:2104.08139 [cs]},
	author = {Guo, Junliang and Zhang, Zhirui and Zhang, Linlin and Xu, Linli and Chen, Boxing and Chen, Enhong and Luo, Weihua},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.08139},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/FPP6WVZS/2104.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/ZBSNKUYA/Guo et al. - 2021 - Towards Variable-Length Textual Adversarial Attack.pdf:application/pdf},
}

@article{liu_adversarial_2020-1,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2004.08994},
	abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
	urldate = {2021-04-30},
	journal = {arXiv:2004.08994 [cs]},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.08994},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 13 pages, 9 tables, 2 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/BG5HQ56P/2004.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/SZEBKH6F/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf:application/pdf},
}

@article{zang_learning_2020,
	title = {Learning to {Attack}: {Towards} {Textual} {Adversarial} {Attacking} in {Real}-world {Situations}},
	shorttitle = {Learning to {Attack}},
	url = {http://arxiv.org/abs/2009.09192},
	abstract = {Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public.},
	urldate = {2021-04-30},
	journal = {arXiv:2009.09192 [cs]},
	author = {Zang, Yuan and Hou, Bairu and Qi, Fanchao and Liu, Zhiyuan and Meng, Xiaojun and Sun, Maosong},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.09192},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence},
	annote = {Comment: work in progress, 10 pages, 6 figures},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/MGU5E88K/2009.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/99N5JB8S/Zang et al. - 2020 - Learning to Attack Towards Textual Adversarial At.pdf:application/pdf},
}

@misc{tum_survey_2020,
	title = {A {Survey} of the {State}-of-the-{Art} {Language} {Models} up to {Early} 2020},
	url = {https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6},
	abstract = {This is a survey of the different approaches in natural language processing (NLP) from an early day to the most recent state-of-the-art…},
	language = {en},
	urldate = {2021-04-30},
	journal = {Medium},
	author = {Tum, Phylypo},
	month = nov,
	year = {2020},
	annote = {can get lots of best language model},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/WG6QTWSU/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6.html:text/html},
}

@article{cheng_seq2sick_2020,
	title = {{Seq2Sick}: {Evaluating} the {Robustness} of {Sequence}-to-{Sequence} {Models} with {Adversarial} {Examples}},
	shorttitle = {{Seq2Sick}},
	url = {http://arxiv.org/abs/1803.01128},
	abstract = {Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.},
	urldate = {2021-04-30},
	journal = {arXiv:1803.01128 [cs]},
	author = {Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
	month = apr,
	year = {2020},
	note = {arXiv: 1803.01128},
	keywords = {Computer Science - Machine Learning},
	annote = {Can be treated as baseline model.
 },
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/EPSSN3KE/1803.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/WWLPP8RE/Cheng et al. - 2020 - Seq2Sick Evaluating the Robustness of Sequence-to.pdf:application/pdf},
}

@article{nicolae_adversarial_2019-1,
	title = {Adversarial {Robustness} {Toolbox} v1.0.0},
	url = {http://arxiv.org/abs/1807.01069},
	abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
	urldate = {2021-04-30},
	journal = {arXiv:1807.01069 [cs, stat]},
	author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
	month = nov,
	year = {2019},
	note = {arXiv: 1807.01069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 34 pages},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/V6LARAVU/1807.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/QGD9XEP2/Nicolae et al. - 2019 - Adversarial Robustness Toolbox v1.0.0.pdf:application/pdf},
}

@article{rauber_foolbox_2018,
	title = {Foolbox: {A} {Python} toolbox to benchmark the robustness of machine learning models},
	shorttitle = {Foolbox},
	url = {http://arxiv.org/abs/1707.04131},
	abstract = {Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox . The most up-to-date documentation can be found at http://foolbox.readthedocs.io .},
	urldate = {2021-04-30},
	journal = {arXiv:1707.04131 [cs, stat]},
	author = {Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
	month = mar,
	year = {2018},
	note = {arXiv: 1707.04131},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code and examples available at https://github.com/bethgelab/foolbox and documentation available at http://foolbox.readthedocs.io},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/DH6R97E7/1707.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/6FHMC639/Rauber et al. - 2018 - Foolbox A Python toolbox to benchmark the robustn.pdf:application/pdf},
}

@article{huq_adversarial_2020,
	title = {Adversarial {Attacks} and {Defense} on {Texts}: {A} {Survey}},
	shorttitle = {Adversarial {Attacks} and {Defense} on {Texts}},
	url = {https://arxiv.org/abs/2005.14108v3},
	abstract = {Deep learning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis, and many others. However, in recent years it has been shown that these models possess weakness to noises which force the model to misclassify. This issue has been studied profoundly in the image and audio domain. Very little has been studied on this issue concerning textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript, we accumulated and analyzed different attacking techniques and various defense models to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome to move forward in this field.},
	language = {en},
	urldate = {2021-04-30},
	author = {Huq, Aminul and Pervin, Mst Tasnim},
	month = may,
	year = {2020},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/WQXJECUM/2005.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/SQ5H7Z55/Huq and Pervin - 2020 - Adversarial Attacks and Defense on Texts A Survey.pdf:application/pdf},
}

@article{morris_reevaluating_2020,
	title = {Reevaluating {Adversarial} {Examples} in {Natural} {Language}},
	url = {https://arxiv.org/abs/2004.14174v2},
	abstract = {State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38\% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.},
	language = {en},
	urldate = {2021-04-30},
	author = {Morris, John X. and Lifland, Eli and Lanchantin, Jack and Ji, Yangfeng and Qi, Yanjun},
	month = apr,
	year = {2020},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/BGWPSGAV/2004.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/79PHR3GG/Morris et al. - 2020 - Reevaluating Adversarial Examples in Natural Langu.pdf:application/pdf},
}

@article{ren_generating_2020,
	title = {Generating {Natural} {Language} {Adversarial} {Examples} on a {Large} {Scale} with {Generative} {Models}},
	url = {https://arxiv.org/abs/2003.10388v1},
	abstract = {Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.},
	language = {en},
	urldate = {2021-04-30},
	author = {Ren, Yankun and Lin, Jianbin and Tang, Siliang and Zhou, Jun and Yang, Shuang and Qi, Yuan and Ren, Xiang},
	month = mar,
	year = {2020},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/GE62AIGN/Ren et al. - 2020 - Generating Natural Language Adversarial Examples o.pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/MKHRYVBE/2003.html:text/html},
}

@article{zhang_adversarial_2019,
	title = {Adversarial {Attacks} on {Deep} {Learning} {Models} in {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Adversarial {Attacks} on {Deep} {Learning} {Models} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1901.06796},
	abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.},
	urldate = {2021-04-29},
	journal = {arXiv:1901.06796 [cs]},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.06796},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/MVREFBFT/1901.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MQIWCTZQ/Zhang et al. - 2019 - Adversarial Attacks on Deep Learning Models in Nat.pdf:application/pdf},
}

@misc{noauthor_thunlptaadpapers_2021,
	title = {thunlp/{TAADpapers}},
	url = {https://github.com/thunlp/TAADpapers},
	abstract = {Must-read Papers on Textual Adversarial Attack and Defense},
	urldate = {2021-04-29},
	publisher = {THUNLP},
	month = apr,
	year = {2021},
	note = {original-date: 2019-06-09T09:46:05Z},
	keywords = {adversarial-attacks, nlp, adversarial-defense, adversarial-learning, paper-list},
}

@inproceedings{hsieh_robustness_2019,
	address = {Florence, Italy},
	title = {On the {Robustness} of {Self}-{Attentive} {Models}},
	url = {https://www.aclweb.org/anthology/P19-1147},
	doi = {10.18653/v1/P19-1147},
	abstract = {This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hsieh, Yu-Lun and Cheng, Minhao and Juan, Da-Cheng and Wei, Wei and Hsu, Wen-Lian and Hsieh, Cho-Jui},
	month = jul,
	year = {2019},
	pages = {1520--1529},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/XL46HHGJ/Hsieh et al. - 2019 - On the Robustness of Self-Attentive Models.pdf:application/pdf},
}

@article{yang_greedy_nodate,
	title = {Greedy {Attack} and {Gumbel} {Attack}: {Generating} {Adversarial} {Examples} for {Discrete} {Data}},
	abstract = {We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various stateof-the-art models for text classiﬁcation, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only ﬁve characters through Greedy Attack.},
	language = {en},
	author = {Yang, Puyudi and Chen, Jianbo and Hsieh, Cho-Jui and Wang, Jane-Ling and Jordan, Michael I},
	pages = {36},
	file = {Yang et al. - Greedy Attack and Gumbel Attack Generating Advers.pdf:/Users/teetusaini/Zotero/storage/FCP4T3W9/Yang et al. - Greedy Attack and Gumbel Attack Generating Advers.pdf:application/pdf},
}

@article{cheng_seq2sick_2020-1,
	title = {{Seq2Sick}: {Evaluating} the {Robustness} of {Sequence}-to-{Sequence} {Models} with {Adversarial} {Examples}},
	shorttitle = {{Seq2Sick}},
	url = {http://arxiv.org/abs/1803.01128},
	abstract = {Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.},
	urldate = {2021-04-29},
	journal = {arXiv:1803.01128 [cs]},
	author = {Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
	month = apr,
	year = {2020},
	note = {arXiv: 1803.01128},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/CCK2ELTQ/1803.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/7SS3FL2R/Cheng et al. - 2020 - Seq2Sick Evaluating the Robustness of Sequence-to.pdf:application/pdf},
}

@inproceedings{zheng_evaluating_2020,
	address = {Online},
	title = {Evaluating and {Enhancing} the {Robustness} of {Neural} {Network}-based {Dependency} {Parsing} {Models} with {Adversarial} {Examples}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.590},
	doi = {10.18653/v1/2020.acl-main.590},
	abstract = {Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77\% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zheng, Xiaoqing and Zeng, Jiehang and Zhou, Yi and Hsieh, Cho-Jui and Cheng, Minhao and Huang, Xuanjing},
	month = jul,
	year = {2020},
	pages = {6600--6610},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/YXJZ4E2M/Zheng et al. - 2020 - Evaluating and Enhancing the Robustness of Neural .pdf:application/pdf},
}

@inproceedings{yin_robustness_2020,
	address = {Online},
	title = {On the {Robustness} of {Language} {Encoders} against {Grammatical} {Errors}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.310},
	doi = {10.18653/v1/2020.acl-main.310},
	abstract = {We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Fan and Long, Quanyu and Meng, Tao and Chang, Kai-Wei},
	month = jul,
	year = {2020},
	pages = {3386--3403},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/ZDENM6X5/Yin et al. - 2020 - On the Robustness of Language Encoders against Gra.pdf:application/pdf},
}

@inproceedings{tan_its_2020,
	address = {Online},
	title = {It's {Morphin}' {Time}! {Combating} {Linguistic} {Discrimination} with {Inflectional} {Perturbations}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.263},
	doi = {10.18653/v1/2020.acl-main.263},
	abstract = {Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Samson and Joty, Shafiq and Kan, Min-Yen and Socher, Richard},
	month = jul,
	year = {2020},
	pages = {2920--2935},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/WRULE6FD/Tan et al. - 2020 - It's Morphin' Time! Combating Linguistic Discrimin.pdf:application/pdf},
}

@inproceedings{shi_robustness_2020,
	address = {Online},
	title = {Robustness to {Modification} with {Shared} {Words} in {Paraphrase} {Identification}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.16},
	doi = {10.18653/v1/2020.findings-emnlp.16},
	abstract = {Revealing the robustness issues of natural language processing models and improving their robustness is important to their performance under difficult situations. In this paper, we study the robustness of paraphrase identification models from a new perspective – via modification with shared words, and we show that the models have significant robustness issues when facing such modifications. To modify an example consisting of a sentence pair, we either replace some words shared by both sentences or introduce new shared words. We aim to construct a valid new example such that a target model makes a wrong prediction. To find a modification solution, we use beam search constrained by heuristic rules, and we leverage a BERT masked language model for generating substitution words compatible with the context. Experiments show that the performance of the target models has a dramatic drop on the modified examples, thereby revealing the robustness issue. We also show that adversarial training can mitigate this issue.},
	urldate = {2021-04-29},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Shi, Zhouxing and Huang, Minlie},
	month = nov,
	year = {2020},
	pages = {164--171},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/X4YHYZLW/Shi and Huang - 2020 - Robustness to Modification with Shared Words in Pa.pdf:application/pdf},
}

@inproceedings{garg_bae_2020,
	address = {Online},
	title = {{BAE}: {BERT}-based {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{BAE}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.498},
	doi = {10.18653/v1/2020.emnlp-main.498},
	abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Garg, Siddhant and Ramakrishnan, Goutham},
	month = nov,
	year = {2020},
	pages = {6174--6181},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/73IQHHX5/Garg and Ramakrishnan - 2020 - BAE BERT-based Adversarial Examples for Text Clas.pdf:application/pdf},
}

@inproceedings{meng_geometry-inspired_2020,
	address = {Barcelona, Spain (Online)},
	title = {A {Geometry}-{Inspired} {Attack} for {Generating} {Natural} {Language} {Adversarial} {Examples}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.585},
	doi = {10.18653/v1/2020.coling-main.585},
	abstract = {Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Meng, Zhao and Wattenhofer, Roger},
	month = dec,
	year = {2020},
	pages = {6679--6689},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/2YVNA52V/Meng and Wattenhofer - 2020 - A Geometry-Inspired Attack for Generating Natural .pdf:application/pdf},
}

@article{li_contextualized_2021,
	title = {Contextualized {Perturbation} for {Textual} {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2009.07502},
	abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
	urldate = {2021-04-29},
	journal = {arXiv:2009.07502 [cs]},
	author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.07502},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by NAACL 2021, long paper},
	file = {arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/WC22HCKZ/2009.html:text/html;arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/QPU85HX3/Li et al. - 2021 - Contextualized Perturbation for Textual Adversaria.pdf:application/pdf},
}

@inproceedings{ribeiro_semantically_2018,
	address = {Melbourne, Australia},
	title = {Semantically {Equivalent} {Adversarial} {Rules} for {Debugging} {NLP} models},
	url = {https://www.aclweb.org/anthology/P18-1079},
	doi = {10.18653/v1/P18-1079},
	abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jul,
	year = {2018},
	pages = {856--865},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/ZTWB68B3/Ribeiro et al. - 2018 - Semantically Equivalent Adversarial Rules for Debu.pdf:application/pdf},
}

@inproceedings{zhang_paws_2019-1,
	address = {Minneapolis, Minnesota},
	title = {{PAWS}: {Paraphrase} {Adversaries} from {Word} {Scrambling}},
	shorttitle = {{PAWS}},
	url = {https://www.aclweb.org/anthology/N19-1131},
	doi = {10.18653/v1/N19-1131},
	abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS ({\textbackslash}textless40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
	urldate = {2021-04-29},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
	month = jun,
	year = {2019},
	pages = {1298--1308},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/KKXRKSFV/Zhang et al. - 2019 - PAWS Paraphrase Adversaries from Word Scrambling.pdf:application/pdf},
}

@article{wallace_trick_2019,
	title = {Trick {Me} {If} {You} {Can}: {Human}-in-the-{Loop} {Generation} of {Adversarial} {Examples} for {Question} {Answering}},
	volume = {7},
	shorttitle = {Trick {Me} {If} {You} {Can}},
	url = {https://www.aclweb.org/anthology/Q19-1029},
	doi = {10.1162/tacl_a_00279},
	abstract = {Adversarial evaluation stress-tests a model's understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human–computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.},
	urldate = {2021-04-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and Boyd-Graber, Jordan},
	month = mar,
	year = {2019},
	pages = {387--401},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/FNHKGK82/Wallace et al. - 2019 - Trick Me If You Can Human-in-the-Loop Generation .pdf:application/pdf},
}

@article{maheshwary_generating_2021,
	title = {Generating {Natural} {Language} {Attacks} in a {Hard} {Label} {Black} {Box} {Setting}},
	url = {http://arxiv.org/abs/2012.14956},
	abstract = {We study an important and challenging task of attacking natural language processing models in a hard label black box setting. We propose a decision-based attack strategy that crafts high quality adversarial examples on text classification and entailment tasks. Our proposed attack strategy leverages population-based optimization algorithm to craft plausible and semantically similar adversarial examples by observing only the top label predicted by the target model. At each iteration, the optimization procedure allow word replacements that maximizes the overall semantic similarity between the original and the adversarial text. Further, our approach does not rely on using substitute models or any kind of training data. We demonstrate the efficacy of our proposed approach through extensive experimentation and ablation studies on five state-of-the-art target models across seven benchmark datasets. In comparison to attacks proposed in prior literature, we are able to achieve a higher success rate with lower word perturbation percentage that too in a highly restricted setting.},
	urldate = {2021-06-29},
	journal = {arXiv:2012.14956 [cs]},
	author = {Maheshwary, Rishabh and Maheshwary, Saket and Pudi, Vikram},
	month = apr,
	year = {2021},
	note = {arXiv: 2012.14956
version: 2},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at AAAI 2021 (Main Conference)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MWGKG6MI/Maheshwary et al. - 2021 - Generating Natural Language Attacks in a Hard Labe.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/B3FKE7HM/2012.html:text/html},
}

@article{feng_survey_2021,
	title = {A {Survey} of {Data} {Augmentation} {Approaches} for {NLP}},
	url = {http://arxiv.org/abs/2105.03075},
	abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP},
	urldate = {2021-06-30},
	journal = {arXiv:2105.03075 [cs]},
	author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03075},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/IS67P73S/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/P2M75A5R/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.html:text/html},
}

@inproceedings{ren_generating_2019,
	address = {Florence, Italy},
	title = {Generating {Natural} {Language} {Adversarial} {Examples} through {Probability} {Weighted} {Word} {Saliency}},
	url = {https://aclanthology.org/P19-1103},
	doi = {10.18653/v1/P19-1103},
	abstract = {We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.},
	urldate = {2021-07-31},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
	month = jul,
	year = {2019},
	pages = {1085--1097},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/DC366H9X/Ren et al. - 2019 - Generating Natural Language Adversarial Examples t.pdf:application/pdf},
}

@article{jin_is_2020-1,
	title = {Is {BERT} {Really} {Robust}? {A} {Strong} {Baseline} for {Natural} {Language} {Attack} on {Text} {Classification} and {Entailment}},
	shorttitle = {Is {BERT} {Really} {Robust}?},
	url = {http://arxiv.org/abs/1907.11932},
	abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.},
	urldate = {2021-07-31},
	journal = {arXiv:1907.11932 [cs]},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
	month = apr,
	year = {2020},
	note = {arXiv: 1907.11932},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: AAAI 2020 (Oral)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/5B9H8NR9/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/VFK26WA8/1907.html:text/html},
}

@article{garg_bae_2020-1,
	title = {{BAE}: {BERT}-based {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{BAE}},
	url = {http://arxiv.org/abs/2004.01970},
	abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
	urldate = {2021-07-31},
	journal = {arXiv:2004.01970 [cs]},
	author = {Garg, Siddhant and Ramakrishnan, Goutham},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.01970},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at EMNLP 2020 Main Conference},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/JSNHP4XP/Garg and Ramakrishnan - 2020 - BAE BERT-based Adversarial Examples for Text Clas.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/DUJWQ6ET/2004.html:text/html},
}

@article{li_textbugger_2019,
	title = {{TextBugger}: {Generating} {Adversarial} {Text} {Against} {Real}-world {Applications}},
	shorttitle = {{TextBugger}},
	url = {http://arxiv.org/abs/1812.05271},
	doi = {10.14722/ndss.2019.23138},
	abstract = {Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9{\textbackslash}\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100{\textbackslash}\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97{\textbackslash}\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.},
	urldate = {2021-07-31},
	journal = {Proceedings 2019 Network and Distributed System Security Symposium},
	author = {Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
	year = {2019},
	note = {arXiv: 1812.05271},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: To appear in NDSS 2019},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MYDNKSM6/Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/4C4B5NMU/1812.html:text/html},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	number = {4},
	urldate = {2021-08-18},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	note = {arXiv: 1906.02691},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {307--392},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/JF5K7WWC/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/62AI9SLL/1906.html:text/html},
}

@article{foster_generative_nodate,
	title = {Generative {Deep} {Learning}},
	language = {en},
	author = {Foster, David},
	pages = {330},
	file = {Foster - Generative Deep Learning.pdf:/Users/teetusaini/Zotero/storage/NY7VZDTH/Foster - Generative Deep Learning.pdf:application/pdf},
}

@book{ravichandiran_getting_2021,
	title = {Getting {Started} with {Google} {BERT}},
	isbn = {978-1-83882-159-3},
	url = {https://learning.oreilly.com/library/view/-/9781838821593/?ar},
	abstract = {Kickstart your NLP journey by exploring BERT and its variants such as ALBERT, RoBERTa, DistilBERT, VideoBERT, and more with Hugging Face's transformers library Key Features Explore the encoder and decoder of the transformer model Become well-versed with BERT along with ALBERT, RoBERTa, and DistilBERT Discover how to pre-train and fine-tune BERT models for several NLP tasks Book Description BERT (bidirectional encoder representations from transformer) has revolutionized the world of natural language processing (NLP) with promising results. This book is an introductory guide that will help you get to grips with Google's BERT architecture. With a detailed explanation of the transformer architecture, this book will help you understand how the transformer's encoder and decoder work. You'll explore the BERT architecture by learning how the BERT model is pre-trained and how to use pre-trained BERT for downstream tasks by fine-tuning it for NLP tasks such as sentiment analysis and text summarization with the Hugging Face transformers library. As you advance, you'll learn about different variants of BERT such as ALBERT, RoBERTa, and ELECTRA, and look at SpanBERT, which is used for NLP tasks like question answering. You'll also cover simpler and faster BERT variants based on knowledge distillation such as DistilBERT and TinyBERT. The book takes you through MBERT, XLM, and XLM-R in detail and then introduces you to sentence-BERT, which is used for obtaining sentence representation. Finally, you'll discover domain-specific BERT models such as BioBERT and ClinicalBERT, and discover an interesting variant called VideoBERT. By the end of this BERT book, you'll be well-versed with using BERT and its variants for performing practical NLP tasks. What you will learn Understand the transformer model from the ground up Find out how BERT works and pre-train it using masked language model (MLM) and next sentence prediction (NSP) tasks Get hands-on with BERT by learning to generate contextual word and sentence embeddings Fine-tune BERT for downstream tasks Get to grips with ALBERT, RoBERTa, ELECTRA, and SpanBERT models Get the hang of the BERT models based on knowledge distillation Understand cross-lingual models such as XLM and XLM-R Explore Sentence-BERT, VideoBERT, and BART Who this book is for This book is for NLP professionals and data scientists looking to simplify NLP tasks to enable efficient language understanding using BERT. A basic understanding of NLP con...},
	language = {en},
	urldate = {2021-08-18},
	author = {Ravichandiran, Sudharsan and Safari, an O'Reilly Media Company},
	year = {2021},
	note = {OCLC: 1241738046},
	file = {Ravichandiran and Safari - 2021 - Getting Started with Google BERT.pdf:/Users/teetusaini/Zotero/storage/NR3SGRR9/Ravichandiran and Safari - 2021 - Getting Started with Google BERT.pdf:application/pdf},
}

@article{li_contextualized_2021-1,
	title = {Contextualized {Perturbation} for {Textual} {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2009.07502},
	abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
	urldate = {2021-11-13},
	journal = {arXiv:2009.07502 [cs]},
	author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.07502},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by NAACL 2021, long paper},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/D6FH8JSY/Li et al. - 2021 - Contextualized Perturbation for Textual Adversaria.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/DXS2SFMY/2009.html:text/html},
}

@inproceedings{tan_its_2020-1,
	address = {Online},
	title = {It's {Morphin}' {Time}! {Combating} {Linguistic} {Discrimination} with {Inflectional} {Perturbations}},
	url = {https://aclanthology.org/2020.acl-main.263},
	doi = {10.18653/v1/2020.acl-main.263},
	abstract = {Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.},
	urldate = {2021-11-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Samson and Joty, Shafiq and Kan, Min-Yen and Socher, Richard},
	month = jul,
	year = {2020},
	pages = {2920--2935},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/JHP8WKZK/Tan et al. - 2020 - It's Morphin' Time! Combating Linguistic Discrimin.pdf:application/pdf},
}

@article{ribeiro_beyond_2020,
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} models with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {http://arxiv.org/abs/2005.04118},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2021-11-16},
	journal = {arXiv:2005.04118 [cs]},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04118},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/NXCRGQH5/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP models .pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/32ZI897H/2005.html:text/html},
}

@article{gao_black-box_2018,
	title = {Black-box {Generation} of {Adversarial} {Text} {Sequences} to {Evade} {Deep} {Learning} {Classifiers}},
	url = {http://arxiv.org/abs/1801.04354},
	abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68{\textbackslash}\% on average for a Word-LSTM model and 48{\textbackslash}\% on average for a Char-CNN model.},
	urldate = {2021-11-16},
	journal = {arXiv:1801.04354 [cs]},
	author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
	month = may,
	year = {2018},
	note = {arXiv: 1801.04354},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Information Retrieval},
	annote = {Comment: This is an extended version of the 6page Workshop version appearing in 1st Deep Learning and Security Workshop colocated with IEEE S\&P},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/EG7T53MX/Gao et al. - 2018 - Black-box Generation of Adversarial Text Sequences.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/VBZBNZWC/1801.html:text/html},
}

@article{rawlinson_significance_2007,
	title = {The {Significance} of {Letter} {Position} in {Word} {Recognition}},
	volume = {22},
	issn = {1557-959X},
	doi = {10.1109/MAES.2007.327521},
	abstract = {Presents the summary of a PhD thesis on letter position in word recognition, which was written in 1976. The thesis was the source of the text used on the cover of the October 2006 issue of the IEEE Aerospace and Electronic Systems Magazine.},
	number = {1},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Rawlinson, Graham},
	month = jan,
	year = {2007},
	note = {Conference Name: IEEE Aerospace and Electronic Systems Magazine},
	keywords = {Testing, Books, Copper, Electromagnetic scattering, Kirk field collapse effect, Marine vehicles, Metamaterials, Microwave frequencies, Roads, Shape},
	pages = {26--27},
	file = {IEEE Xplore Abstract Record:/Users/teetusaini/Zotero/storage/P2WWZJYI/4126417.html:text/html},
}

@article{alzantot_generating_2018,
	title = {Generating {Natural} {Language} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1804.07998},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
	urldate = {2021-11-22},
	journal = {arXiv:1804.07998 [cs]},
	author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
	month = sep,
	year = {2018},
	note = {arXiv: 1804.07998},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted in EMNLP 2018 (Conference on Empirical Methods in Natural Language Processing)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/YRPEYTAL/Alzantot et al. - 2018 - Generating Natural Language Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/V3U9BX5U/1804.html:text/html},
}

@article{mrksic_counter-fitting_2016,
	title = {Counter-fitting {Word} {Vectors} to {Linguistic} {Constraints}},
	url = {http://arxiv.org/abs/1603.00892},
	abstract = {In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.},
	urldate = {2021-11-22},
	journal = {arXiv:1603.00892 [cs]},
	author = {Mrkšić, Nikola and Séaghdha, Diarmuid Ó and Thomson, Blaise and Gašić, Milica and Rojas-Barahona, Lina and Su, Pei-Hao and Vandyke, David and Wen, Tsung-Hsien and Young, Steve},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.00892},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Paper accepted for the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/KW85WNTD/Mrkšić et al. - 2016 - Counter-fitting Word Vectors to Linguistic Constra.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/D2DIX7BT/1603.html:text/html},
}

@article{cer_universal_2018,
	title = {Universal {Sentence} {Encoder}},
	url = {http://arxiv.org/abs/1803.11175},
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
	urldate = {2021-11-22},
	journal = {arXiv:1803.11175 [cs]},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.11175},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages; fixed module URL in Listing 1},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/G3XN23A6/Cer et al. - 2018 - Universal Sentence Encoder.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/PZYUIY6N/1803.html:text/html},
}

@article{li_understanding_2017,
	title = {Understanding {Neural} {Networks} through {Representation} {Erasure}},
	url = {http://arxiv.org/abs/1612.08220},
	abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
	urldate = {2021-11-23},
	journal = {arXiv:1612.08220 [cs]},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	month = jan,
	year = {2017},
	note = {arXiv: 1612.08220},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/8RFSYGE7/Li et al. - 2017 - Understanding Neural Networks through Representati.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/AD2YRS2Z/1612.html:text/html},
}

@article{kim_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	url = {http://arxiv.org/abs/1408.5882},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	urldate = {2021-11-23},
	journal = {arXiv:1408.5882 [cs]},
	author = {Kim, Yoon},
	month = sep,
	year = {2014},
	note = {arXiv: 1408.5882},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear in EMNLP 2014},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/FBJDJRDM/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/I9B6Y7MS/1408.html:text/html},
}

@article{zhang_character-level_2016,
	title = {Character-level {Convolutional} {Networks} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1509.01626},
	abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
	urldate = {2021-11-23},
	journal = {arXiv:1509.01626 [cs]},
	author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
	month = apr,
	year = {2016},
	note = {arXiv: 1509.01626},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: An early version of this work entitled "Text Understanding from Scratch" was posted in Feb 2015 as arXiv:1502.01710. The present paper has considerably more experimental results and a rewritten introduction, Advances in Neural Information Processing Systems 28 (NIPS 2015)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/A2AJSS8X/Zhang et al. - 2016 - Character-level Convolutional Networks for Text Cl.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/DUCF75RD/1509.html:text/html},
}

@article{papernot_crafting_2016,
	title = {Crafting {Adversarial} {Input} {Sequences} for {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1604.08275},
	abstract = {Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.},
	urldate = {2021-11-24},
	journal = {arXiv:1604.08275 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Swami, Ananthram and Harang, Richard},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08275},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/KRYTNPGN/Papernot et al. - 2016 - Crafting Adversarial Input Sequences for Recurrent.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/M2EQRRC4/1604.html:text/html},
}

@article{belinkov_synthetic_2018,
	title = {Synthetic and {Natural} {Noise} {Both} {Break} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1711.02173},
	abstract = {Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.},
	urldate = {2021-11-24},
	journal = {arXiv:1711.02173 [cs]},
	author = {Belinkov, Yonatan and Bisk, Yonatan},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.02173},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
	annote = {Comment: ICLR 2018 camera-ready},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/33JV9EGK/Belinkov and Bisk - 2018 - Synthetic and Natural Noise Both Break Neural Mach.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/MZUDEQH5/1711.html:text/html},
}

@article{ebrahimi_how_2021,
	title = {How {Does} {Adversarial} {Fine}-{Tuning} {Benefit} {BERT}?},
	url = {http://arxiv.org/abs/2108.13602},
	abstract = {Adversarial training (AT) is one of the most reliable methods for defending against adversarial attacks in machine learning. Variants of this method have been used as regularization mechanisms to achieve SOTA results on NLP benchmarks, and they have been found to be useful for transfer learning and continual learning. We search for the reasons for the effectiveness of AT by contrasting vanilla and adversarially fine-tuned BERT models. We identify partial preservation of BERT's syntactic abilities during fine-tuning as the key to the success of AT. We observe that adversarially fine-tuned models remain more faithful to BERT's language modeling behavior and are more sensitive to the word order. As concrete examples of syntactic abilities, an adversarially fine-tuned model could have an advantage of up to 38\% on anaphora agreement and up to 11\% on dependency parsing. Our analysis demonstrates that vanilla fine-tuning oversimplifies the sentence representation by focusing heavily on a small subset of words. AT, however, moderates the effect of these influential words and encourages representational diversity. This allows for a more hierarchical representation of a sentence and leads to the mitigation of BERT's loss of syntactic abilities.},
	urldate = {2021-11-24},
	journal = {arXiv:2108.13602 [cs]},
	author = {Ebrahimi, Javid and Yang, Hao and Zhang, Wei},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.13602},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/MTXWKY4K/Ebrahimi et al. - 2021 - How Does Adversarial Fine-Tuning Benefit BERT.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/YSBT6GVG/2108.html:text/html},
}

@article{jia_certified_2019-1,
	title = {Certified {Robustness} to {Adversarial} {Word} {Substitutions}},
	url = {https://arxiv.org/abs/1909.00986v1},
	abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75{\textbackslash}\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8{\textbackslash}\%\$ and \$35{\textbackslash}\%\$, respectively.},
	language = {en},
	urldate = {2021-11-25},
	author = {Jia, Robin and Raghunathan, Aditi and Göksel, Kerem and Liang, Percy},
	month = sep,
	year = {2019},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/XYYGXDDM/1909.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/JG9P53X6/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf:application/pdf},
}

@article{jones_robust_2020,
	title = {Robust {Encodings}: {A} {Framework} for {Combating} {Adversarial} {Typos}},
	shorttitle = {Robust {Encodings}},
	url = {https://arxiv.org/abs/2005.01229v1},
	abstract = {Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3\% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3\% accuracy against a simple greedy attack.},
	language = {en},
	urldate = {2021-11-25},
	author = {Jones, Erik and Jia, Robin and Raghunathan, Aditi and Liang, Percy},
	month = may,
	year = {2020},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/VTFHSLC4/Jones et al. - 2020 - Robust Encodings A Framework for Combating Advers.pdf:application/pdf},
}

@inproceedings{xu_lexicalat_2019,
	address = {Hong Kong, China},
	title = {{LexicalAT}: {Lexical}-{Based} {Adversarial} {Reinforcement} {Training} for {Robust} {Sentiment} {Classification}},
	shorttitle = {{LexicalAT}},
	url = {https://aclanthology.org/D19-1554},
	doi = {10.18653/v1/D19-1554},
	abstract = {Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.},
	urldate = {2021-11-25},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Jingjing and Zhao, Liang and Yan, Hanqi and Zeng, Qi and Liang, Yun and Sun, Xu},
	month = nov,
	year = {2019},
	pages = {5518--5527},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/ETXQ8E5Z/Xu et al. - 2019 - LexicalAT Lexical-Based Adversarial Reinforcement.pdf:application/pdf},
}

@article{zhou_learning_2019,
	title = {Learning to {Discriminate} {Perturbations} for {Blocking} {Adversarial} {Attacks} in {Text} {Classification}},
	url = {https://arxiv.org/abs/1909.03084v1},
	abstract = {Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis. In this paper, we propose a novel framework, learning to DIScriminate Perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations. For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.},
	language = {en},
	urldate = {2021-11-25},
	author = {Zhou, Yichao and Jiang, Jyun-Yu and Chang, Kai-Wei and Wang, Wei},
	month = sep,
	year = {2019},
	file = {Snapshot:/Users/teetusaini/Zotero/storage/5U69NUPT/1909.html:text/html;Full Text PDF:/Users/teetusaini/Zotero/storage/DWTX4D95/Zhou et al. - 2019 - Learning to Discriminate Perturbations for Blockin.pdf:application/pdf},
}

@article{wang_infobert_2021,
	title = {{InfoBERT}: {Improving} {Robustness} of {Language} {Models} from {An} {Information} {Theoretic} {Perspective}},
	shorttitle = {{InfoBERT}},
	url = {http://arxiv.org/abs/2010.02329},
	abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.},
	urldate = {2021-12-06},
	journal = {arXiv:2010.02329 [cs]},
	author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.02329},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ICLR 2021. 23 pages, 9 tables, 3 figures},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/XTX9GJJS/Wang et al. - 2021 - InfoBERT Improving Robustness of Language Models .pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/PSFFFC79/2010.html:text/html},
}

@article{moradi_evaluating_2021,
	title = {Evaluating the {Robustness} of {Neural} {Language} {Models} to {Input} {Perturbations}},
	url = {http://arxiv.org/abs/2108.12237},
	abstract = {High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems robustness.},
	urldate = {2021-12-06},
	journal = {arXiv:2108.12237 [cs]},
	author = {Moradi, Milad and Samwald, Matthias},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.12237},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted by EMNLP 2021},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/CNZK233H/Moradi and Samwald - 2021 - Evaluating the Robustness of Neural Language Model.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/E9RV5X2S/2108.html:text/html},
}

@inproceedings{wang_infobert_2020,
	title = {{InfoBERT}: {Improving} {Robustness} of {Language} {Models} from {An} {Information} {Theoretic} {Perspective}},
	shorttitle = {{InfoBERT}},
	url = {https://openreview.net/forum?id=hpH98mK5Puk},
	abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing...},
	language = {en},
	urldate = {2021-12-06},
	author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/WPVNHUXV/Wang et al. - 2020 - InfoBERT Improving Robustness of Language Models .pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/7FAETCG7/forum.html:text/html},
}

@inproceedings{han_robust_2021,
	address = {Online},
	title = {Robust {Transfer} {Learning} with {Pretrained} {Language} {Models} through {Adapters}},
	url = {https://aclanthology.org/2021.acl-short.108},
	doi = {10.18653/v1/2021.acl-short.108},
	abstract = {Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Han, Wenjuan and Pang, Bo and Wu, Ying Nian},
	month = aug,
	year = {2021},
	pages = {854--861},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/LDI8A3F8/Han et al. - 2021 - Robust Transfer Learning with Pretrained Language .pdf:application/pdf},
}

@inproceedings{wang_infobert_2020-1,
	title = {{InfoBERT}: {Improving} {Robustness} of {Language} {Models} from {An} {Information} {Theoretic} {Perspective}},
	shorttitle = {{InfoBERT}},
	url = {https://openreview.net/forum?id=hpH98mK5Puk},
	abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing...},
	language = {en},
	urldate = {2021-12-06},
	author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
	month = sep,
	year = {2020},
}

@inproceedings{bhavani_review_2021,
	title = {A {Review} of {State} {Art} of {Text} {Classification} {Algorithms}},
	doi = {10.1109/ICCMC51019.2021.9418262},
	abstract = {In the recent years, the categorization of text documents into predefined classifications has perceived a growing interest due to the growing of documents in digital form and needs to organize them. Text categorization is one of the extensively used for natural language processing (NLP) applications have achieved using machine learning algorithms. Text classification is a challenging researcher to find the best suitable structure and technique. Classification process done using manual and automatic classification. This research paper covers the preprocessing, feature extraction, different algorithms and techniques for text classification and finally evaluates the performance metrics for assessment.},
	booktitle = {2021 5th {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	author = {Bhavani, A. and Santhosh Kumar, B.},
	month = apr,
	year = {2021},
	keywords = {Feature extraction, Social networking (online), Support vector machines, Machine learning algorithms, Classification Algorithms, Deep Learning Models, Evaluation Masures, Feature Extraction, Law, Manuals, Natural Language Processing, Preprocessing, Text categorization, Text Classification},
	pages = {1484--1490},
	file = {IEEE Xplore Abstract Record:/Users/teetusaini/Zotero/storage/WAPX38Y5/9418262.html:text/html;IEEE Xplore Full Text PDF:/Users/teetusaini/Zotero/storage/M7SAY3BV/Bhavani and Santhosh Kumar - 2021 - A Review of State Art of Text Classification Algor.pdf:application/pdf},
}

@inproceedings{li_textshield_2020,
	title = {\{{TextShield}\}: {Robust} {Text} {Classification} {Based} on {Multimodal} {Embedding} and {Neural} {Machine} {Translation}},
	isbn = {978-1-939133-17-5},
	shorttitle = {\{{TextShield}\}},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/li-jinfeng},
	language = {en},
	urldate = {2021-12-09},
	author = {Li, Jinfeng and Du, Tianyu and Ji, Shouling and Zhang, Rong and Lu, Quan and Yang, Min and Wang, Ting},
	year = {2020},
	pages = {1381--1398},
	file = {Full Text PDF:/Users/teetusaini/Zotero/storage/CVC9642H/Li et al. - 2020 - TextShield Robust Text Classification Based on .pdf:application/pdf;Snapshot:/Users/teetusaini/Zotero/storage/VRVRKX89/li-jinfeng.html:text/html},
}

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2021-12-14},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/H99XZZEQ/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/UIFM2NE6/1409.html:text/html},
}

@article{liu_roberta_2019-1,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2021-12-14},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/teetusaini/Zotero/storage/FPK626J4/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/teetusaini/Zotero/storage/4NIYRVAE/1907.html:text/html},
}
