% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{akhtar_threat_2018}{article}{}
      \name{author}{2}{}{%
        {{hash=dae51cf934e0ef7645c0d9bad7d37367}{%
           family={Akhtar},
           familyi={A\bibinitperiod},
           given={Naveed},
           giveni={N\bibinitperiod}}}%
        {{hash=e66ddb661075e1ec2d86e6cc0fa46aef}{%
           family={Mian},
           familyi={M\bibinitperiod},
           given={Ajmal},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \strng{fullhash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \strng{bibnamehash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \strng{authorbibnamehash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \strng{authornamehash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \strng{authorfullhash}{a04ff9a75231fd60c4b81ddb76feec0a}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1801.00553 [cs]}
      \field{month}{2}
      \field{shorttitle}{Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}}
      \field{title}{Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}: {{A Survey}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1801.00553
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/9IGBAP98/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf;/Users/teetusaini/Zotero/storage/H7YLBURW/1801.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{almeida_word_2019-2}{article}{}
      \name{author}{2}{}{%
        {{hash=3f5568726662a2b0b9886666909e8eef}{%
           family={Almeida},
           familyi={A\bibinitperiod},
           given={Felipe},
           giveni={F\bibinitperiod}}}%
        {{hash=d5926506e8908f6071fa97c3c41c80e7}{%
           family={Xexéo},
           familyi={X\bibinitperiod},
           given={Geraldo},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{25aa401aef902b1e6036768e8511c979}
      \strng{fullhash}{25aa401aef902b1e6036768e8511c979}
      \strng{bibnamehash}{25aa401aef902b1e6036768e8511c979}
      \strng{authorbibnamehash}{25aa401aef902b1e6036768e8511c979}
      \strng{authornamehash}{25aa401aef902b1e6036768e8511c979}
      \strng{authorfullhash}{25aa401aef902b1e6036768e8511c979}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1901.09069 [cs, stat]}
      \field{month}{1}
      \field{shorttitle}{Word {{Embeddings}}}
      \field{title}{Word {{Embeddings}}: {{A Survey}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1901.09069
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/HUR5DM7I/Almeida and Xexéo - 2019 - Word Embeddings A Survey.pdf;/Users/teetusaini/Zotero/storage/SJ73EIRT/1901.html
      \endverb
      \keyw{A.1,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7,Statistics - Machine Learning}
    \endentry
    \entry{alzantot_generating_2018}{article}{}
      \name{author}{6}{}{%
        {{hash=c69e9f027393992730c4a8b751f8ecb3}{%
           family={Alzantot},
           familyi={A\bibinitperiod},
           given={Moustafa},
           giveni={M\bibinitperiod}}}%
        {{hash=1349d05adfb40ff0d344ee942288c81d}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Yash},
           giveni={Y\bibinitperiod}}}%
        {{hash=514c5a89329abfd6b45f6f62a2bbcf7a}{%
           family={Elgohary},
           familyi={E\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
        {{hash=8103e305b284a6e69a99f7df814144fb}{%
           family={Ho},
           familyi={H\bibinitperiod},
           given={Bo-Jhang},
           giveni={B\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=8e28df4108c3a8eda6003d3c4da4e044}{%
           family={Srivastava},
           familyi={S\bibinitperiod},
           given={Mani},
           giveni={M\bibinitperiod}}}%
        {{hash=e92742530fcafbae797b45437638bb01}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Kai-Wei},
           giveni={K\bibinithyphendelim W\bibinitperiod}}}%
      }
      \strng{namehash}{c675d8a271a73560315128128e62001c}
      \strng{fullhash}{1fc9a9438c86988aade8463d51c650f9}
      \strng{bibnamehash}{c675d8a271a73560315128128e62001c}
      \strng{authorbibnamehash}{c675d8a271a73560315128128e62001c}
      \strng{authornamehash}{c675d8a271a73560315128128e62001c}
      \strng{authorfullhash}{1fc9a9438c86988aade8463d51c650f9}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1804.07998 [cs]}
      \field{month}{9}
      \field{title}{Generating {{Natural Language Adversarial Examples}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1804.07998
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/YRPEYTAL/Alzantot et al. - 2018 - Generating Natural Language Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/V3U9BX5U/1804.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{bahdanau_neural_2015}{article}{}
      \name{author}{3}{}{%
        {{hash=6d80adec79a13a33e73215c5f46f1605}{%
           family={Bahdanau},
           familyi={B\bibinitperiod},
           given={Dzmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=3da7501a79d9346572c7fd6e41b615df}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Kyunghyun},
           giveni={K\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{fullhash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{bibnamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authorbibnamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authornamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authorfullhash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.}
      \field{journaltitle}{ICLR}
      \field{title}{Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}}
      \field{year}{2015}
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/L7BZJ237/Bahdanau et al. - 2015 - Neural Machine Translation by Jointly Learning to .pdf
      \endverb
    \endentry
    \entry{bao_defending_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=97981803fb038e642234e9ccdc31c5f0}{%
           family={Bao},
           familyi={B\bibinitperiod},
           given={Rongzhou},
           giveni={R\bibinitperiod}}}%
        {{hash=556c8050264b7b80ed069c35bcbbfdd7}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jiayi},
           giveni={J\bibinitperiod}}}%
        {{hash=83dfa007c00ea6000ab8745d0b697520}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Hai},
           giveni={H\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{dbb676d416aaa7fd5865458517faf7c5}
      \strng{fullhash}{dbb676d416aaa7fd5865458517faf7c5}
      \strng{bibnamehash}{dbb676d416aaa7fd5865458517faf7c5}
      \strng{authorbibnamehash}{dbb676d416aaa7fd5865458517faf7c5}
      \strng{authornamehash}{dbb676d416aaa7fd5865458517faf7c5}
      \strng{authorfullhash}{dbb676d416aaa7fd5865458517faf7c5}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021}
      \field{month}{8}
      \field{title}{Defending {{Pre-trained Language Models}} from {{Adversarial Word Substitution Without Performance Sacrifice}}}
      \field{year}{2021}
      \field{pages}{3248\bibrangedash 3258}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18653/v1/2021.findings-acl.287
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/77ASIQR2/Bao et al. - 2021 - Defending Pre-trained Language Models from Adversa.pdf
      \endverb
    \endentry
    \entry{belinkov_synthetic_2018}{article}{}
      \name{author}{2}{}{%
        {{hash=1b1e0145098271245102c2c35a46ffe4}{%
           family={Belinkov},
           familyi={B\bibinitperiod},
           given={Yonatan},
           giveni={Y\bibinitperiod}}}%
        {{hash=4cc4ca13eda1b8380f1589532b8e33ae}{%
           family={Bisk},
           familyi={B\bibinitperiod},
           given={Yonatan},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{e44655a7fbb442efdfec5409668575e6}
      \strng{fullhash}{e44655a7fbb442efdfec5409668575e6}
      \strng{bibnamehash}{e44655a7fbb442efdfec5409668575e6}
      \strng{authorbibnamehash}{e44655a7fbb442efdfec5409668575e6}
      \strng{authornamehash}{e44655a7fbb442efdfec5409668575e6}
      \strng{authorfullhash}{e44655a7fbb442efdfec5409668575e6}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1711.02173 [cs]}
      \field{month}{2}
      \field{title}{Synthetic and {{Natural Noise Both Break Neural Machine Translation}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1711.02173
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/33JV9EGK/Belinkov and Bisk - 2018 - Synthetic and Natural Noise Both Break Neural Mach.pdf;/Users/teetusaini/Zotero/storage/MZUDEQH5/1711.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7}
    \endentry
    \entry{cer_universal_2018}{article}{}
      \name{author}{13}{}{%
        {{hash=73291503692ca44df1071d1d8a59a3a1}{%
           family={Cer},
           familyi={C\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=c3469691a86ad5a26b5cb41949a4846d}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yinfei},
           giveni={Y\bibinitperiod}}}%
        {{hash=23e9ffebfc442055db5f3b1c968467b9}{%
           family={Kong},
           familyi={K\bibinitperiod},
           given={Sheng-yi},
           giveni={S\bibinithyphendelim y\bibinitperiod}}}%
        {{hash=6588cf93a4cebeea6e927fba9622f2f4}{%
           family={Hua},
           familyi={H\bibinitperiod},
           given={Nan},
           giveni={N\bibinitperiod}}}%
        {{hash=8b5db42cb17dbf02e4c813439dc04b69}{%
           family={Limtiaco},
           familyi={L\bibinitperiod},
           given={Nicole},
           giveni={N\bibinitperiod}}}%
        {{hash=f7c5b4794860e40cf375d50102fc55dd}{%
           family={John},
           familyi={J\bibinitperiod},
           given={Rhomni\bibnamedelima St},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=6a8ed325a5622cf4ec06f17777376a1b}{%
           family={Constant},
           familyi={C\bibinitperiod},
           given={Noah},
           giveni={N\bibinitperiod}}}%
        {{hash=94bd1c653d1b3b8418ad02d6053ef1dd}{%
           family={{Guajardo-Cespedes}},
           familyi={G\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod}}}%
        {{hash=f32fdce1c367171ceabdefaa34084051}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Steve},
           giveni={S\bibinitperiod}}}%
        {{hash=e6257b01506614e1078a92b1e26d7402}{%
           family={Tar},
           familyi={T\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=70c1773d78d28313ed822a258ea54a7b}{%
           family={Sung},
           familyi={S\bibinitperiod},
           given={Yun-Hsuan},
           giveni={Y\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=61083c3888f790053cbb1787aff8bc64}{%
           family={Strope},
           familyi={S\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=0adc9b7df5d54aca253a97676a4f9ee3}{%
           family={Kurzweil},
           familyi={K\bibinitperiod},
           given={Ray},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{d6c3eae5665c60f403878a0173452ce1}
      \strng{fullhash}{1a2611a24a8184e979427525bd94b07d}
      \strng{bibnamehash}{d6c3eae5665c60f403878a0173452ce1}
      \strng{authorbibnamehash}{d6c3eae5665c60f403878a0173452ce1}
      \strng{authornamehash}{d6c3eae5665c60f403878a0173452ce1}
      \strng{authorfullhash}{1a2611a24a8184e979427525bd94b07d}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1803.11175 [cs]}
      \field{month}{4}
      \field{title}{Universal {{Sentence Encoder}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1803.11175
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/G3XN23A6/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/Users/teetusaini/Zotero/storage/PZYUIY6N/1803.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{chen_robustness_2019}{article}{}
      \name{author}{6}{}{%
        {{hash=36c4699ac615580682b33548ad271433}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Hongge},
           giveni={H\bibinitperiod}}}%
        {{hash=cfba473a0ef6d69137684e320ea6eac9}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Huan},
           giveni={H\bibinitperiod}}}%
        {{hash=1d9bec203d7a6cdf2651ad8a4cf990c6}{%
           family={Si},
           familyi={S\bibinitperiod},
           given={Si},
           giveni={S\bibinitperiod}}}%
        {{hash=2af761d9bef859ea29248418adce0dd7}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yang},
           giveni={Y\bibinitperiod}}}%
        {{hash=5e88c88571156a888e9d2217371bf882}{%
           family={Boning},
           familyi={B\bibinitperiod},
           given={Duane},
           giveni={D\bibinitperiod}}}%
        {{hash=8ce122ee46af7bc8820d9d1c340793dc}{%
           family={Hsieh},
           familyi={H\bibinitperiod},
           given={Cho-Jui},
           giveni={C\bibinithyphendelim J\bibinitperiod}}}%
      }
      \strng{namehash}{8c21626b54b7f9fe61bbe148b5f3f356}
      \strng{fullhash}{39eae7c8815d251436b2bb90d38e599d}
      \strng{bibnamehash}{8c21626b54b7f9fe61bbe148b5f3f356}
      \strng{authorbibnamehash}{8c21626b54b7f9fe61bbe148b5f3f356}
      \strng{authornamehash}{8c21626b54b7f9fe61bbe148b5f3f356}
      \strng{authorfullhash}{39eae7c8815d251436b2bb90d38e599d}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the robustness verification problem for tree-based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles, the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on the robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. OnRF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than the previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1906.03849 [cs, stat]}
      \field{month}{12}
      \field{title}{Robustness {{Verification}} of {{Tree-based Models}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1906.03849
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/RR25BMEX/Chen et al. - 2019 - Robustness Verification of Tree-based Models.pdf;/Users/teetusaini/Zotero/storage/UZS4KWBJ/1906.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{chowdhury_natural_2003}{article}{}
      \name{author}{1}{}{%
        {{hash=35d516fbcb406eadfb8be8a0af3bbde2}{%
           family={Chowdhury},
           familyi={C\bibinitperiod},
           given={Gobinda\bibnamedelima G.},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \strng{fullhash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \strng{bibnamehash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \strng{authorbibnamehash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \strng{authornamehash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \strng{authorfullhash}{35d516fbcb406eadfb8be8a0af3bbde2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440370103}
      \field{issn}{1550-8382}
      \field{journaltitle}{Annual Review of Information Science and Technology}
      \field{langid}{english}
      \field{number}{1}
      \field{title}{Natural Language Processing}
      \field{volume}{37}
      \field{year}{2003}
      \field{pages}{51\bibrangedash 89}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1002/aris.1440370103
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/BW3UFKUJ/Chowdhury - 2003 - Natural language processing.pdf;/Users/teetusaini/Zotero/storage/ML4HD95B/aris.html
      \endverb
    \endentry
    \entry{devlin_bert_2019-1}{article}{}
      \name{author}{4}{}{%
        {{hash=13202969e372bc82318f9629cbdd199b}{%
           family={Devlin},
           familyi={D\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod}}}%
        {{hash=a45784fe7163b45f11d166564f5d24b6}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Ming-Wei},
           giveni={M\bibinithyphendelim W\bibinitperiod}}}%
        {{hash=8dde73b4194f5bc4230c4808f3fc1534}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kenton},
           giveni={K\bibinitperiod}}}%
        {{hash=b92aa283415413bb8d2a1548716d0c7d}{%
           family={Toutanova},
           familyi={T\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{fullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{bibnamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authorbibnamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authornamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authorfullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1810.04805 [cs]}
      \field{month}{5}
      \field{shorttitle}{{{BERT}}}
      \field{title}{{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1810.04805
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2XYZKUZY/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/teetusaini/Zotero/storage/YDRXA6K7/1810.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{dong_how_2021}{article}{}
      \name{author}{5}{}{%
        {{hash=319ddf495fff6fbf88eae8948d06a46d}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Xinhsuai},
           giveni={X\bibinitperiod}}}%
        {{hash=1199716092382c05331f2501b470db7e}{%
           family={Tuan},
           familyi={T\bibinitperiod},
           given={Luu\bibnamedelima Anh},
           giveni={L\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d2827b813d1d73df44360201e3296bea}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod}}}%
        {{hash=7a7a92b64300d6c39c3ae492b9ded385}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Shuicheng},
           giveni={S\bibinitperiod}}}%
        {{hash=6f410a596e335c299e3550a4e9990bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Hanwang},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{10a19c05ffef0b86262d2509d4725e7f}
      \strng{fullhash}{d44d8f77a125ba462a2aaa1e1fe3a35b}
      \strng{bibnamehash}{10a19c05ffef0b86262d2509d4725e7f}
      \strng{authorbibnamehash}{10a19c05ffef0b86262d2509d4725e7f}
      \strng{authornamehash}{10a19c05ffef0b86262d2509d4725e7f}
      \strng{authorfullhash}{d44d8f77a125ba462a2aaa1e1fe3a35b}
      \field{extraname}{1}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2112.11668 [cs]}
      \field{month}{12}
      \field{title}{How {{Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness}}?}
      \field{year}{2021}
      \verb{eprint}
      \verb 2112.11668
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/YZL59BTI/Dong et al. - 2021 - How Should Pre-Trained Language Models Be Fine-Tun.pdf;/Users/teetusaini/Zotero/storage/2L2JHMCJ/2112.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{dong_towards_2021-2}{article}{}
      \name{author}{4}{}{%
        {{hash=7b17dcca0cf57a88523d630f2712e9bd}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Xinshuai},
           giveni={X\bibinitperiod}}}%
        {{hash=c0917652a29659d0da485f438f2b3c36}{%
           family={Luu},
           familyi={L\bibinitperiod},
           given={Anh\bibnamedelima Tuan},
           giveni={A\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=f0dbd2a2fc01479ff581294d0322b708}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Rongrong},
           giveni={R\bibinitperiod}}}%
        {{hash=44a453a69684789133e3b3393f75ef31}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Hong},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{3cb800eae1105a05ed883b86ddf7fc28}
      \strng{fullhash}{eca0c0b0d55ad794c7e925e322968f6d}
      \strng{bibnamehash}{3cb800eae1105a05ed883b86ddf7fc28}
      \strng{authorbibnamehash}{3cb800eae1105a05ed883b86ddf7fc28}
      \strng{authornamehash}{3cb800eae1105a05ed883b86ddf7fc28}
      \strng{authorfullhash}{eca0c0b0d55ad794c7e925e322968f6d}
      \field{extraname}{2}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either \$l\_2\$-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel \textbackslash textit\{Adversarial Sparse Convex Combination\} (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on the ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, \textbackslash emph\{i.e.\}, sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2107.13541 [cs]}
      \field{month}{7}
      \field{title}{Towards {{Robustness Against Natural Language Word Substitutions}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2107.13541
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/TUC7SVZU/Dong et al. - 2021 - Towards Robustness Against Natural Language Word S.pdf;/Users/teetusaini/Zotero/storage/SYXZMWPA/2107.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{ebrahimi_hotflip_2018}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=8f0ae898e2e3f05092eb4144b6551383}{%
           family={Ebrahimi},
           familyi={E\bibinitperiod},
           given={Javid},
           giveni={J\bibinitperiod}}}%
        {{hash=0eb63bbaf2838d9027cabeb7c9cbb26a}{%
           family={Rao},
           familyi={R\bibinitperiod},
           given={Anyi},
           giveni={A\bibinitperiod}}}%
        {{hash=0b754fc5deaf0a8cd3e41d53d76ec051}{%
           family={Lowd},
           familyi={L\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=ece50b88ec2cd445d40b4195c8f69610}{%
           family={Dou},
           familyi={D\bibinitperiod},
           given={Dejing},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Melbourne, Australia}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{e2e89ebd33daaa024854f25733b31a3e}
      \strng{fullhash}{7c106bae6ffb3ab35f5883c83ee94a3f}
      \strng{bibnamehash}{e2e89ebd33daaa024854f25733b31a3e}
      \strng{authorbibnamehash}{e2e89ebd33daaa024854f25733b31a3e}
      \strng{authornamehash}{e2e89ebd33daaa024854f25733b31a3e}
      \strng{authorfullhash}{7c106bae6ffb3ab35f5883c83ee94a3f}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.}
      \field{booktitle}{Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})}
      \field{month}{7}
      \field{shorttitle}{{{HotFlip}}}
      \field{title}{{{HotFlip}}: {{White-Box Adversarial Examples}} for {{Text Classification}}}
      \field{year}{2018}
      \field{pages}{31\bibrangedash 36}
      \range{pages}{6}
      \verb{doi}
      \verb 10.18653/v1/P18-2006
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/IJRZGNT4/Ebrahimi et al. - 2018 - HotFlip White-Box Adversarial Examples for Text C.pdf
      \endverb
    \endentry
    \entry{eger_text_2019}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=fa4a8f2209aafcafe807835e7d616b7b}{%
           family={Eger},
           familyi={E\bibinitperiod},
           given={Steffen},
           giveni={S\bibinitperiod}}}%
        {{hash=3b8c6b9d5ac2233b662a2a92c5e7ccbd}{%
           family={Şahin},
           familyi={Ş\bibinitperiod},
           given={Gözde\bibnamedelima Gül},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=488e2f1d5b0a1b8ba791ca6d14c87683}{%
           family={Rücklé},
           familyi={R\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=922fb29fc1b05f0f55d0b82065e8d049}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Ji-Ung},
           giveni={J\bibinithyphendelim U\bibinitperiod}}}%
        {{hash=fc13d3b43dc83a85fffe8963da124cbe}{%
           family={Schulz},
           familyi={S\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=242efba381cbfc9ecd79e27f3a19c2ce}{%
           family={Mesgar},
           familyi={M\bibinitperiod},
           given={Mohsen},
           giveni={M\bibinitperiod}}}%
        {{hash=a7369511d6d78511d52db4a8037d617b}{%
           family={Swarnkar},
           familyi={S\bibinitperiod},
           given={Krishnkant},
           giveni={K\bibinitperiod}}}%
        {{hash=9009c5ad711bc358a4b5135d91c849a8}{%
           family={Simpson},
           familyi={S\bibinitperiod},
           given={Edwin},
           giveni={E\bibinitperiod}}}%
        {{hash=ea70b44be1920f6e8c379706bc7d047a}{%
           family={Gurevych},
           familyi={G\bibinitperiod},
           given={Iryna},
           giveni={I\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Minneapolis, Minnesota}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{9559f6cd8b2cc21a1576886837117405}
      \strng{fullhash}{c83714ea2bb924b250af8a233cb0c390}
      \strng{bibnamehash}{9559f6cd8b2cc21a1576886837117405}
      \strng{authorbibnamehash}{9559f6cd8b2cc21a1576886837117405}
      \strng{authornamehash}{9559f6cd8b2cc21a1576886837117405}
      \strng{authorfullhash}{c83714ea2bb924b250af8a233cb0c390}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., ``!d10t'') or as a writing style (``1337'' in ``leet speak''), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82\%. We then explore three shielding methods—visual character embeddings, adversarial training, and rule-based recovery—which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.}
      \field{booktitle}{Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})}
      \field{month}{6}
      \field{shorttitle}{Text {{Processing Like Humans Do}}}
      \field{title}{Text {{Processing Like Humans Do}}: {{Visually Attacking}} and {{Shielding NLP Systems}}}
      \field{year}{2019}
      \field{pages}{1634\bibrangedash 1647}
      \range{pages}{14}
      \verb{doi}
      \verb 10.18653/v1/N19-1165
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/TR5USAIR/Eger et al. - 2019 - Text Processing Like Humans Do Visually Attacking.pdf
      \endverb
    \endentry
    \entry{englesson_consistency_2021}{article}{}
      \name{author}{2}{}{%
        {{hash=dc8d2e294d91e1aff6fb3e1dd9c85143}{%
           family={Englesson},
           familyi={E\bibinitperiod},
           given={Erik},
           giveni={E\bibinitperiod}}}%
        {{hash=c17edee7ec28bf43377cb1536c940e54}{%
           family={Azizpour},
           familyi={A\bibinitperiod},
           given={Hossein},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{34abef9d587bd3623f6e17166f2d8524}
      \strng{fullhash}{34abef9d587bd3623f6e17166f2d8524}
      \strng{bibnamehash}{34abef9d587bd3623f6e17166f2d8524}
      \strng{authorbibnamehash}{34abef9d587bd3623f6e17166f2d8524}
      \strng{authornamehash}{34abef9d587bd3623f6e17166f2d8524}
      \strng{authorfullhash}{34abef9d587bd3623f6e17166f2d8524}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings test-time robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for training-time robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and(ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-of-the-art results.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2110.01242 [cs, stat]}
      \field{month}{10}
      \field{title}{Consistency {{Regularization Can Improve Robustness}} to {{Label Noise}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2110.01242
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/SUUW7WQU/Englesson and Azizpour - 2021 - Consistency Regularization Can Improve Robustness .pdf;/Users/teetusaini/Zotero/storage/N26NRG3E/2110.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{gao_black-box_2018}{article}{}
      \name{author}{4}{}{%
        {{hash=400f8af6b2564a5a72cf91f54d5842cd}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Ji},
           giveni={J\bibinitperiod}}}%
        {{hash=0202b1011024c94e59d5deef99733913}{%
           family={Lanchantin},
           familyi={L\bibinitperiod},
           given={Jack},
           giveni={J\bibinitperiod}}}%
        {{hash=5bb52f32a34cb3507c22128c7d437b12}{%
           family={Soffa},
           familyi={S\bibinitperiod},
           given={Mary\bibnamedelima Lou},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=04b7a7c641a6bc5e81aba25f88ffec13}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Yanjun},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{58cab86e0c7e5fe8aa29eb5f7234aa21}
      \strng{fullhash}{6cf8ed8aa057fcd8ce00f575bd5bce35}
      \strng{bibnamehash}{58cab86e0c7e5fe8aa29eb5f7234aa21}
      \strng{authorbibnamehash}{58cab86e0c7e5fe8aa29eb5f7234aa21}
      \strng{authornamehash}{58cab86e0c7e5fe8aa29eb5f7234aa21}
      \strng{authorfullhash}{6cf8ed8aa057fcd8ce00f575bd5bce35}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68\textbackslash\% on average for a Word-LSTM model and 48\textbackslash\% on average for a Char-CNN model.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1801.04354 [cs]}
      \field{month}{5}
      \field{title}{Black-Box {{Generation}} of {{Adversarial Text Sequences}} to {{Evade Deep Learning Classifiers}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1801.04354
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/EG7T53MX/Gao et al. - 2018 - Black-box Generation of Adversarial Text Sequences.pdf;/Users/teetusaini/Zotero/storage/VBZBNZWC/1801.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Information Retrieval,Computer Science - Machine Learning}
    \endentry
    \entry{garg_bae_2020}{article}{}
      \name{author}{2}{}{%
        {{hash=b866b110ba35e2e6d0423a24b879da4e}{%
           family={Garg},
           familyi={G\bibinitperiod},
           given={Siddhant},
           giveni={S\bibinitperiod}}}%
        {{hash=170d943c213dbe5f93dbe6a071a7b6eb}{%
           family={Ramakrishnan},
           familyi={R\bibinitperiod},
           given={Goutham},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \strng{fullhash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \strng{bibnamehash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \strng{authorbibnamehash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \strng{authornamehash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \strng{authorfullhash}{9f0404d8fc982eb7f4b1e771b813fc8b}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2004.01970 [cs]}
      \field{month}{10}
      \field{shorttitle}{{{BAE}}}
      \field{title}{{{BAE}}: {{BERT-based Adversarial Examples}} for {{Text Classification}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2004.01970
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/JSNHP4XP/Garg and Ramakrishnan - 2020 - BAE BERT-based Adversarial Examples for Text Clas.pdf;/Users/teetusaini/Zotero/storage/DUJWQ6ET/2004.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{goodfellow_explaining_2015}{article}{}
      \name{author}{3}{}{%
        {{hash=d4f74ef4c79f3bb1e51e378184d8850e}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian\bibnamedelima J.},
           giveni={I\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=8f128e70084608a2c29c497ebd794f87}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jonathon},
           giveni={J\bibinitperiod}}}%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \strng{fullhash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \strng{bibnamehash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \strng{authorbibnamehash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \strng{authornamehash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \strng{authorfullhash}{07b2928cf9addb6cf2d09332e9d7ce18}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1412.6572 [cs, stat]}
      \field{month}{3}
      \field{title}{Explaining and {{Harnessing Adversarial Examples}}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1412.6572
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/DRELKALP/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/D5T7IWEX/1412.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{graves_neural_2014}{article}{}
      \name{author}{3}{}{%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=f96594cc1d20df4a987bf8b9770d1ef0}{%
           family={Wayne},
           familyi={W\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=970650e9394fccd4144d4b829505d2b3}{%
           family={Danihelka},
           familyi={D\bibinitperiod},
           given={Ivo},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{fullhash}{a6d94af175112562d0f33e48f97adfff}
      \strng{bibnamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authorbibnamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authornamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authorfullhash}{a6d94af175112562d0f33e48f97adfff}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1410.5401 [cs]}
      \field{month}{12}
      \field{title}{Neural {{Turing Machines}}}
      \field{year}{2014}
      \verb{eprint}
      \verb 1410.5401
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/ANRNY9VB/Graves et al. - 2014 - Neural Turing Machines.pdf;/Users/teetusaini/Zotero/storage/S3HNAZY3/1410.html
      \endverb
      \keyw{Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{guo_rosearch_2021}{article}{}
      \name{author}{5}{}{%
        {{hash=b10fcfadf669bd48a908e04f32864424}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Xin},
           giveni={X\bibinitperiod}}}%
        {{hash=729145c7aea391915799f0f0868d1327}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Jianlei},
           giveni={J\bibinitperiod}}}%
        {{hash=c7def614c5b94aa5499428a9644f024d}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Haoyi},
           giveni={H\bibinitperiod}}}%
        {{hash=9751b60d06a9c630efb6d95db66c7d91}{%
           family={Ye},
           familyi={Y\bibinitperiod},
           given={Xucheng},
           giveni={X\bibinitperiod}}}%
        {{hash=221d23a1cc1702c31044af688c0b7c39}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jianxin},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ece64fb243e879e62ec6eb403bbf2413}
      \strng{fullhash}{cba66cf7e2d384e2e8ca821527ae0a0d}
      \strng{bibnamehash}{ece64fb243e879e62ec6eb403bbf2413}
      \strng{authorbibnamehash}{ece64fb243e879e62ec6eb403bbf2413}
      \strng{authornamehash}{ece64fb243e879e62ec6eb403bbf2413}
      \strng{authorfullhash}{cba66cf7e2d384e2e8ca821527ae0a0d}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7\%\textasciitilde 18\% up to 45.8\%\textasciitilde 47.8\% on different datasets with comparable weight compression ratio to existing distillation methods (4.6\$\textbackslash times\$\textasciitilde 6.5\$\textbackslash times\$ improvement from teacher model BERT\_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2106.03613 [cs]}
      \field{month}{6}
      \field{shorttitle}{{{RoSearch}}}
      \field{title}{{{RoSearch}}: {{Search}} for {{Robust Student Architectures When Distilling Pre-trained Language Models}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2106.03613
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/NESR8SR8/Guo et al. - 2021 - RoSearch Search for Robust Student Architectures .pdf;/Users/teetusaini/Zotero/storage/7Q8E7CWV/2106.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{han_robust_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=b9b26653cdee132c3a587a45ebdbab16}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Wenjuan},
           giveni={W\bibinitperiod}}}%
        {{hash=f954c023cbaefd74cfaba0aa87f35048}{%
           family={Pang},
           familyi={P\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=7d4179fa3081e683ea1525d8fe4e63bb}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Ying\bibnamedelima Nian},
           giveni={Y\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{ded329472706e15eed941c99095ea84c}
      \strng{fullhash}{ded329472706e15eed941c99095ea84c}
      \strng{bibnamehash}{ded329472706e15eed941c99095ea84c}
      \strng{authorbibnamehash}{ded329472706e15eed941c99095ea84c}
      \strng{authornamehash}{ded329472706e15eed941c99095ea84c}
      \strng{authorfullhash}{ded329472706e15eed941c99095ea84c}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.}
      \field{booktitle}{Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})}
      \field{month}{8}
      \field{title}{Robust {{Transfer Learning}} with {{Pretrained Language Models}} through {{Adapters}}}
      \field{year}{2021}
      \field{pages}{854\bibrangedash 861}
      \range{pages}{8}
      \verb{doi}
      \verb 10.18653/v1/2021.acl-short.108
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/LDI8A3F8/Han et al. - 2021 - Robust Transfer Learning with Pretrained Language .pdf
      \endverb
    \endentry
    \entry{harris_distributional_1954}{article}{}
      \name{author}{1}{}{%
        {{hash=c93d2fea5cc5e59f6d92c0cfb5ba20a8}{%
           family={Harris},
           familyi={H\bibinitperiod},
           given={Zellig\bibnamedelima S.},
           giveni={Z\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \strng{fullhash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \strng{bibnamehash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \strng{authorbibnamehash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \strng{authornamehash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \strng{authorfullhash}{c93d2fea5cc5e59f6d92c0cfb5ba20a8}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0043-7956, 2373-5112}
      \field{journaltitle}{\emph{WORD}}
      \field{langid}{english}
      \field{month}{8}
      \field{number}{2-3}
      \field{title}{Distributional {{Structure}}}
      \field{volume}{10}
      \field{year}{1954}
      \field{pages}{146\bibrangedash 162}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1080/00437956.1954.11659520
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/D6LMPG66/Harris - 1954 - Distributional Structure.pdf
      \endverb
    \endentry
    \entry{hinton_distilling_2015}{article}{}
      \name{author}{3}{}{%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=62f954323339abeba6c6a240e9d2855b}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeff},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \strng{fullhash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \strng{bibnamehash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \strng{authorbibnamehash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \strng{authornamehash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \strng{authorfullhash}{ffe4fa3a93a3b1a8ca1d3c8b76f864d7}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1503.02531 [cs, stat]}
      \field{month}{3}
      \field{title}{Distilling the {{Knowledge}} in a {{Neural Network}}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1503.02531
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/HFZ2X9ZD/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/Users/teetusaini/Zotero/storage/EZW9DM3D/1503.html
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{huq_adversarial_2020-1}{article}{}
      \name{author}{2}{}{%
        {{hash=d2b9bc52d75503bf301897ddd90acbee}{%
           family={Huq},
           familyi={H\bibinitperiod},
           given={Aminul},
           giveni={A\bibinitperiod}}}%
        {{hash=329ba3143879b925fd0840347809d12b}{%
           family={Pervin},
           familyi={P\bibinitperiod},
           given={Mst\bibnamedelima Tasnim},
           giveni={M\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \strng{fullhash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \strng{bibnamehash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \strng{authorbibnamehash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \strng{authornamehash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \strng{authorfullhash}{cc55f57da301f7a0a06a86f5fd0afdb2}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis, and many others. However, in recent years it has been shown that these models possess weakness to noises which force the model to misclassify. This issue has been studied profoundly in the image and audio domain. Very little has been studied on this issue concerning textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript, we accumulated and analyzed different attacking techniques and various defense models to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome to move forward in this field.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2005.14108 [cs]}
      \field{month}{6}
      \field{shorttitle}{Adversarial {{Attacks}} and {{Defense}} on {{Texts}}}
      \field{title}{Adversarial {{Attacks}} and {{Defense}} on {{Texts}}: {{A Survey}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2005.14108
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/Z8UZAFI8/Huq and Pervin - 2020 - Adversarial Attacks and Defense on Texts A Survey.pdf;/Users/teetusaini/Zotero/storage/5Y6GSQJP/2005.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{jia_certified_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=7b5fcae550daa1c14f3b61f057f1c373}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Robin},
           giveni={R\bibinitperiod}}}%
        {{hash=7bb584b014f043d94e43a8395735799e}{%
           family={Raghunathan},
           familyi={R\bibinitperiod},
           given={Aditi},
           giveni={A\bibinitperiod}}}%
        {{hash=54bac038946294e04620adbae2c93c78}{%
           family={Göksel},
           familyi={G\bibinitperiod},
           given={Kerem},
           giveni={K\bibinitperiod}}}%
        {{hash=86c032bbc45c3b616f0a1170befc0e82}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{1859b20dde75271da3d85ac76fc071f6}
      \strng{fullhash}{55b89a012db4dfafc267697499ee446c}
      \strng{bibnamehash}{1859b20dde75271da3d85ac76fc071f6}
      \strng{authorbibnamehash}{1859b20dde75271da3d85ac76fc071f6}
      \strng{authornamehash}{1859b20dde75271da3d85ac76fc071f6}
      \strng{authorfullhash}{55b89a012db4dfafc267697499ee446c}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75\textbackslash\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8\textbackslash\%\$ and \$35\textbackslash\%\$, respectively.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1909.00986 [cs]}
      \field{month}{9}
      \field{title}{Certified {{Robustness}} to {{Adversarial Word Substitutions}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1909.00986
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/CWP6P4NX/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf;/Users/teetusaini/Zotero/storage/68BPL5HA/1909.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{jiang_smart_2020-1}{article}{}
      \name{author}{6}{}{%
        {{hash=a5643fa1ae8fe0824b15f38d5bd6a580}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Haoming},
           giveni={H\bibinitperiod}}}%
        {{hash=c53e3e860dbeebd073b0651c1f711567}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Pengcheng},
           giveni={P\bibinitperiod}}}%
        {{hash=05d71a9c5e7ac79222e01a55af0ccb93}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Weizhu},
           giveni={W\bibinitperiod}}}%
        {{hash=b881686cdf68b52745dbcb6374b8008b}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xiaodong},
           giveni={X\bibinitperiod}}}%
        {{hash=159bca5607e3797666654a2e3022978a}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Jianfeng},
           giveni={J\bibinitperiod}}}%
        {{hash=5aabe643c77f92846adf1b83e9c78387}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Tuo},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{820094a027d5d5dae7b0d2da9549eb95}
      \strng{fullhash}{fd2abecbcae8ab53a18ba511cbe06376}
      \strng{bibnamehash}{820094a027d5d5dae7b0d2da9549eb95}
      \strng{authorbibnamehash}{820094a027d5d5dae7b0d2da9549eb95}
      \strng{authornamehash}{820094a027d5d5dae7b0d2da9549eb95}
      \strng{authorfullhash}{fd2abecbcae8ab53a18ba511cbe06376}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research. Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model. To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models. Specifically, our proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the capacity of the model; 2. Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting. Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}
      \field{shorttitle}{{{SMART}}}
      \field{title}{{{SMART}}: {{Robust}} and {{Efficient Fine-Tuning}} for {{Pre-trained Natural Language Models}} through {{Principled Regularized Optimization}}}
      \field{year}{2020}
      \field{pages}{2177\bibrangedash 2190}
      \range{pages}{14}
      \verb{doi}
      \verb 10.18653/v1/2020.acl-main.197
      \endverb
      \verb{eprint}
      \verb 1911.03437
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2WXSMWUY/Jiang et al. - 2020 - SMART Robust and Efficient Fine-Tuning for Pre-tr.pdf;/Users/teetusaini/Zotero/storage/27C47Y7D/1911.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Mathematics - Optimization and Control}
    \endentry
    \entry{jin_is_2020-1}{article}{}
      \name{author}{4}{}{%
        {{hash=2f889329de29e295fcb391586c7978f0}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Di},
           giveni={D\bibinitperiod}}}%
        {{hash=c28d4d196ae83a7396fc9a7a676d25f7}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zhijing},
           giveni={Z\bibinitperiod}}}%
        {{hash=9925875deb29278be816cb57377a7dee}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Joey\bibnamedelima Tianyi},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=e4ec701f3e9b3400f41c7dcb84d2e88a}{%
           family={Szolovits},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{fullhash}{7af79824207f3d18477c7e5a0b68e357}
      \strng{bibnamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authorbibnamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authornamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authorfullhash}{7af79824207f3d18477c7e5a0b68e357}
      \field{extraname}{1}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1907.11932 [cs]}
      \field{month}{4}
      \field{shorttitle}{Is {{BERT Really Robust}}?}
      \field{title}{Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 1907.11932
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/5B9H8NR9/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf;/Users/teetusaini/Zotero/storage/VFK26WA8/1907.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{jin_is_2020}{article}{}
      \name{author}{4}{}{%
        {{hash=2f889329de29e295fcb391586c7978f0}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Di},
           giveni={D\bibinitperiod}}}%
        {{hash=c28d4d196ae83a7396fc9a7a676d25f7}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Zhijing},
           giveni={Z\bibinitperiod}}}%
        {{hash=9925875deb29278be816cb57377a7dee}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Joey\bibnamedelima Tianyi},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=e4ec701f3e9b3400f41c7dcb84d2e88a}{%
           family={Szolovits},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{fullhash}{7af79824207f3d18477c7e5a0b68e357}
      \strng{bibnamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authorbibnamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authornamehash}{89c5d83675f58dc9b08ab863da6caf7b}
      \strng{authorfullhash}{7af79824207f3d18477c7e5a0b68e357}
      \field{extraname}{2}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{2374-3468}
      \field{journaltitle}{Proceedings of the AAAI Conference on Artificial Intelligence}
      \field{langid}{english}
      \field{month}{4}
      \field{number}{05}
      \field{shorttitle}{Is {{BERT Really Robust}}?}
      \field{title}{Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}}
      \field{volume}{34}
      \field{year}{2020}
      \field{pages}{8018\bibrangedash 8025}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1609/aaai.v34i05.6311
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/5V8MIJXJ/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf;/Users/teetusaini/Zotero/storage/K4TFYJ5B/6311.html
      \endverb
    \endentry
    \entry{kardakis_examining_2021}{article}{}
      \name{author}{4}{}{%
        {{hash=de83acebf12cbb64abd72157de2da9f2}{%
           family={Kardakis},
           familyi={K\bibinitperiod},
           given={Spyridon},
           giveni={S\bibinitperiod}}}%
        {{hash=76d008c84120b16a96273aedba304441}{%
           family={Perikos},
           familyi={P\bibinitperiod},
           given={Isidoros},
           giveni={I\bibinitperiod}}}%
        {{hash=46ffb4c27c4688f6928227327b0458ed}{%
           family={Grivokostopoulou},
           familyi={G\bibinitperiod},
           given={Foteini},
           giveni={F\bibinitperiod}}}%
        {{hash=800f763ee1a5a642f018001d641a95d8}{%
           family={Hatzilygeroudis},
           familyi={H\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{96c26f92994dddd7c722be2a22b53cdb}
      \strng{fullhash}{3ca6ccc0f844d30341bdce0e8bfe7a34}
      \strng{bibnamehash}{96c26f92994dddd7c722be2a22b53cdb}
      \strng{authorbibnamehash}{96c26f92994dddd7c722be2a22b53cdb}
      \strng{authornamehash}{96c26f92994dddd7c722be2a22b53cdb}
      \strng{authorfullhash}{3ca6ccc0f844d30341bdce0e8bfe7a34}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models' ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5\% improvement in their accuracy.}
      \field{journaltitle}{Applied Sciences}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{9}
      \field{title}{Examining {{Attention Mechanisms}} in {{Deep Learning Models}} for {{Sentiment Analysis}}}
      \field{volume}{11}
      \field{year}{2021}
      \field{pages}{3883}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/app11093883
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/QGWUZSPA/Kardakis et al. - 2021 - Examining Attention Mechanisms in Deep Learning Mo.pdf
      \endverb
      \keyw{attention mechanism,deep neural networks,global-attention,hierarchical-attention,self-attention,sentiment analysis}
    \endentry
    \entry{laine_temporal_2017}{article}{}
      \name{author}{2}{}{%
        {{hash=e83c74e19bcc032813595328513ebd5c}{%
           family={Laine},
           familyi={L\bibinitperiod},
           given={Samuli},
           giveni={S\bibinitperiod}}}%
        {{hash=70c1210f7c167faac22c68d1db1d9175}{%
           family={Aila},
           familyi={A\bibinitperiod},
           given={Timo},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{be19e92b76e6214ab8bdf9668bba709a}
      \strng{fullhash}{be19e92b76e6214ab8bdf9668bba709a}
      \strng{bibnamehash}{be19e92b76e6214ab8bdf9668bba709a}
      \strng{authorbibnamehash}{be19e92b76e6214ab8bdf9668bba709a}
      \strng{authornamehash}{be19e92b76e6214ab8bdf9668bba709a}
      \strng{authorfullhash}{be19e92b76e6214ab8bdf9668bba709a}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44\% to 7.05\% in SVHN with 500 labels and from 18.63\% to 16.55\% in CIFAR-10 with 4000 labels, and further to 5.12\% and 12.16\% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1610.02242 [cs]}
      \field{month}{3}
      \field{title}{Temporal {{Ensembling}} for {{Semi-Supervised Learning}}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1610.02242
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/V2563NJD/Laine and Aila - 2017 - Temporal Ensembling for Semi-Supervised Learning.pdf;/Users/teetusaini/Zotero/storage/I94RNCRM/1610.html
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{lan_albert_2020}{article}{}
      \name{author}{6}{}{%
        {{hash=924c1ffe49f0aefbeb28e7b15d4e48bb}{%
           family={Lan},
           familyi={L\bibinitperiod},
           given={Zhenzhong},
           giveni={Z\bibinitperiod}}}%
        {{hash=08b5630342daea2e5f628e3f87066e8f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Mingda},
           giveni={M\bibinitperiod}}}%
        {{hash=b284e5d3c52adbcb5bedccea855d56f8}{%
           family={Goodman},
           familyi={G\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=6bc70be3492f0800ecee6926d290244f}{%
           family={Gimpel},
           familyi={G\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=0fc819992cb1cfcc4dcb9ab98220ffb3}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Piyush},
           giveni={P\bibinitperiod}}}%
        {{hash=66796c4fdac5a7416e6d810de13f59f4}{%
           family={Soricut},
           familyi={S\bibinitperiod},
           given={Radu},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{1bbe8a70602dcc259e9d182c69eceb46}
      \strng{fullhash}{f62267fa6c8125f17bfa99dc4be65616}
      \strng{bibnamehash}{1bbe8a70602dcc259e9d182c69eceb46}
      \strng{authorbibnamehash}{1bbe8a70602dcc259e9d182c69eceb46}
      \strng{authornamehash}{1bbe8a70602dcc259e9d182c69eceb46}
      \strng{authorfullhash}{f62267fa6c8125f17bfa99dc4be65616}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \textbackslash squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1909.11942 [cs]}
      \field{month}{2}
      \field{shorttitle}{{{ALBERT}}}
      \field{title}{{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 1909.11942
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2NNW9ILA/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf;/Users/teetusaini/Zotero/storage/KQ53JPPS/1909.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
    \endentry
    \entry{li_textbugger_2019}{article}{}
      \name{author}{5}{}{%
        {{hash=50a7945c11b556f8d5eda5b4e80e1e83}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jinfeng},
           giveni={J\bibinitperiod}}}%
        {{hash=6468302f7741c170edc641669f883ce7}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Shouling},
           giveni={S\bibinitperiod}}}%
        {{hash=9462c64ef6374c9d19bb3d19eea8c942}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Tianyu},
           giveni={T\bibinitperiod}}}%
        {{hash=a9c8728e355996d027a7e56b60820a18}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod}}}%
        {{hash=aa69c3b06b345fb07acdd7c525c45997}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ting},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{a058f1dfd97d29f2c476cb3cbfe69e20}
      \strng{fullhash}{804307dff9da70f65dd083558426bf2b}
      \strng{bibnamehash}{a058f1dfd97d29f2c476cb3cbfe69e20}
      \strng{authorbibnamehash}{a058f1dfd97d29f2c476cb3cbfe69e20}
      \strng{authornamehash}{a058f1dfd97d29f2c476cb3cbfe69e20}
      \strng{authorfullhash}{804307dff9da70f65dd083558426bf2b}
      \field{extraname}{1}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\textbackslash\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\textbackslash\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\textbackslash\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{Proceedings 2019 Network and Distributed System Security Symposium}
      \field{shorttitle}{{{TextBugger}}}
      \field{title}{{{TextBugger}}: {{Generating Adversarial Text Against Real-world Applications}}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.14722/ndss.2019.23138
      \endverb
      \verb{eprint}
      \verb 1812.05271
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/MYDNKSM6/Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf;/Users/teetusaini/Zotero/storage/4C4B5NMU/1812.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
    \endentry
    \entry{li_understanding_2017}{article}{}
      \name{author}{3}{}{%
        {{hash=9e784630a0c25d3ed602dfe506235c30}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jiwei},
           giveni={J\bibinitperiod}}}%
        {{hash=af4e7fdbdccc11aa8a2243afc04916e2}{%
           family={Monroe},
           familyi={M\bibinitperiod},
           given={Will},
           giveni={W\bibinitperiod}}}%
        {{hash=3147296c99a3f829087becd1a4eaec08}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{f92c49b3117d7bb1c223beb19bddd011}
      \strng{fullhash}{f92c49b3117d7bb1c223beb19bddd011}
      \strng{bibnamehash}{f92c49b3117d7bb1c223beb19bddd011}
      \strng{authorbibnamehash}{f92c49b3117d7bb1c223beb19bddd011}
      \strng{authornamehash}{f92c49b3117d7bb1c223beb19bddd011}
      \strng{authorfullhash}{f92c49b3117d7bb1c223beb19bddd011}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1612.08220 [cs]}
      \field{month}{1}
      \field{title}{Understanding {{Neural Networks}} through {{Representation Erasure}}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1612.08220
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/8RFSYGE7/Li et al. - 2017 - Understanding Neural Networks through Representati.pdf;/Users/teetusaini/Zotero/storage/AD2YRS2Z/1612.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{li_bert-attack_2020}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=4c305f909afd2192eedb6f46f0883d14}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
        {{hash=a1229419c173aa01843aa42960f4aae3}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Ruotian},
           giveni={R\bibinitperiod}}}%
        {{hash=b0a2cf877428e8682e00b9ae6089a38e}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Qipeng},
           giveni={Q\bibinitperiod}}}%
        {{hash=bf518624a157d0b98008549d2f394d6e}{%
           family={Xue},
           familyi={X\bibinitperiod},
           given={Xiangyang},
           giveni={X\bibinitperiod}}}%
        {{hash=9057ec1bc6c2faaf12dfb4b37a8d37d0}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Xipeng},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{24171f47d015b0997d5f9bc82b011eae}
      \strng{fullhash}{7d295f14b86a9f9e09d31f246d690d40}
      \strng{bibnamehash}{24171f47d015b0997d5f9bc82b011eae}
      \strng{authorbibnamehash}{24171f47d015b0997d5f9bc82b011eae}
      \strng{authornamehash}{24171f47d015b0997d5f9bc82b011eae}
      \strng{authorfullhash}{7d295f14b86a9f9e09d31f246d690d40}
      \field{extraname}{2}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Adversarial attacks for discrete data (such as text) has been proved significantly more challenging than continuous data (such as image), since it is difficult to generate adversarial samples with gradient-based methods. Currently, the successful attack methods for text usually adopt heuristic replacement strategies on character or word level, which remains challenging to find the optimal solution in the massive space of possible combination of replacements, while preserving semantic consistency and language fluency. In this paper, we propose \textbackslash textbf\{BERT-Attack\}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models for downstream tasks. Our method successfully misleads the target models to predict incorrectly, outperforming state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations.}
      \field{booktitle}{{{EMNLP}}}
      \field{shorttitle}{{{BERT-ATTACK}}}
      \field{title}{{{BERT-ATTACK}}: {{Adversarial Attack Against BERT Using BERT}}}
      \field{year}{2020}
      \verb{doi}
      \verb 10.18653/v1/2020.emnlp-main.500
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/H7LCX3UC/Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf
      \endverb
    \endentry
    \entry{liu_adversarial_2020}{article}{}
      \name{author}{7}{}{%
        {{hash=b881686cdf68b52745dbcb6374b8008b}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xiaodong},
           giveni={X\bibinitperiod}}}%
        {{hash=f72448b448f784a30f7d333a8eee90be}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
        {{hash=c53e3e860dbeebd073b0651c1f711567}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Pengcheng},
           giveni={P\bibinitperiod}}}%
        {{hash=05d71a9c5e7ac79222e01a55af0ccb93}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Weizhu},
           giveni={W\bibinitperiod}}}%
        {{hash=b2eccb185095fccfeb40fdffd8b2105f}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=3e2b0bc20e35ee839c381d54ed997131}{%
           family={Poon},
           familyi={P\bibinitperiod},
           given={Hoifung},
           giveni={H\bibinitperiod}}}%
        {{hash=159bca5607e3797666654a2e3022978a}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Jianfeng},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{0ee583db8604a3d3171065164778aa7f}
      \strng{fullhash}{de6bc293e8320c509ce280c9b28fa0f7}
      \strng{bibnamehash}{0ee583db8604a3d3171065164778aa7f}
      \strng{authorbibnamehash}{0ee583db8604a3d3171065164778aa7f}
      \strng{authornamehash}{0ee583db8604a3d3171065164778aa7f}
      \strng{authorfullhash}{de6bc293e8320c509ce280c9b28fa0f7}
      \field{extraname}{1}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2004.08994 [cs]}
      \field{month}{4}
      \field{title}{Adversarial {{Training}} for {{Large Neural Language Models}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2004.08994
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/SZEBKH6F/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf;/Users/teetusaini/Zotero/storage/BG5HQ56P/2004.html
      \endverb
      \keyw{Computer Science - Computation and Language,Done}
    \endentry
    \entry{liu_roberta_2019-1}{article}{}
      \name{author}{10}{}{%
        {{hash=94530daf7a9db4abe89cfb22935373eb}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yinhan},
           giveni={Y\bibinitperiod}}}%
        {{hash=b13aa2bad5258fa5ef7a733f2d305a49}{%
           family={Ott},
           familyi={O\bibinitperiod},
           given={Myle},
           giveni={M\bibinitperiod}}}%
        {{hash=78a420900f50a8470ad085fb05231bf6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Naman},
           giveni={N\bibinitperiod}}}%
        {{hash=e483b52ce2cd3ba2fd08e7bf4fef4ff0}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Jingfei},
           giveni={J\bibinitperiod}}}%
        {{hash=e5bfa941bcdbfecb5f3e42bb6ac9eb5f}{%
           family={Joshi},
           familyi={J\bibinitperiod},
           given={Mandar},
           giveni={M\bibinitperiod}}}%
        {{hash=aed188db3fc6855ac5666aa14e04a3bb}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Danqi},
           giveni={D\bibinitperiod}}}%
        {{hash=038965e189161e03f6255a4278c280a1}{%
           family={Levy},
           familyi={L\bibinitperiod},
           given={Omer},
           giveni={O\bibinitperiod}}}%
        {{hash=c97f53fdeeb04c28de19e3324cb4bc0a}{%
           family={Lewis},
           familyi={L\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod}}}%
        {{hash=1dbd3a5b42828fb2cebd7786488ba425}{%
           family={Zettlemoyer},
           familyi={Z\bibinitperiod},
           given={Luke},
           giveni={L\bibinitperiod}}}%
        {{hash=9b97236dd2d1a104fc5cf9660dd5582e}{%
           family={Stoyanov},
           familyi={S\bibinitperiod},
           given={Veselin},
           giveni={V\bibinitperiod}}}%
      }
      \strng{namehash}{bbac34a28b7aba5cd543f7c72e0d987d}
      \strng{fullhash}{7579c765ce38b2c34c8f9768ec3bc342}
      \strng{bibnamehash}{bbac34a28b7aba5cd543f7c72e0d987d}
      \strng{authorbibnamehash}{bbac34a28b7aba5cd543f7c72e0d987d}
      \strng{authornamehash}{bbac34a28b7aba5cd543f7c72e0d987d}
      \strng{authorfullhash}{7579c765ce38b2c34c8f9768ec3bc342}
      \field{extraname}{2}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1907.11692 [cs]}
      \field{month}{7}
      \field{shorttitle}{{{RoBERTa}}}
      \field{title}{{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1907.11692
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/ZYKXTIKI/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/teetusaini/Zotero/storage/GKMJMCIG/1907.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{luong_effective_2015}{article}{}
      \name{author}{3}{}{%
        {{hash=5fc2dc715fc35ea0dd825a5d84ac0e60}{%
           family={Luong},
           familyi={L\bibinitperiod},
           given={Minh-Thang},
           giveni={M\bibinithyphendelim T\bibinitperiod}}}%
        {{hash=c7225c8adf8195ef056d44ae3f589af2}{%
           family={Pham},
           familyi={P\bibinitperiod},
           given={Hieu},
           giveni={H\bibinitperiod}}}%
        {{hash=2214edb8305f7ccd7cdc310b3a8ae1b4}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher\bibnamedelima D.},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{04efcca29e42bdde7dee2684159e2f4b}
      \strng{fullhash}{04efcca29e42bdde7dee2684159e2f4b}
      \strng{bibnamehash}{04efcca29e42bdde7dee2684159e2f4b}
      \strng{authorbibnamehash}{04efcca29e42bdde7dee2684159e2f4b}
      \strng{authornamehash}{04efcca29e42bdde7dee2684159e2f4b}
      \strng{authorfullhash}{04efcca29e42bdde7dee2684159e2f4b}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1508.04025 [cs]}
      \field{month}{9}
      \field{title}{Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1508.04025
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/GZ2B2SVV/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/Users/teetusaini/Zotero/storage/S6VIW2IL/1508.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{ma_nlpaug_2022}{misc}{}
      \name{author}{1}{}{%
        {{hash=54a35d468a3be3327b9472a055dccda1}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{54a35d468a3be3327b9472a055dccda1}
      \strng{fullhash}{54a35d468a3be3327b9472a055dccda1}
      \strng{bibnamehash}{54a35d468a3be3327b9472a055dccda1}
      \strng{authorbibnamehash}{54a35d468a3be3327b9472a055dccda1}
      \strng{authornamehash}{54a35d468a3be3327b9472a055dccda1}
      \strng{authorfullhash}{54a35d468a3be3327b9472a055dccda1}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Data augmentation for NLP}
      \field{howpublished}{https://github.com/makcedward/nlpaug}
      \field{month}{1}
      \field{shorttitle}{Data Augmentation for {{NLP}} Python Package.}
      \field{title}{Nlpaug}
      \field{year}{2022}
      \keyw{adversarial-attacks,adversarial-example,ai,artificial-intelligence,augmentation,data-science,machine-learning,ml,natural-language-processing,nlp}
    \endentry
    \entry{maas_learning_2011-1}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=ed7b150d84a40a9f9067abd89a23a133}{%
           family={Maas},
           familyi={M\bibinitperiod},
           given={Andrew\bibnamedelima L.},
           giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=dbd1c8d049a82506383a95d0362e99d2}{%
           family={Daly},
           familyi={D\bibinitperiod},
           given={Raymond\bibnamedelima E.},
           giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=9909a10a7faf66b4f1a72afe398d3f8c}{%
           family={Pham},
           familyi={P\bibinitperiod},
           given={Peter\bibnamedelima T.},
           giveni={P\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=723f50cb8399170cf7a83a00b73da4f8}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=49e889356ff39df159461bc2895c7e16}{%
           family={Ng},
           familyi={N\bibinitperiod},
           given={Andrew\bibnamedelima Y.},
           giveni={A\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=699d626ef41a1a827d623529c1a45e5a}{%
           family={Potts},
           familyi={P\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Portland, Oregon, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{17c0fb1c72658866046c6d044b7821d5}
      \strng{fullhash}{52af3773bc6f449fc99b5d9a9d72cb5b}
      \strng{bibnamehash}{17c0fb1c72658866046c6d044b7821d5}
      \strng{authorbibnamehash}{17c0fb1c72658866046c6d044b7821d5}
      \strng{authornamehash}{17c0fb1c72658866046c6d044b7821d5}
      \strng{authorfullhash}{52af3773bc6f449fc99b5d9a9d72cb5b}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}}
      \field{month}{6}
      \field{title}{Learning {{Word Vectors}} for {{Sentiment Analysis}}}
      \field{year}{2011}
      \field{pages}{142\bibrangedash 150}
      \range{pages}{9}
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/MWXVSZ4B/Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf
      \endverb
    \endentry
    \entry{malkiel_mml_2019}{article}{}
      \name{author}{2}{}{%
        {{hash=318b53983027d949b5fc28ad22b5f08c}{%
           family={Malkiel},
           familyi={M\bibinitperiod},
           given={Itzik},
           giveni={I\bibinitperiod}}}%
        {{hash=f70d0dee88ad32fb0faf165ab5d0e9ab}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Lior},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{ca139f62f9c1236a3c454b38a948f8a9}
      \strng{fullhash}{ca139f62f9c1236a3c454b38a948f8a9}
      \strng{bibnamehash}{ca139f62f9c1236a3c454b38a948f8a9}
      \strng{authorbibnamehash}{ca139f62f9c1236a3c454b38a948f8a9}
      \strng{authornamehash}{ca139f62f9c1236a3c454b38a948f8a9}
      \strng{authorfullhash}{ca139f62f9c1236a3c454b38a948f8a9}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent state-of-the-art language models utilize a two-phase training procedure comprised of (i) unsupervised pre-training on unlabeled text, and (ii) fine-tuning for a specific supervised task. More recently, many studies have been focused on trying to improve these models by enhancing the pre-training phase, either via better choice of hyperparameters or by leveraging an improved formulation. However, the pre-training phase is computationally expensive and often done on private datasets. In this work, we present a method that leverages BERT's fine-tuning phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. Our method allows the model to converge to an optimal number of parallel classifiers, depending on the given dataset at hand. We conduct an extensive inter- and intra-dataset evaluations, showing that our method improves the robustness of BERT, sometimes leading to a +9\textbackslash\% gain in accuracy. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary and our models will be made completely public.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1911.06182 [cs, stat]}
      \field{month}{11}
      \field{shorttitle}{{{MML}}}
      \field{title}{{{MML}}: {{Maximal Multiverse Learning}} for {{Robust Fine-Tuning}} of {{Language Models}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1911.06182
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/3GL95R6L/Malkiel and Wolf - 2019 - MML Maximal Multiverse Learning for Robust Fine-T.pdf;/Users/teetusaini/Zotero/storage/CP5UNZ4J/1911.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{mikolov_efficient_2013}{article}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{d4849f6d57e38d9b9846cfb360e45131}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{d4849f6d57e38d9b9846cfb360e45131}
      \strng{authorbibnamehash}{d4849f6d57e38d9b9846cfb360e45131}
      \strng{authornamehash}{d4849f6d57e38d9b9846cfb360e45131}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1301.3781 [cs]}
      \field{month}{9}
      \field{title}{Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}}
      \field{year}{2013}
      \verb{eprint}
      \verb 1301.3781
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/M9HIIZIU/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/teetusaini/Zotero/storage/4IXDPT6P/1301.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{miller_wordnet_1995}{article}{}
      \name{author}{1}{}{%
        {{hash=52cf88006a41a8cd9aab3bcb5e428b80}{%
           family={Miller},
           familyi={M\bibinitperiod},
           given={George\bibnamedelima A.},
           giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \strng{fullhash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \strng{bibnamehash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \strng{authorbibnamehash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \strng{authornamehash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \strng{authorfullhash}{52cf88006a41a8cd9aab3bcb5e428b80}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].}
      \field{issn}{0001-0782}
      \field{journaltitle}{Communications of the ACM}
      \field{month}{11}
      \field{number}{11}
      \field{shorttitle}{{{WordNet}}}
      \field{title}{{{WordNet}}: A Lexical Database for {{English}}}
      \field{volume}{38}
      \field{year}{1995}
      \field{pages}{39\bibrangedash 41}
      \range{pages}{3}
      \verb{doi}
      \verb 10.1145/219717.219748
      \endverb
    \endentry
    \entry{miyato_adversarial_2017}{article}{}
      \name{author}{3}{}{%
        {{hash=8457e939735fb91de3d84cc0637781d3}{%
           family={Miyato},
           familyi={M\bibinitperiod},
           given={Takeru},
           giveni={T\bibinitperiod}}}%
        {{hash=978bf3b58698c55cce487279eb72f59f}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Andrew\bibnamedelima M.},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{7e58d389b92b5f5e0307bef583867c19}
      \strng{fullhash}{7e58d389b92b5f5e0307bef583867c19}
      \strng{bibnamehash}{7e58d389b92b5f5e0307bef583867c19}
      \strng{authorbibnamehash}{7e58d389b92b5f5e0307bef583867c19}
      \strng{authornamehash}{7e58d389b92b5f5e0307bef583867c19}
      \strng{authorfullhash}{7e58d389b92b5f5e0307bef583867c19}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1605.07725 [cs, stat]}
      \field{month}{5}
      \field{title}{Adversarial {{Training Methods}} for {{Semi-Supervised Text Classification}}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1605.07725
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/JS2XZ39B/Miyato et al. - 2017 - Adversarial Training Methods for Semi-Supervised T.pdf;/Users/teetusaini/Zotero/storage/BI9FJVAE/1605.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{miyato_virtual_2018}{article}{}
      \name{author}{4}{}{%
        {{hash=8457e939735fb91de3d84cc0637781d3}{%
           family={Miyato},
           familyi={M\bibinitperiod},
           given={Takeru},
           giveni={T\bibinitperiod}}}%
        {{hash=b94d20c02901b9c67b805ec083b4d23b}{%
           family={Maeda},
           familyi={M\bibinitperiod},
           given={Shin-ichi},
           giveni={S\bibinithyphendelim i\bibinitperiod}}}%
        {{hash=d18b784f7f35111bc05bcbd245a34ce7}{%
           family={Koyama},
           familyi={K\bibinitperiod},
           given={Masanori},
           giveni={M\bibinitperiod}}}%
        {{hash=ec4440c20df83d1d2a1b6c0cb694c23b}{%
           family={Ishii},
           familyi={I\bibinitperiod},
           given={Shin},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{9ea3399b53c2fde34fbc1ee4d3bc1a7a}
      \strng{fullhash}{80989db184a683587dc99c6079a84994}
      \strng{bibnamehash}{9ea3399b53c2fde34fbc1ee4d3bc1a7a}
      \strng{authorbibnamehash}{9ea3399b53c2fde34fbc1ee4d3bc1a7a}
      \strng{authornamehash}{9ea3399b53c2fde34fbc1ee4d3bc1a7a}
      \strng{authorfullhash}{80989db184a683587dc99c6079a84994}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1704.03976 [cs, stat]}
      \field{month}{6}
      \field{shorttitle}{Virtual {{Adversarial Training}}}
      \field{title}{Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi-Supervised Learning}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1704.03976
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/W9ETVEI8/Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf;/Users/teetusaini/Zotero/storage/7PTPI7YX/1704.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{moradi_evaluating_2021-1}{article}{}
      \name{author}{2}{}{%
        {{hash=e2e49dda8cbc839b457e0495a4faf500}{%
           family={Moradi},
           familyi={M\bibinitperiod},
           given={Milad},
           giveni={M\bibinitperiod}}}%
        {{hash=866a9f2b0090e6a46aab482434be3165}{%
           family={Samwald},
           familyi={S\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \strng{fullhash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \strng{bibnamehash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \strng{authorbibnamehash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \strng{authornamehash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \strng{authorfullhash}{5c893d4d2a29da3b5db65f7e4293bc78}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems robustness.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2108.12237 [cs]}
      \field{month}{8}
      \field{title}{Evaluating the {{Robustness}} of {{Neural Language Models}} to {{Input Perturbations}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2108.12237
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/TKNU9UAS/Moradi and Samwald - 2021 - Evaluating the Robustness of Neural Language Model.pdf;/Users/teetusaini/Zotero/storage/F38G8BRM/2108.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
    \endentry
    \entry{morris_textattack_2020}{article}{}
      \name{author}{6}{}{%
        {{hash=c197d948964c01e6ac22ea8605684b51}{%
           family={Morris},
           familyi={M\bibinitperiod},
           given={John\bibnamedelima X.},
           giveni={J\bibinitperiod\bibinitdelim X\bibinitperiod}}}%
        {{hash=efbf56d459a5f7190e3094d07aced977}{%
           family={Lifland},
           familyi={L\bibinitperiod},
           given={Eli},
           giveni={E\bibinitperiod}}}%
        {{hash=e84651053d2c6a041977ff7f3daecfdd}{%
           family={Yoo},
           familyi={Y\bibinitperiod},
           given={Jin\bibnamedelima Yong},
           giveni={J\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=ea355db3c4742d931e4581e0949ec411}{%
           family={Grigsby},
           familyi={G\bibinitperiod},
           given={Jake},
           giveni={J\bibinitperiod}}}%
        {{hash=2f889329de29e295fcb391586c7978f0}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Di},
           giveni={D\bibinitperiod}}}%
        {{hash=04b7a7c641a6bc5e81aba25f88ffec13}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Yanjun},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{93743829c616c8dc24810fce1ba4a193}
      \strng{fullhash}{00e9d350113983004e9c0de2871d6453}
      \strng{bibnamehash}{93743829c616c8dc24810fce1ba4a193}
      \strng{authorbibnamehash}{93743829c616c8dc24810fce1ba4a193}
      \strng{authornamehash}{93743829c616c8dc24810fce1ba4a193}
      \strng{authorfullhash}{00e9d350113983004e9c0de2871d6453}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2005.05909 [cs]}
      \field{month}{10}
      \field{shorttitle}{{{TextAttack}}}
      \field{title}{{{TextAttack}}: {{A Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2005.05909
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2LDHB8U7/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf;/Users/teetusaini/Zotero/storage/IJBNNZEH/2005.html
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{mozes_frequency-guided_2021}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=4ea35322f3c54e153670f5905180f26b}{%
           family={Mozes},
           familyi={M\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=e154114a2a329ffadc304f4cc24b34da}{%
           family={Stenetorp},
           familyi={S\bibinitperiod},
           given={Pontus},
           giveni={P\bibinitperiod}}}%
        {{hash=af5985790abb6d09f5a39104e7c5e4b3}{%
           family={Kleinberg},
           familyi={K\bibinitperiod},
           given={Bennett},
           giveni={B\bibinitperiod}}}%
        {{hash=a3cba2acc96d203d05cc76903a54d829}{%
           family={Griffin},
           familyi={G\bibinitperiod},
           given={Lewis},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{b95e77d93b75353dae46c24477273528}
      \strng{fullhash}{9f90221bc2d5f7efd42fe8d0c2b1a7fd}
      \strng{bibnamehash}{b95e77d93b75353dae46c24477273528}
      \strng{authorbibnamehash}{b95e77d93b75353dae46c24477273528}
      \strng{authornamehash}{b95e77d93b75353dae46c24477273528}
      \strng{authorfullhash}{9f90221bc2d5f7efd42fe8d0c2b1a7fd}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4\% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0\% F1.}
      \field{booktitle}{Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}}
      \field{month}{4}
      \field{title}{Frequency-{{Guided Word Substitutions}} for {{Detecting Textual Adversarial Examples}}}
      \field{year}{2021}
      \field{pages}{171\bibrangedash 186}
      \range{pages}{16}
      \verb{doi}
      \verb 10.18653/v1/2021.eacl-main.13
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/4LRMQN94/Mozes et al. - 2021 - Frequency-Guided Word Substitutions for Detecting .pdf
      \endverb
    \endentry
    \entry{nadaraya_estimating_1964}{article}{}
      \name{author}{1}{}{%
        {{hash=7c96cb892e3d709febb05f732aafb736}{%
           family={Nadaraya},
           familyi={N\bibinitperiod},
           given={E.\bibnamedelimi A.},
           giveni={E\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Society for Industrial and Applied Mathematics}%
      }
      \strng{namehash}{7c96cb892e3d709febb05f732aafb736}
      \strng{fullhash}{7c96cb892e3d709febb05f732aafb736}
      \strng{bibnamehash}{7c96cb892e3d709febb05f732aafb736}
      \strng{authorbibnamehash}{7c96cb892e3d709febb05f732aafb736}
      \strng{authornamehash}{7c96cb892e3d709febb05f732aafb736}
      \strng{authorfullhash}{7c96cb892e3d709febb05f732aafb736}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly.}
      \field{issn}{0040-585X}
      \field{journaltitle}{Theory of Probability \& Its Applications}
      \field{month}{1}
      \field{number}{1}
      \field{title}{On {{Estimating Regression}}}
      \field{volume}{9}
      \field{year}{1964}
      \field{pages}{141\bibrangedash 142}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1137/1109020
      \endverb
    \endentry
    \entry{naseem_comprehensive_2020}{article}{}
      \name{author}{4}{}{%
        {{hash=88d3f1b75f756938113e439b70b8f245}{%
           family={Naseem},
           familyi={N\bibinitperiod},
           given={Usman},
           giveni={U\bibinitperiod}}}%
        {{hash=fd387fe52d796956a92df0795137307c}{%
           family={Razzak},
           familyi={R\bibinitperiod},
           given={Imran},
           giveni={I\bibinitperiod}}}%
        {{hash=1acc60fdef3798c20d72a4dcd3dc190a}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Shah\bibnamedelima Khalid},
           giveni={S\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=68301785f41847cc41bbc2a5227c0016}{%
           family={Prasad},
           familyi={P\bibinitperiod},
           given={Mukesh},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{b2eab09b8d9321d60d57c2fab41ffa35}
      \strng{fullhash}{0fd9dd91f572865a7df4c2709a44116e}
      \strng{bibnamehash}{b2eab09b8d9321d60d57c2fab41ffa35}
      \strng{authorbibnamehash}{b2eab09b8d9321d60d57c2fab41ffa35}
      \strng{authornamehash}{b2eab09b8d9321d60d57c2fab41ffa35}
      \strng{authorfullhash}{0fd9dd91f572865a7df4c2709a44116e}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2010.15036 [cs]}
      \field{month}{10}
      \field{shorttitle}{A {{Comprehensive Survey}} on {{Word Representation Models}}}
      \field{title}{A {{Comprehensive Survey}} on {{Word Representation Models}}: {{From Classical}} to {{State-Of-The-Art Word Representation Language Models}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2010.15036
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/HGTIW5KJ/Naseem et al. - 2020 - A Comprehensive Survey on Word Representation Mode.pdf;/Users/teetusaini/Zotero/storage/AKT3P9RD/2010.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{nicolae_adversarial_2019}{article}{}
      \name{author}{12}{}{%
        {{hash=b24d419d1ef49e70eed08bd877f2c44f}{%
           family={Nicolae},
           familyi={N\bibinitperiod},
           given={Maria-Irina},
           giveni={M\bibinithyphendelim I\bibinitperiod}}}%
        {{hash=3dd9443041bef001e148133f949c1492}{%
           family={Sinn},
           familyi={S\bibinitperiod},
           given={Mathieu},
           giveni={M\bibinitperiod}}}%
        {{hash=74520b14ca9d8d88b1db7cdf2a71edc6}{%
           family={Tran},
           familyi={T\bibinitperiod},
           given={Minh\bibnamedelima Ngoc},
           giveni={M\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=a89cb5f708523b553857bb088c9fbb4c}{%
           family={Buesser},
           familyi={B\bibinitperiod},
           given={Beat},
           giveni={B\bibinitperiod}}}%
        {{hash=cebb99656cc03afe64fc739394a8fcc7}{%
           family={Rawat},
           familyi={R\bibinitperiod},
           given={Ambrish},
           giveni={A\bibinitperiod}}}%
        {{hash=a78fe4f292957ed0686ef668ef87048e}{%
           family={Wistuba},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=0989a6f32e84829ee6fd57b115007ec3}{%
           family={Zantedeschi},
           familyi={Z\bibinitperiod},
           given={Valentina},
           giveni={V\bibinitperiod}}}%
        {{hash=172345c5b8261672f0363451807ed6e8}{%
           family={Baracaldo},
           familyi={B\bibinitperiod},
           given={Nathalie},
           giveni={N\bibinitperiod}}}%
        {{hash=903eab21851d8f8e7e7c04e15fcc3abf}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Bryant},
           giveni={B\bibinitperiod}}}%
        {{hash=841e1fc6fc03d5b35f91dbae6b14e16e}{%
           family={Ludwig},
           familyi={L\bibinitperiod},
           given={Heiko},
           giveni={H\bibinitperiod}}}%
        {{hash=12ca53b2ecfef2befe9403f16e0e86aa}{%
           family={Molloy},
           familyi={M\bibinitperiod},
           given={Ian\bibnamedelima M.},
           giveni={I\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=be88d24130f141fc3d3ec85a707255af}{%
           family={Edwards},
           familyi={E\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{2c492dd0dccf99229bdf078129a25bd2}
      \strng{fullhash}{02921753739eba446683b703f1ce1c1c}
      \strng{bibnamehash}{2c492dd0dccf99229bdf078129a25bd2}
      \strng{authorbibnamehash}{2c492dd0dccf99229bdf078129a25bd2}
      \strng{authornamehash}{2c492dd0dccf99229bdf078129a25bd2}
      \strng{authorfullhash}{02921753739eba446683b703f1ce1c1c}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1807.01069 [cs, stat]}
      \field{month}{11}
      \field{title}{Adversarial {{Robustness Toolbox}} v1.0.0}
      \field{year}{2019}
      \verb{eprint}
      \verb 1807.01069
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2RZSIXT4/Nicolae et al. - 2019 - Adversarial Robustness Toolbox v1.0.0.pdf;/Users/teetusaini/Zotero/storage/I9FMDFWC/1807.html
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{papernot_crafting_2016}{article}{}
      \name{author}{4}{}{%
        {{hash=58ca864d11bbe53214716bb8177beab9}{%
           family={Papernot},
           familyi={P\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
        {{hash=d011e9ddf088732101fe34c710b24617}{%
           family={McDaniel},
           familyi={M\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=b91873cac919469343e271d452b726b3}{%
           family={Swami},
           familyi={S\bibinitperiod},
           given={Ananthram},
           giveni={A\bibinitperiod}}}%
        {{hash=e1c35214b93eec5836a76908cd3d5e18}{%
           family={Harang},
           familyi={H\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{9085ed4e8c2d74ca7918f23e1db31d8c}
      \strng{fullhash}{7f766a8778d9d5629906cb9921a99597}
      \strng{bibnamehash}{9085ed4e8c2d74ca7918f23e1db31d8c}
      \strng{authorbibnamehash}{9085ed4e8c2d74ca7918f23e1db31d8c}
      \strng{authornamehash}{9085ed4e8c2d74ca7918f23e1db31d8c}
      \strng{authorfullhash}{7f766a8778d9d5629906cb9921a99597}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1604.08275 [cs]}
      \field{month}{4}
      \field{title}{Crafting {{Adversarial Input Sequences}} for {{Recurrent Neural Networks}}}
      \field{year}{2016}
      \verb{eprint}
      \verb 1604.08275
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/KRYTNPGN/Papernot et al. - 2016 - Crafting Adversarial Input Sequences for Recurrent.pdf;/Users/teetusaini/Zotero/storage/M2EQRRC4/1604.html
      \endverb
      \keyw{Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{patwa_fighting_2021}{article}{}
      \name{author}{9}{}{%
        {{hash=25c882fc0a4ad6a408a7dd67dfbe41b9}{%
           family={Patwa},
           familyi={P\bibinitperiod},
           given={Parth},
           giveni={P\bibinitperiod}}}%
        {{hash=f3fae123e7a7789e270d29c850b5f6c3}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Shivam},
           giveni={S\bibinitperiod}}}%
        {{hash=581bf5ae3b859e3833f4947c4881dcbb}{%
           family={Pykl},
           familyi={P\bibinitperiod},
           given={Srinivas},
           giveni={S\bibinitperiod}}}%
        {{hash=94d58f484bdb8beae98b52a3f435e227}{%
           family={Guptha},
           familyi={G\bibinitperiod},
           given={Vineeth},
           giveni={V\bibinitperiod}}}%
        {{hash=6669a05b68bb4d04330a9c6504a615d1}{%
           family={Kumari},
           familyi={K\bibinitperiod},
           given={Gitanjali},
           giveni={G\bibinitperiod}}}%
        {{hash=dbe5532cd643bc6511330e076c8b65cf}{%
           family={Akhtar},
           familyi={A\bibinitperiod},
           given={Md\bibnamedelima Shad},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=22be816ef80eb53773fd2ed1b59abf1b}{%
           family={Ekbal},
           familyi={E\bibinitperiod},
           given={Asif},
           giveni={A\bibinitperiod}}}%
        {{hash=8e286eaf7f7861d5efa03fb385e027d8}{%
           family={Das},
           familyi={D\bibinitperiod},
           given={Amitava},
           giveni={A\bibinitperiod}}}%
        {{hash=12ef4925945d218592da34e0e46c4eb4}{%
           family={Chakraborty},
           familyi={C\bibinitperiod},
           given={Tanmoy},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{981142a4c61b680ec1693865bda28947}
      \strng{fullhash}{1c7d35049810583137d93da315d6221f}
      \strng{bibnamehash}{981142a4c61b680ec1693865bda28947}
      \strng{authorbibnamehash}{981142a4c61b680ec1693865bda28947}
      \strng{authornamehash}{981142a4c61b680ec1693865bda28947}
      \strng{authorfullhash}{1c7d35049810583137d93da315d6221f}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\textbackslash\% F1-score with SVM. The data and code is available at: https://github.com/parthpatwa/covid19-fake-news-dectection}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2011.03327 [cs]}
      \field{month}{3}
      \field{shorttitle}{Fighting an {{Infodemic}}}
      \field{title}{Fighting an {{Infodemic}}: {{COVID-19 Fake News Dataset}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2011.03327
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/D2R6R9G4/Patwa et al. - 2021 - Fighting an Infodemic COVID-19 Fake News Dataset.pdf;/Users/teetusaini/Zotero/storage/D45KXKU8/2011.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Social and Information Networks}
    \endentry
    \entry{pennington_glove_2014}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=ab47b497d1b0cdaddd8594e4bd501ee5}{%
           family={Pennington},
           familyi={P\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=ff4377244c68b8beb3253e53d6387f94}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Doha, Qatar}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \strng{fullhash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \strng{bibnamehash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \strng{authorbibnamehash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \strng{authornamehash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \strng{authorfullhash}{1808086e3c6ed9cc95e38d0eb5455e5e}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})}
      \field{month}{10}
      \field{shorttitle}{{{GloVe}}}
      \field{title}{{{GloVe}}: {{Global Vectors}} for {{Word Representation}}}
      \field{year}{2014}
      \field{pages}{1532\bibrangedash 1543}
      \range{pages}{12}
      \verb{doi}
      \verb 10.3115/v1/D14-1162
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/4S7E58E5/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf
      \endverb
    \endentry
    \entry{peters_deep_2018-3}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=711e37ce316a72d79bd008a205513ef0}{%
           family={Peters},
           familyi={P\bibinitperiod},
           given={Matthew\bibnamedelima E.},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=93f95e4f65833691b9a15cbe2791e2df}{%
           family={Neumann},
           familyi={N\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=48ea6f65be60970c81190df53e86239e}{%
           family={Iyyer},
           familyi={I\bibinitperiod},
           given={Mohit},
           giveni={M\bibinitperiod}}}%
        {{hash=6adacaf607ef85a7b55da44661636929}{%
           family={Gardner},
           familyi={G\bibinitperiod},
           given={Matt},
           giveni={M\bibinitperiod}}}%
        {{hash=9edd3110f9350150b9a22ef5b7d45d25}{%
           family={Clark},
           familyi={C\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=8dde73b4194f5bc4230c4808f3fc1534}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kenton},
           giveni={K\bibinitperiod}}}%
        {{hash=1dbd3a5b42828fb2cebd7786488ba425}{%
           family={Zettlemoyer},
           familyi={Z\bibinitperiod},
           given={Luke},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New Orleans, Louisiana}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{d3eb58783a87d6c0565e02fda2ae5c07}
      \strng{fullhash}{e4ae16c29e91a94c3ee954353fd212ab}
      \strng{bibnamehash}{d3eb58783a87d6c0565e02fda2ae5c07}
      \strng{authorbibnamehash}{d3eb58783a87d6c0565e02fda2ae5c07}
      \strng{authornamehash}{d3eb58783a87d6c0565e02fda2ae5c07}
      \strng{authorfullhash}{e4ae16c29e91a94c3ee954353fd212ab}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
      \field{booktitle}{Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})}
      \field{month}{6}
      \field{title}{Deep {{Contextualized Word Representations}}}
      \field{year}{2018}
      \field{pages}{2227\bibrangedash 2237}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18653/v1/N18-1202
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/76V3T6CZ/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf
      \endverb
    \endentry
    \entry{polyak_acceleration_1992}{article}{}
      \name{author}{2}{}{%
        {{hash=86e8da8c20afcbab24c7fbcca00e3f0e}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={B.\bibnamedelimi T.},
           giveni={B\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=90f07c9c1cf2fd49c3b41d6ae98be179}{%
           family={Juditsky},
           familyi={J\bibinitperiod},
           given={A.\bibnamedelimi B.},
           giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{5f6ea80e1e54657574e315f285327fdb}
      \strng{fullhash}{5f6ea80e1e54657574e315f285327fdb}
      \strng{bibnamehash}{5f6ea80e1e54657574e315f285327fdb}
      \strng{authorbibnamehash}{5f6ea80e1e54657574e315f285327fdb}
      \strng{authornamehash}{5f6ea80e1e54657574e315f285327fdb}
      \strng{authorfullhash}{5f6ea80e1e54657574e315f285327fdb}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0363-0129}
      \field{journaltitle}{SIAM Journal on Control and Optimization}
      \field{month}{7}
      \field{number}{4}
      \field{title}{Acceleration of Stochastic Approximation by Averaging}
      \field{volume}{30}
      \field{year}{1992}
      \field{pages}{838\bibrangedash 855}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1137/0330046
      \endverb
      \keyw{optimal algorithms,recursive estimation,stochastic approximation,stochastic optimization}
    \endentry
    \entry{pruthi_combating_2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=ecc144bbbfb4d50888df1664d8fb12df}{%
           family={Pruthi},
           familyi={P\bibinitperiod},
           given={Danish},
           giveni={D\bibinitperiod}}}%
        {{hash=22a3a1ed163bc6c49fcc9ff7bea77ceb}{%
           family={Dhingra},
           familyi={D\bibinitperiod},
           given={Bhuwan},
           giveni={B\bibinitperiod}}}%
        {{hash=599ae457cf41d4ce64e09433edbc964b}{%
           family={Lipton},
           familyi={L\bibinitperiod},
           given={Zachary\bibnamedelima C.},
           giveni={Z\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Florence, Italy}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{8c8b27fb084243803adc6807cd45c091}
      \strng{fullhash}{8c8b27fb084243803adc6807cd45c091}
      \strng{bibnamehash}{8c8b27fb084243803adc6807cd45c091}
      \strng{authorbibnamehash}{8c8b27fb084243803adc6807cd45c091}
      \strng{authornamehash}{8c8b27fb084243803adc6807cd45c091}
      \strng{authorfullhash}{8c8b27fb084243803adc6807cd45c091}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32\% relative (and 3.3\% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3\% to 45.8\%. Our defense restores accuracy to 75\%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.}
      \field{booktitle}{Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}}
      \field{month}{7}
      \field{title}{Combating {{Adversarial Misspellings}} with {{Robust Word Recognition}}}
      \field{year}{2019}
      \field{pages}{5582\bibrangedash 5591}
      \range{pages}{10}
      \verb{doi}
      \verb 10.18653/v1/P19-1561
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/EDFV2I8D/Pruthi et al. - 2019 - Combating Adversarial Misspellings with Robust Wor.pdf
      \endverb
    \endentry
    \entry{radford_improving_2018-1}{article}{}
      \name{author}{2}{}{%
        {{hash=a812c46caad94fc8701be37871f303ba}{%
           family={Radford},
           familyi={R\bibinitperiod},
           given={Alec},
           giveni={A\bibinitperiod}}}%
        {{hash=997c3143c2e5d593e816d2ff704fbf98}{%
           family={Narasimhan},
           familyi={N\bibinitperiod},
           given={Karthik},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{93049198c7ea4c7c282daf6e881b2cbc}
      \strng{fullhash}{93049198c7ea4c7c282daf6e881b2cbc}
      \strng{bibnamehash}{93049198c7ea4c7c282daf6e881b2cbc}
      \strng{authorbibnamehash}{93049198c7ea4c7c282daf6e881b2cbc}
      \strng{authornamehash}{93049198c7ea4c7c282daf6e881b2cbc}
      \strng{authorfullhash}{93049198c7ea4c7c282daf6e881b2cbc}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).}
      \field{langid}{english}
      \field{title}{Improving {{Language Understanding}} by {{Generative Pre-Training}}}
      \field{year}{2018}
    \endentry
    \entry{rawlinson_significance_2007}{article}{}
      \name{author}{1}{}{%
        {{hash=b5d8e7f2ad0e5c6f2c3005dec0ee8194}{%
           family={Rawlinson},
           familyi={R\bibinitperiod},
           given={Graham},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \strng{fullhash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \strng{bibnamehash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \strng{authorbibnamehash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \strng{authornamehash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \strng{authorfullhash}{b5d8e7f2ad0e5c6f2c3005dec0ee8194}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Presents the summary of a PhD thesis on letter position in word recognition, which was written in 1976. The thesis was the source of the text used on the cover of the October 2006 issue of the IEEE Aerospace and Electronic Systems Magazine.}
      \field{issn}{1557-959X}
      \field{journaltitle}{IEEE Aerospace and Electronic Systems Magazine}
      \field{month}{1}
      \field{number}{1}
      \field{title}{The {{Significance}} of {{Letter Position}} in {{Word Recognition}}}
      \field{volume}{22}
      \field{year}{2007}
      \field{pages}{26\bibrangedash 27}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1109/MAES.2007.327521
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/P2WWZJYI/4126417.html
      \endverb
      \keyw{Books,Copper,Electromagnetic scattering,Kirk field collapse effect,Marine vehicles,Metamaterials,Microwave frequencies,Roads,Shape,Testing}
    \endentry
    \entry{ren_generating_2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=138626d1adbe607b34699718dd377319}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shuhuai},
           giveni={S\bibinitperiod}}}%
        {{hash=9e48e778d5cd373a68481261d90dcc04}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Yihe},
           giveni={Y\bibinitperiod}}}%
        {{hash=a328f63ba0e78525a27b105bb17fe87e}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kun},
           giveni={K\bibinitperiod}}}%
        {{hash=cb0803882fba28a6c7ecb519069a89de}{%
           family={Che},
           familyi={C\bibinitperiod},
           given={Wanxiang},
           giveni={W\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Florence, Italy}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{14b01d4236e824603b02873af157428f}
      \strng{fullhash}{95503987bc0659e8b806df783f04ffc7}
      \strng{bibnamehash}{14b01d4236e824603b02873af157428f}
      \strng{authorbibnamehash}{14b01d4236e824603b02873af157428f}
      \strng{authornamehash}{14b01d4236e824603b02873af157428f}
      \strng{authorfullhash}{95503987bc0659e8b806df783f04ffc7}
      \field{extraname}{1}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.}
      \field{booktitle}{Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}}
      \field{month}{7}
      \field{title}{Generating {{Natural Language Adversarial Examples}} through {{Probability Weighted Word Saliency}}}
      \field{year}{2019}
      \field{pages}{1085\bibrangedash 1097}
      \range{pages}{13}
      \verb{doi}
      \verb 10.18653/v1/P19-1103
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/DC366H9X/Ren et al. - 2019 - Generating Natural Language Adversarial Examples t.pdf
      \endverb
    \endentry
    \entry{ren_generating_2020}{article}{}
      \name{author}{7}{}{%
        {{hash=28129f92f4978b2554255a35adbe507a}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Yankun},
           giveni={Y\bibinitperiod}}}%
        {{hash=057f9958011ca0430621efc18b89b4e2}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Jianbin},
           giveni={J\bibinitperiod}}}%
        {{hash=4172dc323d14cd7be5043a4fa1382f91}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Siliang},
           giveni={S\bibinitperiod}}}%
        {{hash=21b6969d672728cb4ea4381d1d8706b5}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=249da813d604d350b10899da1aa7767f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Shuang},
           giveni={S\bibinitperiod}}}%
        {{hash=a384b480aa61181467d32b3ca38297fd}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=62341ebcc57e03720c55a39d93f0b17d}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Xiang},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{a64924a1d6f4f14140199a9cee7b0581}
      \strng{fullhash}{6d10536d6cc7e72c1c464ffc04e81e2b}
      \strng{bibnamehash}{a64924a1d6f4f14140199a9cee7b0581}
      \strng{authorbibnamehash}{a64924a1d6f4f14140199a9cee7b0581}
      \strng{authornamehash}{a64924a1d6f4f14140199a9cee7b0581}
      \strng{authorfullhash}{6d10536d6cc7e72c1c464ffc04e81e2b}
      \field{extraname}{2}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2003.10388 [cs, stat]}
      \field{month}{3}
      \field{title}{Generating {{Natural Language Adversarial Examples}} on a {{Large Scale}} with {{Generative Models}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2003.10388
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/7F5VJUWM/Ren et al. - 2020 - Generating Natural Language Adversarial Examples o.pdf;/Users/teetusaini/Zotero/storage/LAIZQFWZ/2003.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{salton_vector_1975}{article}{}
      \name{author}{3}{}{%
        {{hash=5299281e19f9b62eb7b3db2c8c4089fd}{%
           family={Salton},
           familyi={S\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
        {{hash=d2a1c5a5506119e30ddf6ef38b9dd45f}{%
           family={Wong},
           familyi={W\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
        {{hash=09bbc7b7b2a15d80318e4af2bf2e4d53}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={C.\bibnamedelimi S.},
           giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{4a341c83215b29d64af72da280194256}
      \strng{fullhash}{4a341c83215b29d64af72da280194256}
      \strng{bibnamehash}{4a341c83215b29d64af72da280194256}
      \strng{authorbibnamehash}{4a341c83215b29d64af72da280194256}
      \strng{authornamehash}{4a341c83215b29d64af72da280194256}
      \strng{authorfullhash}{4a341c83215b29d64af72da280194256}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.}
      \field{issn}{0001-0782}
      \field{journaltitle}{Communications of the ACM}
      \field{month}{11}
      \field{number}{11}
      \field{title}{A Vector Space Model for Automatic Indexing}
      \field{volume}{18}
      \field{year}{1975}
      \field{pages}{613\bibrangedash 620}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/361219.361220
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/PPP633AX/Salton et al. - 1975 - A vector space model for automatic indexing.pdf
      \endverb
      \keyw{automatic indexing,automatic information retrieval,content analysis,document space}
    \endentry
    \entry{sanh_distilbert_2020}{article}{}
      \name{author}{4}{}{%
        {{hash=b17a6e4a2e8a51a0d77017d50c2ef700}{%
           family={Sanh},
           familyi={S\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=4996c576ebaf91c8655cee3341764e3b}{%
           family={Debut},
           familyi={D\bibinitperiod},
           given={Lysandre},
           giveni={L\bibinitperiod}}}%
        {{hash=79bf176bccb31f7489fcf1895b225764}{%
           family={Chaumond},
           familyi={C\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
        {{hash=c34c67badfd5b3624027e9c8c77a69f6}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{9c45110f1218dd76df03ef800db8a970}
      \strng{fullhash}{dc0afc633047d6fca100f460cf89f831}
      \strng{bibnamehash}{9c45110f1218dd76df03ef800db8a970}
      \strng{authorbibnamehash}{9c45110f1218dd76df03ef800db8a970}
      \strng{authornamehash}{9c45110f1218dd76df03ef800db8a970}
      \strng{authorfullhash}{dc0afc633047d6fca100f460cf89f831}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1910.01108 [cs]}
      \field{month}{2}
      \field{shorttitle}{{{DistilBERT}}, a Distilled Version of {{BERT}}}
      \field{title}{{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter}
      \field{year}{2020}
      \verb{eprint}
      \verb 1910.01108
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/6R55MCXP/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf;/Users/teetusaini/Zotero/storage/43KMCF84/1910.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{saxena_textdecepter_2020}{article}{}
      \name{author}{1}{}{%
        {{hash=c5d25dec98935fc635bcd7c682d14c41}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Sachin},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{c5d25dec98935fc635bcd7c682d14c41}
      \strng{fullhash}{c5d25dec98935fc635bcd7c682d14c41}
      \strng{bibnamehash}{c5d25dec98935fc635bcd7c682d14c41}
      \strng{authorbibnamehash}{c5d25dec98935fc635bcd7c682d14c41}
      \strng{authornamehash}{c5d25dec98935fc635bcd7c682d14c41}
      \strng{authorfullhash}{c5d25dec98935fc635bcd7c682d14c41}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Machine learning has been proven to be susceptible to carefully crafted samples, known as adversarial examples. The generation of these adversarial examples helps to make the models more robust and gives us an insight into the underlying decision-making of these models. Over the years, researchers have successfully attacked image classifiers in both, white and black-box settings. However, these methods are not directly applicable to texts as text data is discrete. In recent years, research on crafting adversarial examples against textual applications has been on the rise. In this paper, we present a novel approach for hard-label black-box attacks against Natural Language Processing (NLP) classifiers, where no model information is disclosed, and an attacker can only query the model to get a final decision of the classifier, without confidence scores of the classes involved. Such an attack scenario applies to real-world black-box models being used for security-sensitive applications such as sentiment analysis and toxic content detection.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2008.06860 [cs]}
      \field{month}{12}
      \field{shorttitle}{{{TextDecepter}}}
      \field{title}{{{TextDecepter}}: {{Hard Label Black Box Attack}} on {{Text Classifiers}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2008.06860
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/EVZ9AF6W/Saxena - 2020 - TextDecepter Hard Label Black Box Attack on Text .pdf;/Users/teetusaini/Zotero/storage/6T34RZIQ/2008.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
    \endentry
    \entry{si_better_2021}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=50e55ce524a6cabd5d5e862c580e6869}{%
           family={Si},
           familyi={S\bibinitperiod},
           given={Chenglei},
           giveni={C\bibinitperiod}}}%
        {{hash=27c7fad66da172aaa64e08e3539ff172}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zhengyan},
           giveni={Z\bibinitperiod}}}%
        {{hash=6edbb5e2737f7f49fd4133caf8f6826c}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Fanchao},
           giveni={F\bibinitperiod}}}%
        {{hash=5b9a54b9d83697231299711961622ab8}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=847ddc1cab57f70b90792258a9925cf5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yasheng},
           giveni={Y\bibinitperiod}}}%
        {{hash=e51b2feac8da559d0f1d95788a6eaea4}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qun},
           giveni={Q\bibinitperiod}}}%
        {{hash=faa6f5041a22004ca0dd547c03476790}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Maosong},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{3ba93831df0e0c79198d806b7093b644}
      \strng{fullhash}{01abae8a6faea771d8db632042e46afe}
      \strng{bibnamehash}{3ba93831df0e0c79198d806b7093b644}
      \strng{authorbibnamehash}{3ba93831df0e0c79198d806b7093b644}
      \strng{authornamehash}{3ba93831df0e0c79198d806b7093b644}
      \strng{authorfullhash}{01abae8a6faea771d8db632042e46afe}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021}
      \field{month}{8}
      \field{shorttitle}{Better {{Robustness}} by {{More Coverage}}}
      \field{title}{Better {{Robustness}} by {{More Coverage}}: {{Adversarial}} and {{Mixup Data Augmentation}} for {{Robust Finetuning}}}
      \field{year}{2021}
      \field{pages}{1569\bibrangedash 1576}
      \range{pages}{8}
      \verb{doi}
      \verb 10.18653/v1/2021.findings-acl.137
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/99C258HW/Si et al. - 2021 - Better Robustness by More Coverage Adversarial an.pdf
      \endverb
    \endentry
    \entry{sun_adv-bert_2020}{article}{}
      \name{author}{7}{}{%
        {{hash=48972a9b89e478f69f1b1e95205c6278}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Lichao},
           giveni={L\bibinitperiod}}}%
        {{hash=6f2df84c7f945ebdbc201efc545e4b28}{%
           family={Hashimoto},
           familyi={H\bibinitperiod},
           given={Kazuma},
           giveni={K\bibinitperiod}}}%
        {{hash=f04e25a79b81d6565c2d598e1fd5fa2e}{%
           family={Yin},
           familyi={Y\bibinitperiod},
           given={Wenpeng},
           giveni={W\bibinitperiod}}}%
        {{hash=a0602035e63a56d86a67823386c1a49d}{%
           family={Asai},
           familyi={A\bibinitperiod},
           given={Akari},
           giveni={A\bibinitperiod}}}%
        {{hash=508f0a4e80af71a1970f575437811489}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=8e5d7cd56c9d4ec082be97723ccbb9e5}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Philip},
           giveni={P\bibinitperiod}}}%
        {{hash=1fa4dfdd5ecf049d39575fe7ba1ec9f3}{%
           family={Xiong},
           familyi={X\bibinitperiod},
           given={Caiming},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{f3265eb3576c6d62f4aad97dd8004028}
      \strng{fullhash}{3591f92ac928aeb65248d6c1f17ac755}
      \strng{bibnamehash}{f3265eb3576c6d62f4aad97dd8004028}
      \strng{authorbibnamehash}{f3265eb3576c6d62f4aad97dd8004028}
      \strng{authornamehash}{f3265eb3576c6d62f4aad97dd8004028}
      \strng{authorfullhash}{3591f92ac928aeb65248d6c1f17ac755}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{There is an increasing amount of literature that claims the brittleness of deep neural networks in dealing with adversarial examples that are created maliciously. It is unclear, however, how the models will perform in realistic scenarios where \textbackslash textit\{natural rather than malicious\} adversarial instances often exist. This work systematically explores the robustness of BERT, the state-of-the-art Transformer-style model in NLP, in dealing with noisy data, particularly mistakes in typing the keyboard, that occur inadvertently. Intensive experiments on sentiment analysis and question answering benchmarks indicate that: (i) Typos in various words of a sentence do not influence equally. The typos in informative words make severer damages; (ii) Mistype is the most damaging factor, compared with inserting, deleting, etc.; (iii) Humans and machines have different focuses on recognizing adversarial attacks.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2003.04985 [cs]}
      \field{month}{2}
      \field{shorttitle}{Adv-{{BERT}}}
      \field{title}{Adv-{{BERT}}: {{BERT}} Is Not Robust on Misspellings! {{Generating}} Nature Adversarial Samples on {{BERT}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2003.04985
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/TDW6TXYR/Sun et al. - 2020 - Adv-BERT BERT is not robust on misspellings! Gene.pdf;/Users/teetusaini/Zotero/storage/ETS5A5G9/2003.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{szegedy_intriguing_2014}{article}{}
      \name{author}{7}{}{%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=e9fec85bbce1b087a6ebefe26e73f7bf}{%
           family={Zaremba},
           familyi={Z\bibinitperiod},
           given={Wojciech},
           giveni={W\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=c83b564e32475f10dcc91be7e66d3e81}{%
           family={Bruna},
           familyi={B\bibinitperiod},
           given={Joan},
           giveni={J\bibinitperiod}}}%
        {{hash=8bbc4c5d96f205bada839e74e0202146}{%
           family={Erhan},
           familyi={E\bibinitperiod},
           given={Dumitru},
           giveni={D\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{fullhash}{c8c3e9b8f095d055ce92294b35ff475c}
      \strng{bibnamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authorbibnamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authornamehash}{80f8e6bfc3aff3e75b2807a6f6962740}
      \strng{authorfullhash}{c8c3e9b8f095d055ce92294b35ff475c}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1312.6199 [cs]}
      \field{month}{2}
      \field{title}{Intriguing Properties of Neural Networks}
      \field{year}{2014}
      \verb{eprint}
      \verb 1312.6199
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/M2CHLT8E/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf;/Users/teetusaini/Zotero/storage/6PBELWQN/1312.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{tarvainen_mean_2018}{article}{}
      \name{author}{2}{}{%
        {{hash=3043c52b71829288274c6587601704a8}{%
           family={Tarvainen},
           familyi={T\bibinitperiod},
           given={Antti},
           giveni={A\bibinitperiod}}}%
        {{hash=130b7c3a12dc949e574f845c6b83a9af}{%
           family={Valpola},
           familyi={V\bibinitperiod},
           given={Harri},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{9945020ab9fe51f572128e962cdd769f}
      \strng{fullhash}{9945020ab9fe51f572128e962cdd769f}
      \strng{bibnamehash}{9945020ab9fe51f572128e962cdd769f}
      \strng{authorbibnamehash}{9945020ab9fe51f572128e962cdd769f}
      \strng{authornamehash}{9945020ab9fe51f572128e962cdd769f}
      \strng{authorfullhash}{9945020ab9fe51f572128e962cdd769f}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1703.01780 [cs, stat]}
      \field{month}{4}
      \field{shorttitle}{Mean Teachers Are Better Role Models}
      \field{title}{Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results}
      \field{year}{2018}
      \verb{eprint}
      \verb 1703.01780
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/QHT4R9TD/Tarvainen and Valpola - 2018 - Mean teachers are better role models Weight-avera.pdf;/Users/teetusaini/Zotero/storage/VJB9JEHF/1703.html
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{vaswani_attention_2017}{article}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{fullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \strng{bibnamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authorbibnamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authornamehash}{ee273ab30cfb889666f8c4d806eb9ce7}
      \strng{authorfullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1706.03762 [cs]}
      \field{month}{12}
      \field{title}{Attention {{Is All You Need}}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1706.03762
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/AHTHPI6X/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/teetusaini/Zotero/storage/ISN3SHEF/1706.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{wang_cat-gen_2020}{article}{}
      \name{author}{8}{}{%
        {{hash=ed7ba810bd13657b7195f0d05f714306}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Tianlu},
           giveni={T\bibinitperiod}}}%
        {{hash=27d22d8f05f44b47f7dcfe9b7fb85584}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xuezhi},
           giveni={X\bibinitperiod}}}%
        {{hash=cfdac54e7bfea47ba952935073fcdbd3}{%
           family={Qin},
           familyi={Q\bibinitperiod},
           given={Yao},
           giveni={Y\bibinitperiod}}}%
        {{hash=3ae5a02f4420592011d3066c09cc15d8}{%
           family={Packer},
           familyi={P\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=f32edc767f7574ca356c7480491fa999}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Kang},
           giveni={K\bibinitperiod}}}%
        {{hash=8106549c70bb754bff2f25ad733d8cc7}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Jilin},
           giveni={J\bibinitperiod}}}%
        {{hash=af12cc8ca94c1e303f4ae44f22264508}{%
           family={Beutel},
           familyi={B\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=4c62c73d10bbf27f93c07774d30f647c}{%
           family={Chi},
           familyi={C\bibinitperiod},
           given={Ed},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{d3f38c123dfb5f8d32609d9493717607}
      \strng{fullhash}{4542e551159917653b6e74354ff4bc08}
      \strng{bibnamehash}{d3f38c123dfb5f8d32609d9493717607}
      \strng{authorbibnamehash}{d3f38c123dfb5f8d32609d9493717607}
      \strng{authornamehash}{d3f38c123dfb5f8d32609d9493717607}
      \strng{authorfullhash}{4542e551159917653b6e74354ff4bc08}
      \field{extraname}{1}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2010.02338 [cs]}
      \field{month}{10}
      \field{shorttitle}{{{CAT-Gen}}}
      \field{title}{{{CAT-Gen}}: {{Improving Robustness}} in {{NLP Models}} via {{Controlled Adversarial Text Generation}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2010.02338
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/LR5BEYI6/Wang et al. - 2020 - CAT-Gen Improving Robustness in NLP Models via Co.pdf;/Users/teetusaini/Zotero/storage/LFKDN9QU/2010.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{wang_towards_2021}{article}{}
      \name{author}{5}{}{%
        {{hash=11cf8ccb393cc032fe70334df28645aa}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Wenqi},
           giveni={W\bibinitperiod}}}%
        {{hash=f3e2470056bb08dfeecc5612dc7fe9c5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Run},
           giveni={R\bibinitperiod}}}%
        {{hash=60aee30f9eb08411d7caa34701641b21}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Lina},
           giveni={L\bibinitperiod}}}%
        {{hash=ea584057bf47b10761d1f41a96b038a2}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Zhibo},
           giveni={Z\bibinitperiod}}}%
        {{hash=8ca5bcb9da3f685d5be11a1b89fa5a7b}{%
           family={Ye},
           familyi={Y\bibinitperiod},
           given={Aoshuang},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{cf461fdf4f0fb51c9effdd8ec1b48c12}
      \strng{fullhash}{2c270e6832886d9d1f76fa69eea84242}
      \strng{bibnamehash}{cf461fdf4f0fb51c9effdd8ec1b48c12}
      \strng{authorbibnamehash}{cf461fdf4f0fb51c9effdd8ec1b48c12}
      \strng{authornamehash}{cf461fdf4f0fb51c9effdd8ec1b48c12}
      \strng{authorfullhash}{2c270e6832886d9d1f76fa69eea84242}
      \field{extraname}{2}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques. We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1902.07285 [cs]}
      \field{month}{4}
      \field{shorttitle}{Towards a {{Robust Deep Neural Network}} in {{Texts}}}
      \field{title}{Towards a {{Robust Deep Neural Network}} in {{Texts}}: {{A Survey}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 1902.07285
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/P2NHSKAZ/Wang et al. - 2021 - Towards a Robust Deep Neural Network in Texts A S.pdf;/Users/teetusaini/Zotero/storage/PP3DSKQC/1902.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
    \endentry
    \entry{wang_natural_2020-1}{article}{}
      \name{author}{3}{}{%
        {{hash=b0f8a63bc64e1bee18634e9e0eb45232}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiaosen},
           giveni={X\bibinitperiod}}}%
        {{hash=71c4882c58df08492958e248e77cd853}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
        {{hash=a328f63ba0e78525a27b105bb17fe87e}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kun},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{5838e51e6cd0f498c2d49af122acda82}
      \strng{fullhash}{5838e51e6cd0f498c2d49af122acda82}
      \strng{bibnamehash}{5838e51e6cd0f498c2d49af122acda82}
      \strng{authorbibnamehash}{5838e51e6cd0f498c2d49af122acda82}
      \strng{authornamehash}{5838e51e6cd0f498c2d49af122acda82}
      \strng{authorfullhash}{5838e51e6cd0f498c2d49af122acda82}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, inspired by a mass of researches on adversarial examples for computer vision, there has been a growing interest in designing adversarial attacks for Natural Language Processing (NLP) tasks, followed by very few works of adversarial defenses for NLP. To our knowledge, there exists no defense method against the successful synonym substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to be perceived by humans. We contribute to fill this gap and propose a novel adversarial defense method called \textbackslash textit\{Synonym Encoding Method\} (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with little decay on the accuracy for benign examples. To better evaluate SEM, we also design a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic for synonym substitution based attacks. Compared with the first genetic based adversarial attack proposed in 2018, IGA can achieve higher attack success rate with lower word substitution rate, at the same time maintain the transferability of adversarial examples.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1909.06723 [cs]}
      \field{month}{4}
      \field{title}{Natural {{Language Adversarial Attacks}} and {{Defenses}} in {{Word Level}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 1909.06723
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/6I52DNUL/Wang et al. - 2020 - Natural Language Adversarial Attacks and Defenses .pdf;/Users/teetusaini/Zotero/storage/RUKARYWE/1909.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{wang_adversarial_2021-1}{article}{}
      \name{author}{4}{}{%
        {{hash=b0f8a63bc64e1bee18634e9e0eb45232}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xiaosen},
           giveni={X\bibinitperiod}}}%
        {{hash=9f15df347f7499e975e80688fbc4b253}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yichen},
           giveni={Y\bibinitperiod}}}%
        {{hash=9e48e778d5cd373a68481261d90dcc04}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Yihe},
           giveni={Y\bibinitperiod}}}%
        {{hash=a328f63ba0e78525a27b105bb17fe87e}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kun},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{d16939a002b44db76db2f207af4e022d}
      \strng{fullhash}{a0ed257341a23d50bdf19ec8725cc17b}
      \strng{bibnamehash}{d16939a002b44db76db2f207af4e022d}
      \strng{authorbibnamehash}{d16939a002b44db76db2f207af4e022d}
      \strng{authornamehash}{d16939a002b44db76db2f207af4e022d}
      \strng{authorfullhash}{a0ed257341a23d50bdf19ec8725cc17b}
      \field{extraname}{3}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Adversarial training is the most empirically successful approach in improving the robustness of deep neural networks for image classification. For text classification, however, existing synonym substitution based adversarial attacks are effective but not very efficient to be incorporated into practical text adversarial training. Gradient-based attacks, which are very efficient for images, are hard to be implemented for synonym substitution based text attacks due to the lexical, grammatical and semantic constraints and the discrete text input space. Thereby, we propose a fast text adversarial attack method called Fast Gradient Projection Method (FGPM) based on synonym substitution, which is about 20 times faster than existing text attack methods and could achieve similar attack performance. We then incorporate FGPM with adversarial training and propose a text defense method called Adversarial Training with FGPM enhanced by Logit pairing (ATFL). Experiments show that ATFL could significantly improve the model robustness and block the transferability of adversarial examples.}
      \field{issn}{2374-3468}
      \field{journaltitle}{Proceedings of the AAAI Conference on Artificial Intelligence}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{16}
      \field{title}{Adversarial {{Training}} with {{Fast Gradient Projection Method}} against {{Synonym Substitution Based Text Attacks}}}
      \field{volume}{35}
      \field{year}{2021}
      \field{pages}{13997\bibrangedash 14005}
      \range{pages}{9}
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/C8V9ZTTS/Wang et al. - 2021 - Adversarial Training with Fast Gradient Projection.pdf
      \endverb
      \keyw{Adversarial Attacks \& Robustness}
    \endentry
    \entry{yuan_adversarial_2018}{article}{}
      \name{author}{4}{}{%
        {{hash=cefe12b8e17312819a213a0091c046db}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Xiaoyong},
           giveni={X\bibinitperiod}}}%
        {{hash=6e1c0c48781a197a0b2d79aa91644570}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Pan},
           giveni={P\bibinitperiod}}}%
        {{hash=7073ad1ffaff73809b226c67fb4a102a}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Qile},
           giveni={Q\bibinitperiod}}}%
        {{hash=a6278bd27ac7d7873f7c9aa0363482b8}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Xiaolin},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{721385adf3c32f569daa469c895cabe7}
      \strng{fullhash}{0c725b2897aea640cd296ce149643605}
      \strng{bibnamehash}{721385adf3c32f569daa469c895cabe7}
      \strng{authorbibnamehash}{721385adf3c32f569daa469c895cabe7}
      \strng{authornamehash}{721385adf3c32f569daa469c895cabe7}
      \strng{authorfullhash}{0c725b2897aea640cd296ce149643605}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1712.07107 [cs, stat]}
      \field{month}{7}
      \field{shorttitle}{Adversarial {{Examples}}}
      \field{title}{Adversarial {{Examples}}: {{Attacks}} and {{Defenses}} for {{Deep Learning}}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1712.07107
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/WH2XII8L/Yuan et al. - 2018 - Adversarial Examples Attacks and Defenses for Dee.pdf;/Users/teetusaini/Zotero/storage/8XZTLDVI/1712.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{zang_word-level_2019}{article}{}
      \name{author}{7}{}{%
        {{hash=35dc7d15a49b7d8ae385f610a1e607dd}{%
           family={Zang},
           familyi={Z\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=6edbb5e2737f7f49fd4133caf8f6826c}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Fanchao},
           giveni={F\bibinitperiod}}}%
        {{hash=08d30e2ee660bb1e052fb3b98013b34f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Chenghao},
           giveni={C\bibinitperiod}}}%
        {{hash=5b9a54b9d83697231299711961622ab8}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=1276b0c695bd412950c2be5e737aeed5}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Meng},
           giveni={M\bibinitperiod}}}%
        {{hash=e51b2feac8da559d0f1d95788a6eaea4}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qun},
           giveni={Q\bibinitperiod}}}%
        {{hash=faa6f5041a22004ca0dd547c03476790}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Maosong},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{eca3198ebd6c4d5eb7fe3c9757412410}
      \strng{fullhash}{448f6925d92f7ca104ad242da8033ff9}
      \strng{bibnamehash}{eca3198ebd6c4d5eb7fe3c9757412410}
      \strng{authorbibnamehash}{eca3198ebd6c4d5eb7fe3c9757412410}
      \strng{authornamehash}{eca3198ebd6c4d5eb7fe3c9757412410}
      \strng{authorfullhash}{448f6925d92f7ca104ad242da8033ff9}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Word-Level {{Textual Adversarial Attacking}} as {{Combinatorial Optimization}}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.18653/v1/2020.acl-main.540
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/2VYWRSDW/Zang et al. - 2019 - Word-level Textual Adversarial Attacking as Combin.pdf;/Users/teetusaini/Zotero/storage/WG9GYFZW/1910.html
      \endverb
    \endentry
    \entry{zhang_adversarial_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=cf375052af0e4d4880ab65b601471090}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Wei\bibnamedelima Emma},
           giveni={W\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=0516d009aef1e930a0bbcded4ecb28af}{%
           family={Sheng},
           familyi={S\bibinitperiod},
           given={Quan\bibnamedelima Z.},
           giveni={Q\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=86b6c2300b749f6e00c2e764c734e990}{%
           family={Alhazmi},
           familyi={A\bibinitperiod},
           given={Ahoud},
           giveni={A\bibinitperiod}}}%
        {{hash=fc90189e403c903655a1cbc1af93456b}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Chenliang},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{990df704d24e673d1ce72d932d840e92}
      \strng{fullhash}{a3c4460723f78e5d618cc7e420dee943}
      \strng{bibnamehash}{990df704d24e673d1ce72d932d840e92}
      \strng{authorbibnamehash}{990df704d24e673d1ce72d932d840e92}
      \strng{authornamehash}{990df704d24e673d1ce72d932d840e92}
      \strng{authorfullhash}{a3c4460723f78e5d618cc7e420dee943}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:1901.06796 [cs]}
      \field{month}{4}
      \field{shorttitle}{Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}}
      \field{title}{Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}: {{A Survey}}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1901.06796
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/MQIWCTZQ/Zhang et al. - 2019 - Adversarial Attacks on Deep Learning Models in Nat.pdf;/Users/teetusaini/Zotero/storage/MVREFBFT/1901.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{zhou_defense_2020}{article}{}
      \name{author}{5}{}{%
        {{hash=3625efaf108a2751cfdacddd7a073c3d}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=960deaf1c28d838494639cd2ab3cd411}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Xiaoqing},
           giveni={X\bibinitperiod}}}%
        {{hash=8ce122ee46af7bc8820d9d1c340793dc}{%
           family={Hsieh},
           familyi={H\bibinitperiod},
           given={Cho-Jui},
           giveni={C\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=f63d8342d0bfe9c7d764d804c3741535}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Kai-wei},
           giveni={K\bibinithyphendelim w\bibinitperiod}}}%
        {{hash=ed0f3403ef6238bb67876d4c6ad27464}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Xuanjing},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{5486ce563fc3f3125c46603bf0d2b79c}
      \strng{fullhash}{cfb874cc69c6d991fd2dd174cf3ef71a}
      \strng{bibnamehash}{5486ce563fc3f3125c46603bf0d2b79c}
      \strng{authorbibnamehash}{5486ce563fc3f3125c46603bf0d2b79c}
      \strng{authornamehash}{5486ce563fc3f3125c46603bf0d2b79c}
      \strng{authorfullhash}{cfb874cc69c6d991fd2dd174cf3ef71a}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Despite neural networks have achieved prominent performance on many natural language processing (NLP) tasks, they are vulnerable to adversarial examples. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to defense substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models for NLP applications. We demonstrate through extensive experimentation that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2006.11627 [cs]}
      \field{month}{6}
      \field{title}{Defense against {{Adversarial Attacks}} in {{NLP}} via {{Dirichlet Neighborhood Ensemble}}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2006.11627
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/Z7AUENF5/Zhou et al. - 2020 - Defense against Adversarial Attacks in NLP via Dir.pdf;/Users/teetusaini/Zotero/storage/ILBBVL3H/2006.html
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{zhu_at-bert_2021}{article}{}
      \name{author}{7}{}{%
        {{hash=4aa885c78ddf54a4a93061f1004f660e}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Danqing},
           giveni={D\bibinitperiod}}}%
        {{hash=6b1ee3caad4245114fb68f2017b424e7}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Wangli},
           giveni={W\bibinitperiod}}}%
        {{hash=e760d38078aff4a5b24c867aa5178c26}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yang},
           giveni={Y\bibinitperiod}}}%
        {{hash=59ebc19d9fd49c2b2dc0b289efa74355}{%
           family={Zhong},
           familyi={Z\bibinitperiod},
           given={Qiwei},
           giveni={Q\bibinitperiod}}}%
        {{hash=e7e3709ca18bfe1de429491849c933bd}{%
           family={Zeng},
           familyi={Z\bibinitperiod},
           given={Guanxiong},
           giveni={G\bibinitperiod}}}%
        {{hash=6a5283c3bee3dba146b992e48d0b607a}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Weilin},
           giveni={W\bibinitperiod}}}%
        {{hash=629c2ea3936bc8a396850bb0e26df981}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Jiayu},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{060816d5987c5e1815fb478d357dea12}
      \strng{fullhash}{d8d6b1b7655cc3fd5485b3b0b6c62e0f}
      \strng{bibnamehash}{060816d5987c5e1815fb478d357dea12}
      \strng{authorbibnamehash}{060816d5987c5e1815fb478d357dea12}
      \strng{authornamehash}{060816d5987c5e1815fb478d357dea12}
      \strng{authorfullhash}{d8d6b1b7655cc3fd5485b3b0b6c62e0f}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Acronym identification focuses on finding the acronyms and the phrases that have been abbreviated, which is crucial for scientific document understanding tasks. However, the limited size of manually annotated datasets hinders further improvement for the problem. Recent breakthroughs of language models pre-trained on large corpora clearly show that unsupervised pre-training can vastly improve the performance of downstream tasks. In this paper, we present an Adversarial Training BERT method named AT-BERT, our winning solution to acronym identification task for Scientific Document Understanding (SDU) Challenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to capture better semantic representation. Then we incorporate the FGM adversarial training strategy into the fine-tuning of BERT, which makes the model more robust and generalized. Furthermore, an ensemble mechanism is devised to involve the representations learned from multiple BERT variants. Assembling all these components together, the experimental results on the SciAI dataset show that our proposed approach outperforms all other competitive state-of-the-art methods.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{arXiv:2101.03700 [cs]}
      \field{month}{1}
      \field{shorttitle}{{{AT-BERT}}}
      \field{title}{{{AT-BERT}}: {{Adversarial Training BERT}} for {{Acronym Identification Winning Solution}} for {{SDU}}@{{AAAI-21}}}
      \field{year}{2021}
      \verb{eprint}
      \verb 2101.03700
      \endverb
      \verb{file}
      \verb /Users/teetusaini/Zotero/storage/FJ4YLD9I/Zhu et al. - 2021 - AT-BERT Adversarial Training BERT for Acronym Ide.pdf;/Users/teetusaini/Zotero/storage/I3B3CFFX/2101.html
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
  \enddatalist
\endrefsection
\endinput

