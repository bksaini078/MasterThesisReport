
@article{akhtar_threat_2018,
  title = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}},
  author = {Akhtar, Naveed and Mian, Ajmal},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.00553 [cs]},
  eprint = {1801.00553},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/teetusaini/Zotero/storage/9IGBAP98/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf;/Users/teetusaini/Zotero/storage/H7YLBURW/1801.html}
}

@article{al-ansari_survey_2020,
  title = {Survey on {{Word Embedding Techniques}} in {{Natural Language Processing}}},
  author = {{Al-Ansari}, Khaled},
  year = {2020},
  month = aug,
  file = {/Users/teetusaini/Zotero/storage/6ZHKIKJF/Al-Ansari - 2020 - Survey on Word Embedding Techniques in Natural Lan.pdf}
}

@article{almeida_word_2019-1,
  title = {Word {{Embeddings}}: {{A Survey}}},
  shorttitle = {Word {{Embeddings}}},
  author = {Almeida, Felipe and Xex{\'e}o, Geraldo},
  year = {2019},
  journal = {ArXiv},
  abstract = {The main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis, are described, which are now commonly called word embeddings. This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
  file = {/Users/teetusaini/Zotero/storage/YIKBWF7Z/Almeida and Xex√©o - 2019 - Word Embeddings A Survey.pdf}
}

@article{alzantot_generating_2018,
  title = {Generating {{Natural Language Adversarial Examples}}},
  author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
  year = {2018},
  month = sep,
  journal = {arXiv:1804.07998 [cs]},
  eprint = {1804.07998},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/YRPEYTAL/Alzantot et al. - 2018 - Generating Natural Language Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/V3U9BX5U/1804.html}
}

@article{ayoub_combat_2021,
  title = {Combat {{COVID-19}} Infodemic Using Explainable Natural Language Processing Models},
  author = {Ayoub, Jackie and Yang, X. Jessie and Zhou, Feng},
  year = {2021},
  month = jul,
  journal = {Information Processing \& Management},
  volume = {58},
  number = {4},
  pages = {102569},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2021.102569},
  abstract = {Misinformation of COVID-19 is prevalent on social media as the pandemic unfolds, and the associated risks are extremely high. Thus, it is critical to detect and combat such misinformation. Recently, deep learning models using natural language processing techniques, such as BERT (Bidirectional Encoder Representations from Transformers), have achieved great successes in detecting misinformation. In this paper, we proposed an explainable natural language processing model based on DistilBERT and SHAP (Shapley Additive exPlanations) to combat misinformation about COVID-19 due to their efficiency and effectiveness. First, we collected a dataset of 984 claims about COVID-19 with fact-checking. By augmenting the data using back-translation, we doubled the sample size of the dataset and the DistilBERT model was able to obtain good performance (accuracy: 0.972; areas under the curve: 0.993) in detecting misinformation about COVID-19. Our model was also tested on a larger dataset for AAAI2021 \textemdash{} COVID-19 Fake News Detection Shared Task and obtained good performance (accuracy: 0.938; areas under the curve: 0.985). The performance on both datasets was better than traditional machine learning models. Second, in order to boost public trust in model prediction, we employed SHAP to improve model explainability, which was further evaluated using a between-subjects experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE), and text+SHAP explanation+source and evidence (TSESE). The participants were significantly more likely to trust and share information related to COVID-19 in the TSE and TSESE conditions than in the T condition. Our results provided good implications for detecting misinformation about COVID-19 and improving public trust.},
  langid = {english},
  keywords = {BERT,COVID-19,DistilBERT,Misinformation detection,SHAP,Trust},
  file = {/Users/teetusaini/Zotero/storage/EG6WKN76/Ayoub et al. - 2021 - Combat COVID-19 infodemic using explainable natura.pdf;/Users/teetusaini/Zotero/storage/86GIHKPN/S0306457321000704.html}
}

@article{bahdanau_neural_2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = sep,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/JHC27BZC/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf;/Users/teetusaini/Zotero/storage/LDHCRGD6/1409.html}
}

@inproceedings{bao_defending_2021,
  title = {Defending {{Pre-trained Language Models}} from {{Adversarial Word Substitution Without Performance Sacrifice}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Bao, Rongzhou and Wang, Jiayi and Zhao, Hai},
  year = {2021},
  month = aug,
  pages = {3248--3258},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.287},
  file = {/Users/teetusaini/Zotero/storage/77ASIQR2/Bao et al. - 2021 - Defending Pre-trained Language Models from Adversa.pdf}
}

@article{bayer_survey_2021,
  title = {A {{Survey}} on {{Data Augmentation}} for {{Text Classification}}},
  author = {Bayer, Markus and Kaufhold, Marc-Andr{\'e} and Reuter, Christian},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.03158 [cs]},
  eprint = {2107.03158},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data augmentation, the artificial creation of training data for machine learning by transformations, is a widely studied research field across machine learning disciplines. While it is useful for increasing the generalization capabilities of a model, it can also address many other challenges and problems, from overcoming a limited amount of training data over regularizing the objective to limiting the amount data used to protect privacy. Based on a precise description of the goals and applications of data augmentation (C1) and a taxonomy for existing works (C2), this survey is concerned with data augmentation methods for textual classification and aims to achieve a concise and comprehensive overview for researchers and practitioners (C3). Derived from the taxonomy, we divided more than 100 methods into 12 different groupings and provide state-of-the-art references expounding which methods are highly promising (C4). Finally, research perspectives that may constitute a building block for future work are given (C5).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/SFBSLSYB/Bayer et al. - 2021 - A Survey on Data Augmentation for Text Classificat.pdf;/Users/teetusaini/Zotero/storage/KQX7MVUR/2107.html}
}

@article{belinkov_analysis_2019,
  title = {Analysis {{Methods}} in {{Neural Language Processing}}: {{A Survey}}},
  shorttitle = {Analysis {{Methods}} in {{Neural Language Processing}}},
  author = {Belinkov, Yonatan and Glass, James},
  year = {2019},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {49--72},
  doi = {10.1162/tacl_a_00254},
  abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
  file = {/Users/teetusaini/Zotero/storage/BFYG2X5X/Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf}
}

@article{belinkov_synthetic_2018,
  title = {Synthetic and {{Natural Noise Both Break Neural Machine Translation}}},
  author = {Belinkov, Yonatan and Bisk, Yonatan},
  year = {2018},
  month = feb,
  journal = {arXiv:1711.02173 [cs]},
  eprint = {1711.02173},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/Users/teetusaini/Zotero/storage/33JV9EGK/Belinkov and Bisk - 2018 - Synthetic and Natural Noise Both Break Neural Mach.pdf;/Users/teetusaini/Zotero/storage/MZUDEQH5/1711.html}
}

@article{belinkov_synthetic_2018-1,
  title = {Synthetic and {{Natural Noise Both Break Neural Machine Translation}}},
  author = {Belinkov, Yonatan and Bisk, Yonatan},
  year = {2018},
  month = feb,
  journal = {arXiv:1711.02173 [cs]},
  eprint = {1711.02173},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/Users/teetusaini/Zotero/storage/VUHZQYIM/Belinkov and Bisk - 2018 - Synthetic and Natural Noise Both Break Neural Mach.pdf;/Users/teetusaini/Zotero/storage/93KB6SBZ/1711.html}
}

@article{bengio_neural_2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = {2003},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {3},
  number = {null},
  pages = {1137--1155},
  issn = {1532-4435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/Users/teetusaini/Zotero/storage/NKZJ6VBQ/Bengio et al. - 2003 - A neural probabilistic language model.pdf}
}

@inproceedings{bhavani_review_2021,
  title = {A {{Review}} of {{State Art}} of {{Text Classification Algorithms}}},
  booktitle = {2021 5th {{International Conference}} on {{Computing Methodologies}} and {{Communication}} ({{ICCMC}})},
  author = {Bhavani, A. and Santhosh Kumar, B.},
  year = {2021},
  month = apr,
  pages = {1484--1490},
  doi = {10.1109/ICCMC51019.2021.9418262},
  abstract = {In the recent years, the categorization of text documents into predefined classifications has perceived a growing interest due to the growing of documents in digital form and needs to organize them. Text categorization is one of the extensively used for natural language processing (NLP) applications have achieved using machine learning algorithms. Text classification is a challenging researcher to find the best suitable structure and technique. Classification process done using manual and automatic classification. This research paper covers the preprocessing, feature extraction, different algorithms and techniques for text classification and finally evaluates the performance metrics for assessment.},
  keywords = {Classification Algorithms,Deep Learning Models,Evaluation Masures,Feature extraction,Feature Extraction,Law,Machine learning algorithms,Manuals,Natural Language Processing,Preprocessing,Social networking (online),Support vector machines,Text categorization,Text Classification},
  file = {/Users/teetusaini/Zotero/storage/M7SAY3BV/Bhavani and Santhosh Kumar - 2021 - A Review of State Art of Text Classification Algor.pdf;/Users/teetusaini/Zotero/storage/WAPX38Y5/9418262.html}
}

@inproceedings{blundell_weight_2015,
  title = {Weight {{Uncertainty}} in {{Neural Network}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = jun,
  pages = {1613--1622},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularis...},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/SA5AQ6CJ/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf}
}

@inproceedings{brown_adversarial_2020,
  title = {The {{Adversarial UFP}}/{{UFN Attack}}: {{A New Threat}} to {{ML-based Fake News Detection Systems}}?},
  shorttitle = {The {{Adversarial UFP}}/{{UFN Attack}}},
  booktitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Brown, Brandon and Richardson, Alexicia and Smith, Marcellus and Dozier, Gerry and King, Michael C.},
  year = {2020},
  month = dec,
  pages = {1523--1527},
  doi = {10.1109/SSCI47803.2020.9308298},
  abstract = {In this paper, we propose two new attacks: the Adversarial Universal False Positive (UFP) Attack and the Adversarial Universal False Negative (UFN) Attack. The objective of this research is to introduce a new class of attack using only feature vector information. The results show the potential weaknesses of five machine learning (ML) classifiers. These classifiers include k-Nearest Neighbor (KNN), Naive Bayes (NB), Random Forrest (RF), a Support Vector Machine (SVM) with a Radial Basis Function (RBF) Kernel, and XGBoost (XGB).},
  keywords = {Computer science,Fake News Detection Systems,Feature extraction,Radio frequency,Social networking (online),Software engineering,Support vector machines,Universal False Negative,Universal False Positive,Voting},
  file = {/Users/teetusaini/Zotero/storage/I8TRWY5V/9308298.html}
}

@article{cer_universal_2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.11175 [cs]},
  eprint = {1803.11175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/G3XN23A6/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/Users/teetusaini/Zotero/storage/PZYUIY6N/1803.html}
}

@article{chakraborty_adversarial_2018,
  title = {Adversarial {{Attacks}} and {{Defences}}: {{A Survey}}},
  shorttitle = {Adversarial {{Attacks}} and {{Defences}}},
  author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  year = {2018},
  month = sep,
  journal = {arXiv:1810.00069 [cs, stat]},
  eprint = {1810.00069},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/WKQNEES3/Chakraborty et al. - 2018 - Adversarial Attacks and Defences A Survey.pdf;/Users/teetusaini/Zotero/storage/BQXYNW59/1810.html}
}

@inproceedings{chang_bias_2019,
  title = {Bias and {{Fairness}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}}): {{Tutorial Abstracts}}},
  author = {Chang, Kai-Wei and Prabhakaran, Vinod and Ordonez, Vicente},
  year = {2019},
  month = nov,
  abstract = {Kai-Wei Chang, Vinod Prabhakaran, Vicente Ordonez. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts. 2019.},
  langid = {american},
  file = {/Users/teetusaini/Zotero/storage/P8MAFGVT/D19-2004.html}
}

@article{chaudhari_attentive_2021,
  title = {An {{Attentive Survey}} of {{Attention Models}}},
  author = {Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  year = {2021},
  month = jul,
  journal = {arXiv:1904.02874 [cs, stat]},
  eprint = {1904.02874},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated, and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/2V58VKDA/Chaudhari et al. - 2021 - An Attentive Survey of Attention Models.pdf;/Users/teetusaini/Zotero/storage/T9H2SYZZ/1904.html}
}

@article{chen_robustness_2019,
  title = {Robustness {{Verification}} of {{Tree-based Models}}},
  author = {Chen, Hongge and Zhang, Huan and Si, Si and Li, Yang and Boning, Duane and Hsieh, Cho-Jui},
  year = {2019},
  month = dec,
  journal = {arXiv:1906.03849 [cs, stat]},
  eprint = {1906.03849},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We study the robustness verification problem for tree-based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles, the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on the robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. OnRF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than the previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/RR25BMEX/Chen et al. - 2019 - Robustness Verification of Tree-based Models.pdf;/Users/teetusaini/Zotero/storage/UZS4KWBJ/1906.html}
}

@article{cheng_seq2sick_2020,
  title = {{{Seq2Sick}}: {{Evaluating}} the {{Robustness}} of {{Sequence-to-Sequence Models}} with {{Adversarial Examples}}},
  shorttitle = {{{Seq2Sick}}},
  author = {Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
  year = {2020},
  month = apr,
  journal = {arXiv:1803.01128 [cs]},
  eprint = {1803.01128},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/7SS3FL2R/Cheng et al. - 2020 - Seq2Sick Evaluating the Robustness of Sequence-to.pdf;/Users/teetusaini/Zotero/storage/CCK2ELTQ/1803.html}
}

@article{cheng_seq2sick_2020-1,
  title = {{{Seq2Sick}}: {{Evaluating}} the {{Robustness}} of {{Sequence-to-Sequence Models}} with {{Adversarial Examples}}},
  shorttitle = {{{Seq2Sick}}},
  author = {Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
  year = {2020},
  month = apr,
  journal = {arXiv:1803.01128 [cs]},
  eprint = {1803.01128},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/WWLPP8RE/Cheng et al. - 2020 - Seq2Sick Evaluating the Robustness of Sequence-to.pdf;/Users/teetusaini/Zotero/storage/EPSSN3KE/1803.html}
}

@article{cho_properties_2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  month = oct,
  journal = {arXiv:1409.1259 [cs, stat]},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/H99XZZEQ/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf;/Users/teetusaini/Zotero/storage/UIFM2NE6/1409.html}
}

@article{chowdhury_natural_2003,
  title = {Natural Language Processing},
  author = {Chowdhury, Gobinda G.},
  year = {2003},
  journal = {Annual Review of Information Science and Technology},
  volume = {37},
  number = {1},
  pages = {51--89},
  issn = {1550-8382},
  doi = {10.1002/aris.1440370103},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440370103},
  file = {/Users/teetusaini/Zotero/storage/BW3UFKUJ/Chowdhury - 2003 - Natural language processing.pdf;/Users/teetusaini/Zotero/storage/ML4HD95B/aris.html}
}

@inproceedings{croce_provable_2019,
  title = {Provable {{Robustness}} of {{ReLU}} Networks via {{Maximization}} of {{Linear Regions}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
  year = {2019},
  month = apr,
  pages = {2057--2066},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networ...},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/7AKZ2WRE/Croce et al. - 2019 - Provable Robustness of ReLU networks via Maximizat.pdf;/Users/teetusaini/Zotero/storage/UBABYME2/croce19a.html}
}

@article{danilevsky_survey_2020,
  title = {A {{Survey}} of the {{State}} of {{Explainable AI}} for {{Natural Language Processing}}},
  author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.00711 [cs]},
  eprint = {2010.00711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/Users/teetusaini/Zotero/storage/GCTX2IHE/Danilevsky et al. - 2020 - A Survey of the State of Explainable AI for Natura.pdf;/Users/teetusaini/Zotero/storage/JA76ZWKN/2010.html}
}

@article{das_automated_2019,
  title = {Automated Email {{Generation}} for {{Targeted Attacks}} Using {{Natural Language}}},
  author = {Das, Avisha and Verma, Rakesh},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.06893 [cs]},
  eprint = {1908.06893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With an increasing number of malicious attacks, the number of people and organizations falling prey to social engineering attacks is proliferating. Despite considerable research in mitigation systems, attackers continually improve their modus operandi by using sophisticated machine learning, natural language processing techniques with an intent to launch successful targeted attacks aimed at deceiving detection mechanisms as well as the victims. We propose a system for advanced email masquerading attacks using Natural Language Generation (NLG) techniques. Using legitimate as well as an influx of varying malicious content, the proposed deep learning system generates \textbackslash textit\{fake\} emails with malicious content, customized depending on the attacker's intent. The system leverages Recurrent Neural Networks (RNNs) for automated text generation. We also focus on the performance of the generated emails in defeating statistical detectors, and compare and analyze the emails using a proposed baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/Users/teetusaini/Zotero/storage/8Q6YQYDI/Das and Verma - 2019 - Automated email Generation for Targeted Attacks us.pdf;/Users/teetusaini/Zotero/storage/EC7T9S4U/1908.html}
}

@article{devlin_bert_2019-1,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/2XYZKUZY/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/teetusaini/Zotero/storage/YDRXA6K7/1810.html}
}

@article{dong_towards_2021-1,
  title = {{{TOWARDS ROBUSTNESS AGAINST NATURAL LANGUAGE WORD SUBSTITUTIONS}}},
  author = {Dong, Xinshuai and Luu, Anh Tuan and Ji, Rongrong and Liu, Hong},
  year = {2021},
  pages = {14},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/G53HXVLC/Dong et al. - 2021 - TOWARDS ROBUSTNESS AGAINST NATURAL LANGUAGE WORD S.pdf}
}

@article{dong_towards_2021-2,
  title = {Towards {{Robustness Against Natural Language Word Substitutions}}},
  author = {Dong, Xinshuai and Luu, Anh Tuan and Ji, Rongrong and Liu, Hong},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.13541 [cs]},
  eprint = {2107.13541},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either \$l\_2\$-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel \textbackslash textit\{Adversarial Sparse Convex Combination\} (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on the ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, \textbackslash emph\{i.e.\}, sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/TUC7SVZU/Dong et al. - 2021 - Towards Robustness Against Natural Language Word S.pdf;/Users/teetusaini/Zotero/storage/SYXZMWPA/2107.html}
}

@inproceedings{du_adversarial_2020,
  title = {Adversarial and {{Domain-Aware BERT}} for {{Cross-Domain Sentiment Analysis}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Du, Chunning and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  year = {2020},
  month = jul,
  pages = {4019--4028},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.370},
  abstract = {Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.},
  file = {/Users/teetusaini/Zotero/storage/9BX4SLA2/Du et al. - 2020 - Adversarial and Domain-Aware BERT for Cross-Domain.pdf}
}

@inproceedings{ebrahimi_hotflip_2018,
  title = {{{HotFlip}}: {{White-Box Adversarial Examples}} for {{Text Classification}}},
  shorttitle = {{{HotFlip}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
  year = {2018},
  month = jul,
  pages = {31--36},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2006},
  abstract = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.},
  file = {/Users/teetusaini/Zotero/storage/IJRZGNT4/Ebrahimi et al. - 2018 - HotFlip White-Box Adversarial Examples for Text C.pdf}
}

@article{ebrahimi_how_2021-1,
  title = {How {{Does Adversarial Fine-Tuning Benefit BERT}}?},
  author = {Ebrahimi, Javid and Yang, Hao and Zhang, Wei},
  year = {2021},
  month = sep,
  journal = {arXiv:2108.13602 [cs]},
  eprint = {2108.13602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial training (AT) is one of the most reliable methods for defending against adversarial attacks in machine learning. Variants of this method have been used as regularization mechanisms to achieve SOTA results on NLP benchmarks, and they have been found to be useful for transfer learning and continual learning. We search for the reasons for the effectiveness of AT by contrasting vanilla and adversarially fine-tuned BERT models. We identify partial preservation of BERT's syntactic abilities during fine-tuning as the key to the success of AT. We observe that adversarially fine-tuned models remain more faithful to BERT's language modeling behavior and are more sensitive to the word order. As concrete examples of syntactic abilities, an adversarially fine-tuned model could have an advantage of up to 38\% on anaphora agreement and up to 11\% on dependency parsing. Our analysis demonstrates that vanilla fine-tuning oversimplifies the sentence representation by focusing heavily on a small subset of words. AT, however, moderates the effect of these influential words and encourages representational diversity. This allows for a more hierarchical representation of a sentence and leads to the mitigation of BERT's loss of syntactic abilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/MTXWKY4K/Ebrahimi et al. - 2021 - How Does Adversarial Fine-Tuning Benefit BERT.pdf;/Users/teetusaini/Zotero/storage/YSBT6GVG/2108.html}
}

@inproceedings{eger_text_2019,
  title = {Text {{Processing Like Humans Do}}: {{Visually Attacking}} and {{Shielding NLP Systems}}},
  shorttitle = {Text {{Processing Like Humans Do}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Eger, Steffen and {\c S}ahin, G{\"o}zde G{\"u}l and R{\"u}ckl{\'e}, Andreas and Lee, Ji-Ung and Schulz, Claudia and Mesgar, Mohsen and Swarnkar, Krishnkant and Simpson, Edwin and Gurevych, Iryna},
  year = {2019},
  month = jun,
  pages = {1634--1647},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1165},
  abstract = {Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., ``!d10t'') or as a writing style (``1337'' in ``leet speak''), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82\%. We then explore three shielding methods\textemdash visual character embeddings, adversarial training, and rule-based recovery\textemdash which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.},
  file = {/Users/teetusaini/Zotero/storage/TR5USAIR/Eger et al. - 2019 - Text Processing Like Humans Do Visually Attacking.pdf}
}

@article{englesson_consistency_2021,
  title = {Consistency {{Regularization Can Improve Robustness}} to {{Label Noise}}},
  author = {Englesson, Erik and Azizpour, Hossein},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.01242 [cs, stat]},
  eprint = {2110.01242},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings test-time robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for training-time robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and(ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/SUUW7WQU/Englesson and Azizpour - 2021 - Consistency Regularization Can Improve Robustness .pdf;/Users/teetusaini/Zotero/storage/N26NRG3E/2110.html}
}

@article{feng_survey_2021,
  title = {A {{Survey}} of {{Data Augmentation Approaches}} for {{NLP}}},
  author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
  year = {2021},
  month = may,
  journal = {arXiv:2105.03075 [cs]},
  eprint = {2105.03075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/IS67P73S/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf;/Users/teetusaini/Zotero/storage/P2M75A5R/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.html}
}

@article{foster_generative_nodate,
  title = {Generative {{Deep Learning}}},
  author = {Foster, David},
  pages = {330},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/NY7VZDTH/Foster - Generative Deep Learning.pdf}
}

@inproceedings{fukui_attention_2019,
  title = {Attention {{Branch Network}}: {{Learning}} of {{Attention Mechanism}} for {{Visual Explanation}}},
  shorttitle = {Attention {{Branch Network}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fukui, Hiroshi and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
  year = {2019},
  pages = {10705--10714},
  file = {/Users/teetusaini/Zotero/storage/ZXM4DJXT/Fukui et al. - 2019 - Attention Branch Network Learning of Attention Me.pdf;/Users/teetusaini/Zotero/storage/SWA4E28K/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019.html}
}

@inproceedings{gan_improving_2019,
  title = {Improving the {{Robustness}} of {{Question Answering Systems}} to {{Question Paraphrasing}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gan, Wee Chung and Ng, Hwee Tou},
  year = {2019},
  month = jul,
  pages = {6065--6075},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1610},
  abstract = {Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models' over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.},
  file = {/Users/teetusaini/Zotero/storage/6I83LU6Z/Gan and Ng - 2019 - Improving the Robustness of Question Answering Sys.pdf}
}

@article{gao_black-box_2018,
  title = {Black-Box {{Generation}} of {{Adversarial Text Sequences}} to {{Evade Deep Learning Classifiers}}},
  author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  year = {2018},
  month = may,
  journal = {arXiv:1801.04354 [cs]},
  eprint = {1801.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68\textbackslash\% on average for a Word-LSTM model and 48\textbackslash\% on average for a Char-CNN model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/EG7T53MX/Gao et al. - 2018 - Black-box Generation of Adversarial Text Sequences.pdf;/Users/teetusaini/Zotero/storage/VBZBNZWC/1801.html}
}

@article{garg_bae_2020,
  title = {{{BAE}}: {{BERT-based Adversarial Examples}} for {{Text Classification}}},
  shorttitle = {{{BAE}}},
  author = {Garg, Siddhant and Ramakrishnan, Goutham},
  year = {2020},
  month = oct,
  journal = {arXiv:2004.01970 [cs]},
  eprint = {2004.01970},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/JSNHP4XP/Garg and Ramakrishnan - 2020 - BAE BERT-based Adversarial Examples for Text Clas.pdf;/Users/teetusaini/Zotero/storage/DUJWQ6ET/2004.html}
}

@inproceedings{garg_bae_2020-1,
  title = {{{BAE}}: {{BERT-based Adversarial Examples}} for {{Text Classification}}},
  shorttitle = {{{BAE}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Garg, Siddhant and Ramakrishnan, Goutham},
  year = {2020},
  month = nov,
  pages = {6174--6181},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.498},
  abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
  file = {/Users/teetusaini/Zotero/storage/73IQHHX5/Garg and Ramakrishnan - 2020 - BAE BERT-based Adversarial Examples for Text Clas.pdf}
}

@article{goodfellow_explaining_2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  journal = {arXiv:1412.6572 [cs, stat]},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/DRELKALP/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/D5T7IWEX/1412.html}
}

@article{goodman_fastwordbug_2020,
  title = {{{FastWordBug}}: {{A Fast Method To Generate Adversarial Text Against NLP Applications}}},
  shorttitle = {{{FastWordBug}}},
  author = {Goodman, Dou and Zhonghou, Lv and {minghua}, Wang},
  year = {2020},
  month = jan,
  journal = {arXiv:2002.00760 [cs]},
  eprint = {2002.00760},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we present a novel algorithm, FastWordBug, to efficiently generate small text perturbations in a black-box setting that forces a sentiment analysis or text classification mode to make an incorrect prediction. By combining the part of speech attributes of words, we propose a scoring method that can quickly identify important words that affect text classification. We evaluate FastWordBug on three real-world text datasets and two state-of-the-art machine learning models under black-box setting. The results show that our method can significantly reduce the accuracy of the model, and at the same time, we can call the model as little as possible, with the highest attack efficiency. We also attack two popular real-world cloud services of NLP, and the results show that our method works as well.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/Users/teetusaini/Zotero/storage/6IC8SB4M/Goodman et al. - 2020 - FastWordBug A Fast Method To Generate Adversarial.pdf;/Users/teetusaini/Zotero/storage/VXNLG57V/2002.html}
}

@article{graves_neural_2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  journal = {arXiv:1410.5401 [cs]},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/teetusaini/Zotero/storage/ANRNY9VB/Graves et al. - 2014 - Neural Turing Machines.pdf;/Users/teetusaini/Zotero/storage/S3HNAZY3/1410.html}
}

@article{guan_robustness_2020,
  title = {Robustness {{Verification}} of {{Quantum Machine Learning}}},
  author = {Guan, Ji and Fang, Wang and Ying, Mingsheng},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.07230 [quant-ph]},
  eprint = {2008.07230},
  eprinttype = {arxiv},
  primaryclass = {quant-ph},
  abstract = {Several important models of machine learning algorithms have been successfully generalized to the quantum world, with potential applications to data analytics in quantum physics that can be implemented on the near future quantum computers. However, noise and decoherence are two major obstacles to the practical implementation of quantum machine learning. In this work, we introduce a general framework for the robustness analysis of quantum machine learning algorithms against noise and decoherence. We argue that fidelity is the only pick of measuring the robustness. A robust bound is derived and an algorithm is developed to check whether or not a quantum machine learning algorithm is robust with respect to the training data. In particular, this algorithm can help to defense attacks and improve the accuracy as it can identify useful new training data during checking. The effectiveness of our robust bound and algorithm is confirmed by the case study of quantum phase recognition. Furthermore, this experiment demonstrates a trade-off between the accuracy of quantum machine learning algorithms and their robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantum Physics},
  file = {/Users/teetusaini/Zotero/storage/NTUGYAB9/Guan et al. - 2020 - Robustness Verification of Quantum Machine Learnin.pdf;/Users/teetusaini/Zotero/storage/AJISHQKH/2008.html}
}

@inproceedings{guo_gradient-based_2021,
  title = {Gradient-Based {{Adversarial Attacks}} against {{Text Transformers}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Guo, Chuan and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e} and Kiela, Douwe},
  year = {2021},
  month = nov,
  pages = {5747--5757},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.464},
  abstract = {We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.},
  file = {/Users/teetusaini/Zotero/storage/W2JK452A/Guo et al. - 2021 - Gradient-based Adversarial Attacks against Text Tr.pdf}
}

@article{guo_rosearch_2021,
  title = {{{RoSearch}}: {{Search}} for {{Robust Student Architectures When Distilling Pre-trained Language Models}}},
  shorttitle = {{{RoSearch}}},
  author = {Guo, Xin and Yang, Jianlei and Zhou, Haoyi and Ye, Xucheng and Li, Jianxin},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.03613 [cs]},
  eprint = {2106.03613},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7\%\textasciitilde 18\% up to 45.8\%\textasciitilde 47.8\% on different datasets with comparable weight compression ratio to existing distillation methods (4.6\$\textbackslash times\$\textasciitilde 6.5\$\textbackslash times\$ improvement from teacher model BERT\_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/NESR8SR8/Guo et al. - 2021 - RoSearch Search for Robust Student Architectures .pdf;/Users/teetusaini/Zotero/storage/7Q8E7CWV/2106.html}
}

@article{guo_towards_2021,
  title = {Towards {{Variable-Length Textual Adversarial Attacks}}},
  author = {Guo, Junliang and Zhang, Zhirui and Zhang, Linlin and Xu, Linli and Chen, Boxing and Chen, Enhong and Luo, Weihua},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.08139 [cs]},
  eprint = {2104.08139},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial attacks have shown the vulnerability of machine learning models, however, it is non-trivial to conduct textual adversarial attacks on natural language processing tasks due to the discreteness of data. Most previous approaches conduct attacks with the atomic \textbackslash textit\{replacement\} operation, which usually leads to fixed-length adversarial examples and therefore limits the exploration on the decision space. In this paper, we propose variable-length textual adversarial attacks\textasciitilde (VL-Attack) and integrate three atomic operations, namely \textbackslash textit\{insertion\}, \textbackslash textit\{deletion\} and \textbackslash textit\{replacement\}, into a unified framework, by introducing and manipulating a special \textbackslash textit\{blank\} token while attacking. In this way, our approach is able to more comprehensively find adversarial examples around the decision boundary and effectively conduct adversarial attacks. Specifically, our method drops the accuracy of IMDB classification by \$96\textbackslash\%\$ with only editing \$1.3\textbackslash\%\$ tokens while attacking a pre-trained BERT model. In addition, fine-tuning the victim model with generated adversarial samples can improve the robustness of the model without hurting the performance, especially for length-sensitive models. On the task of non-autoregressive machine translation, our method can achieve \$33.18\$ BLEU score on IWSLT14 German-English translation, achieving an improvement of \$1.47\$ over the baseline model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/ZBSNKUYA/Guo et al. - 2021 - Towards Variable-Length Textual Adversarial Attack.pdf;/Users/teetusaini/Zotero/storage/FPP6WVZS/2104.html}
}

@inproceedings{han_adversarial_2020,
  title = {Adversarial {{Attack}} and {{Defense}} of {{Structured Prediction Models}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Han, Wenjuan and Zhang, Liwen and Jiang, Yong and Tu, Kewei},
  year = {2020},
  month = nov,
  pages = {2327--2338},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.182},
  abstract = {Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.},
  file = {/Users/teetusaini/Zotero/storage/YP2RN4DW/Han et al. - 2020 - Adversarial Attack and Defense of Structured Predi.pdf}
}

@inproceedings{han_robust_2021,
  title = {Robust {{Transfer Learning}} with {{Pretrained Language Models}} through {{Adapters}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Han, Wenjuan and Pang, Bo and Wu, Ying Nian},
  year = {2021},
  month = aug,
  pages = {854--861},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-short.108},
  abstract = {Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.},
  file = {/Users/teetusaini/Zotero/storage/LDI8A3F8/Han et al. - 2021 - Robust Transfer Learning with Pretrained Language .pdf}
}

@article{han_robust_2021-1,
  title = {Robust {{Transfer Learning}} with {{Pretrained Language Models}} through {{Adapters}}},
  author = {Han, Wenjuan and Pang, Bo and Wu, Yingnian},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.02340 [cs]},
  eprint = {2108.02340},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/346THKJU/Han et al. - 2021 - Robust Transfer Learning with Pretrained Language .pdf;/Users/teetusaini/Zotero/storage/IGJ93IRF/2108.html}
}

@article{hao_visualizing_2019,
  title = {Visualizing and {{Understanding}} the {{Effectiveness}} of {{BERT}}},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.05620 [cs]},
  eprint = {1908.05620},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/QNNLHL3A/Hao et al. - 2019 - Visualizing and Understanding the Effectiveness of.pdf;/Users/teetusaini/Zotero/storage/2I4SDZ7S/1908.html}
}

@article{harrag_bert_2021,
  title = {{{BERT Transformer}} Model for {{Detecting Arabic GPT2 Auto-Generated Tweets}}},
  author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.09345 [cs]},
  eprint = {2101.09345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/H89SQSFV/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf;/Users/teetusaini/Zotero/storage/GIY6CWB7/2101.html}
}

@article{harrag_bert_2021-1,
  title = {{{BERT Transformer}} Model for {{Detecting Arabic GPT2 Auto-Generated Tweets}}},
  author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.09345 [cs]},
  eprint = {2101.09345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/LRM5P6TE/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf;/Users/teetusaini/Zotero/storage/S8PQI464/2101.html}
}

@article{harrag_bert_2021-2,
  title = {{{BERT Transformer}} Model for {{Detecting Arabic GPT2 Auto-Generated Tweets}}},
  author = {Harrag, Fouzi and Debbah, Maria and Darwish, Kareem and Abdelali, Ahmed},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.09345 [cs]},
  eprint = {2101.09345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {During the last two decades, we have progressively turned to the Internet and social media to find news, entertain conversations and share opinion. Recently, OpenAI has developed a ma-chine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can pro-duce deepfake texts. It can generate blocks of text based on brief writing prompts that look like they were written by humans, facilitating the spread false or auto-generated text. In line with this progress, and in order to counteract potential dangers, several methods have been pro-posed for detecting text written by these language models. In this paper, we propose a transfer learning based model that will be able to detect if an Arabic sentence is written by humans or automatically generated by bots. Our dataset is based on tweets from a previous work, which we have crawled and extended using the Twitter API. We used GPT2-Small-Arabic to generate fake Arabic Sentences. For evaluation, we compared different recurrent neural network (RNN) word embeddings based baseline models, namely: LSTM, BI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new transfer-learning model has obtained an accuracy up to 98\%. To the best of our knowledge, this work is the first study where ARABERT and GPT2 were combined to detect and classify the Arabic auto-generated texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/D3IAG7C6/Harrag et al. - 2021 - BERT Transformer model for Detecting Arabic GPT2 A.pdf;/Users/teetusaini/Zotero/storage/KKLSASUP/2101.html}
}

@article{harris_distributional_1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {\emph{WORD}},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/D6LMPG66/Harris - 1954 - Distributional Structure.pdf}
}

@inproceedings{he_model_2021,
  title = {Model {{Extraction}} and {{Adversarial Transferability}}, {{Your BERT}} Is {{Vulnerable}}!},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {He, Xuanli and Lyu, Lingjuan and Sun, Lichao and Xu, Qiongkai},
  year = {2021},
  month = jun,
  pages = {2006--2012},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.161},
  abstract = {Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.},
  file = {/Users/teetusaini/Zotero/storage/2IXJP823/He et al. - 2021 - Model Extraction and Adversarial Transferability, .pdf}
}

@article{henriksson_performance_2021,
  title = {Performance {{Analysis}} of {{Out-of-Distribution Detection}} on {{Various Trained Neural Networks}}},
  author = {Henriksson, Jens and Berger, Christian and Borg, Markus and Tornberg, Lars and Sathyamoorthy, Sankar Raman and Englund, Cristofer},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.15580 [cs]},
  eprint = {2103.15580},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Several areas have been improved with Deep Learning during the past years. For non-safety related products adoption of AI and ML is not an issue, whereas in safety critical applications, robustness of such approaches is still an issue. A common challenge for Deep Neural Networks (DNN) occur when exposed to out-of-distribution samples that are previously unseen, where DNNs can yield high confidence predictions despite no prior knowledge of the input. In this paper we analyse two supervisors on two well-known DNNs with varied setups of training and find that the outlier detection performance improves with the quality of the training procedure. We analyse the performance of the supervisor after each epoch during the training cycle, to investigate supervisor performance as the accuracy converges. Understanding the relationship between training results and supervisor performance is valuable to improve robustness of the model and indicates where more work has to be done to create generalized models for safety critical applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,D.2.5},
  file = {/Users/teetusaini/Zotero/storage/VNFVQYTU/Henriksson et al. - 2021 - Performance Analysis of Out-of-Distribution Detect.pdf;/Users/teetusaini/Zotero/storage/2WGLHH3J/2103.html}
}

@article{hinton_distilling_2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.02531 [cs, stat]},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/HFZ2X9ZD/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/Users/teetusaini/Zotero/storage/EZW9DM3D/1503.html}
}

@article{hitaj_capture_2020,
  title = {Capture the {{Bot}}: {{Using Adversarial Examples}} to {{Improve CAPTCHA Robustness}} to {{Bot Attacks}}},
  shorttitle = {Capture the {{Bot}}},
  author = {Hitaj, Dorjan and Hitaj, Briland and Jajodia, Sushil and Mancini, Luigi V.},
  year = {2020},
  month = nov,
  journal = {arXiv:2010.16204 [cs]},
  eprint = {2010.16204},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {To this date, CAPTCHAs have served as the first line of defense preventing unauthorized access by (malicious) bots to web-based services, while at the same time maintaining a trouble-free experience for human visitors. However, recent work in the literature has provided evidence of sophisticated bots that make use of advancements in machine learning (ML) to easily bypass existing CAPTCHA-based defenses. In this work, we take the first step to address this problem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial examples. While typically adversarial examples are used to lead an ML model astray, with CAPTURE, we attempt to make a "good use" of such mechanisms. Our empirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to solve by humans while at the same time, effectively thwarting ML-based bot solvers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/FH2XDY22/Hitaj et al. - 2020 - Capture the Bot Using Adversarial Examples to Imp.pdf;/Users/teetusaini/Zotero/storage/JSX8YKM9/2010.html}
}

@article{hossam_explain2attack_2021,
  title = {{{Explain2Attack}}: {{Text Adversarial Attacks}} via {{Cross-Domain Interpretability}}},
  shorttitle = {{{Explain2Attack}}},
  author = {Hossam, Mahmoud and Le, Trung and Zhao, He and Phung, Dinh},
  year = {2021},
  month = jan,
  journal = {arXiv:2010.06812 [cs]},
  eprint = {2010.06812},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Training robust deep learning models for down-stream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,I.2.0,I.5.0},
  file = {/Users/teetusaini/Zotero/storage/6YRBXH46/Hossam et al. - 2021 - Explain2Attack Text Adversarial Attacks via Cross.pdf;/Users/teetusaini/Zotero/storage/TXKSDKZU/2010.html}
}

@article{howard_universal_2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = may,
  journal = {arXiv:1801.06146 [cs, stat]},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/4FZGA7LD/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf;/Users/teetusaini/Zotero/storage/7AK3LRJS/1801.html}
}

@inproceedings{hsieh_robustness_2019,
  title = {On the {{Robustness}} of {{Self-Attentive Models}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hsieh, Yu-Lun and Cheng, Minhao and Juan, Da-Cheng and Wei, Wei and Hsu, Wen-Lian and Hsieh, Cho-Jui},
  year = {2019},
  month = jul,
  pages = {1520--1529},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1147},
  abstract = {This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.},
  file = {/Users/teetusaini/Zotero/storage/XL46HHGJ/Hsieh et al. - 2019 - On the Robustness of Self-Attentive Models.pdf}
}

@article{huq_adversarial_2020,
  title = {Adversarial {{Attacks}} and {{Defense}} on {{Texts}}: {{A Survey}}},
  shorttitle = {Adversarial {{Attacks}} and {{Defense}} on {{Texts}}},
  author = {Huq, Aminul and Pervin, Mst Tasnim},
  year = {2020},
  month = may,
  abstract = {Deep learning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis, and many others. However, in recent years it has been shown that these models possess weakness to noises which force the model to misclassify. This issue has been studied profoundly in the image and audio domain. Very little has been studied on this issue concerning textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript, we accumulated and analyzed different attacking techniques and various defense models to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome to move forward in this field.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/SQ5H7Z55/Huq and Pervin - 2020 - Adversarial Attacks and Defense on Texts A Survey.pdf;/Users/teetusaini/Zotero/storage/WQXJECUM/2005.html}
}

@article{jia_certified_2019,
  title = {Certified {{Robustness}} to {{Adversarial Word Substitutions}}},
  author = {Jia, Robin and Raghunathan, Aditi and G{\"o}ksel, Kerem and Liang, Percy},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.00986 [cs]},
  eprint = {1909.00986},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75\textbackslash\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8\textbackslash\%\$ and \$35\textbackslash\%\$, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/CWP6P4NX/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf;/Users/teetusaini/Zotero/storage/68BPL5HA/1909.html}
}

@article{jia_certified_2019-1,
  title = {Certified {{Robustness}} to {{Adversarial Word Substitutions}}},
  author = {Jia, Robin and Raghunathan, Aditi and G{\"o}ksel, Kerem and Liang, Percy},
  year = {2019},
  month = sep,
  abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75\textbackslash\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8\textbackslash\%\$ and \$35\textbackslash\%\$, respectively.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/JG9P53X6/Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf;/Users/teetusaini/Zotero/storage/XYYGXDDM/1909.html}
}

@article{jiang_smart_2020,
  title = {{{SMART}}: {{Robust}} and {{Efficient Fine-Tuning}} for {{Pre-trained Natural Language Models}} through {{Principled Regularized Optimization}}},
  shorttitle = {{{SMART}}},
  author = {Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, T.},
  year = {2020},
  journal = {ACL},
  doi = {10.18653/v1/2020.acl-main.197},
  abstract = {A new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance and outperforms the state-of-the-art T5 model, which is the largest pre- trained model containing 11 billion parameters, on GLUE. Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.},
  file = {/Users/teetusaini/Zotero/storage/66PR5DUA/Jiang et al. - 2020 - SMART Robust and Efficient Fine-Tuning for Pre-tr.pdf}
}

@article{jiang_smart_2020-1,
  title = {{{SMART}}: {{Robust}} and {{Efficient Fine-Tuning}} for {{Pre-trained Natural Language Models}} through {{Principled Regularized Optimization}}},
  shorttitle = {{{SMART}}},
  author = {Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  year = {2020},
  journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  eprint = {1911.03437},
  eprinttype = {arxiv},
  pages = {2177--2190},
  doi = {10.18653/v1/2020.acl-main.197},
  abstract = {Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research. Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model. To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models. Specifically, our proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the capacity of the model; 2. Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting. Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/teetusaini/Zotero/storage/2WXSMWUY/Jiang et al. - 2020 - SMART Robust and Efficient Fine-Tuning for Pre-tr.pdf;/Users/teetusaini/Zotero/storage/27C47Y7D/1911.html}
}

@inproceedings{jiao_brief_2021,
  title = {A {{Brief Survey}} of {{Word Embedding}} and {{Its Recent Development}}},
  booktitle = {2021 {{IEEE}} 5th {{Advanced Information Technology}}, {{Electronic}} and {{Automation Control Conference}} ({{IAEAC}})},
  author = {Jiao, Qilu and Zhang, Shunyao},
  year = {2021},
  month = mar,
  volume = {5},
  pages = {1697--1701},
  issn = {2689-6621},
  doi = {10.1109/IAEAC50856.2021.9390956},
  abstract = {Learning effective representations of text words has long been a research focus in natural language processing and other machine learning tasks. In many early tasks, a text word is often represented by a one-hot vector in a discrete manner. Such a solution is not only restricted by the dimension curse, but also unable to reflect the semantic relationships between words. Recent developments focus on the learning of low-dimension and continuous vector representations of text words, known as word embedding, which can be easily applied to downstream tasks such as machine translation, natural language inference, semantic analysis and so on. In this paper, we will introduce the development of word embedding, describe the representative methods, and report its recent research trend. This paper can provide a quick guide for understanding the principle of word embedding and its development.},
  keywords = {BERT,Information technology,Machine learning,Machine translation,Market research,natural language processing,Process control,Semantics,Task analysis,word embedding,word2vec},
  file = {/Users/teetusaini/Zotero/storage/G3C36CTT/Jiao and Zhang - 2021 - A Brief Survey of Word Embedding and Its Recent De.pdf}
}

@article{jin_is_2020,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8018--8025},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6311},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/5V8MIJXJ/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf;/Users/teetusaini/Zotero/storage/K4TFYJ5B/6311.html}
}

@article{jin_is_2020-1,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  year = {2020},
  month = apr,
  journal = {arXiv:1907.11932 [cs]},
  eprint = {1907.11932},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/5B9H8NR9/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf;/Users/teetusaini/Zotero/storage/VFK26WA8/1907.html}
}

@article{jones_robust_2020,
  title = {Robust {{Encodings}}: {{A Framework}} for {{Combating Adversarial Typos}}},
  shorttitle = {Robust {{Encodings}}},
  author = {Jones, Erik and Jia, Robin and Raghunathan, Aditi and Liang, Percy},
  year = {2020},
  month = may,
  abstract = {Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3\% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3\% accuracy against a simple greedy attack.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/VTFHSLC4/Jones et al. - 2020 - Robust Encodings A Framework for Combating Advers.pdf}
}

@article{kardakis_examining_2021,
  title = {Examining {{Attention Mechanisms}} in {{Deep Learning Models}} for {{Sentiment Analysis}}},
  author = {Kardakis, Spyridon and Perikos, Isidoros and Grivokostopoulou, Foteini and Hatzilygeroudis, Ioannis},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {9},
  pages = {3883},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app11093883},
  abstract = {Attention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models' ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5\% improvement in their accuracy.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {attention mechanism,deep neural networks,global-attention,hierarchical-attention,self-attention,sentiment analysis},
  file = {/Users/teetusaini/Zotero/storage/QGWUZSPA/Kardakis et al. - 2021 - Examining Attention Mechanisms in Deep Learning Mo.pdf}
}

@article{karimi_adversarial_2020,
  title = {Adversarial {{Training}} for {{Aspect-Based Sentiment Analysis}} with {{BERT}}},
  author = {Karimi, Akbar and Rossi, Leonardo and Prati, Andrea},
  year = {2020},
  month = oct,
  journal = {arXiv:2001.11316 [cs, stat]},
  eprint = {2001.11316},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of sentiments and their targets. Collecting labeled data for this task in order to help neural networks generalize better can be laborious and time-consuming. As an alternative, similar data to the real-world examples can be produced artificially through an adversarial process which is carried out in the embedding space. Although these examples are not real sentences, they have been shown to act as a regularization method which can make neural networks more robust. In this work, we apply adversarial training, which was put forward by Goodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model proposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and Aspect Sentiment Classification in sentiment analysis. After improving the results of post-trained BERT by an ablation study, we propose a novel architecture called BERT Adversarial Training (BAT) to utilize adversarial training in ABSA. The proposed model outperforms post-trained BERT in both tasks. To the best of our knowledge, this is the first study on the application of adversarial training in ABSA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/92J6CURV/Karimi et al. - 2020 - Adversarial Training for Aspect-Based Sentiment An.pdf;/Users/teetusaini/Zotero/storage/DQMATEA5/2001.html}
}

@inproceedings{keller_bert-defense_2021,
  title = {{{BERT-Defense}}: {{A Probabilistic Model Based}} on {{BERT}} to {{Combat Cognitively Inspired Orthographic Adversarial Attacks}}},
  shorttitle = {{{BERT-Defense}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Keller, Yannik and Mackensen, Jan and Eger, Steffen},
  year = {2021},
  month = aug,
  pages = {1616--1629},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.141},
  file = {/Users/teetusaini/Zotero/storage/I3PH88A2/Keller et al. - 2021 - BERT-Defense A Probabilistic Model Based on BERT .pdf}
}

@article{kim_convolutional_2014,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  author = {Kim, Yoon},
  year = {2014},
  month = sep,
  journal = {arXiv:1408.5882 [cs]},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/teetusaini/Zotero/storage/FBJDJRDM/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf;/Users/teetusaini/Zotero/storage/I9B6Y7MS/1408.html}
}

@article{kingma_introduction_2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/JF5K7WWC/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/Users/teetusaini/Zotero/storage/62AI9SLL/1906.html}
}

@article{kowsari_text_2019,
  title = {Text {{Classification Algorithms}}: {{A Survey}}},
  shorttitle = {Text {{Classification Algorithms}}},
  author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
  year = {2019},
  month = apr,
  journal = {Information},
  volume = {10},
  number = {4},
  pages = {150},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/info10040150},
  abstract = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {document classification,text analysis,text categorization,text classification,text mining,text representation},
  file = {/Users/teetusaini/Zotero/storage/RJKYHMDY/Kowsari et al. - 2019 - Text Classification Algorithms A Survey.pdf;/Users/teetusaini/Zotero/storage/I2HVCQS4/150.html}
}

@article{lan_albert_2020,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.11942 [cs]},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \textbackslash squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/2NNW9ILA/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf;/Users/teetusaini/Zotero/storage/KQ53JPPS/1909.html}
}

@article{langendoen_review_1964,
  title = {Review of {{Studies}} in {{Linguistic Analysis}}, Ed. by {{J}}. {{R}}. {{Firth}}},
  author = {Langendoen, Donald},
  year = {1964},
  month = apr,
  journal = {Language},
  volume = {40},
  pages = {305--321},
  doi = {10.2307/411592},
  file = {/Users/teetusaini/Zotero/storage/E5ZQZY3H/Langendoen - 1964 - Review of Studies in Linguistic Analysis, ed. by J.pdf}
}

@article{le_malcom_2020-1,
  title = {{{MALCOM}}: {{Generating Malicious Comments}} to {{Attack Neural Fake News Detection Models}}},
  shorttitle = {{{MALCOM}}},
  author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.01048 [cs, stat]},
  eprint = {2009.01048},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In recent years, the proliferation of so-called "fake news" has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art models to auto-detect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask "what if adversaries attempt to attack such detection models?" and investigate related issues by (i) proposing a novel threat model against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead fake news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94\% and 93.5\% of the time on average MALCOM can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, MALCOM can also fool black box fake news detectors to always output real news labels 90\% of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/KUWS2I5U/Le et al. - 2020 - MALCOM Generating Malicious Comments to Attack Ne.pdf;/Users/teetusaini/Zotero/storage/I72NSGR9/2009.html}
}

@inproceedings{le_sweet_2021,
  title = {A {{Sweet Rabbit Hole}} by {{DARCY}}: {{Using Honeypots}} to {{Detect Universal Trigger}}'s {{Adversarial Attacks}}},
  shorttitle = {A {{Sweet Rabbit Hole}} by {{DARCY}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Le, Thai and Park, Noseong and Lee, Dongwon},
  year = {2021},
  month = aug,
  pages = {3831--3844},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.296},
  abstract = {The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the ``honeypot'' concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to ``bait and catch'' potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger's adversarial attacks with up to 99\% TPR and less than 2\% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1\% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers' varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.},
  file = {/Users/teetusaini/Zotero/storage/J5FKIPZF/Le et al. - 2021 - A Sweet Rabbit Hole by DARCY Using Honeypots to D.pdf}
}

@article{lee_patentbert_2019,
  title = {{{PatentBERT}}: {{Patent Classification}} with {{Fine-Tuning}} a Pre-Trained {{BERT Model}}},
  shorttitle = {{{PatentBERT}}},
  author = {Lee, Jieh-Sheng and Hsiang, Jieh},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.02124 [cs, stat]},
  eprint = {1906.02124},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work we focus on fine-tuning a pre-trained BERT model and applying it to patent classification. When applied to large datasets of over two millions patents, our approach outperforms the state of the art by an approach using CNN with word embeddings. In addition, we focus on patent claims without other parts in patent documents. Our contributions include: (1) a new state-of-the-art method based on pre-trained BERT model and fine-tuning for patent classification, (2) a large dataset USPTO-3M at the CPC subclass level with SQL statements that can be used by future researchers, (3) showing that patent claims alone are sufficient for classification task, in contrast to conventional wisdom.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/ADDVF3PF/Lee and Hsiang - 2019 - PatentBERT Patent Classification with Fine-Tuning.pdf;/Users/teetusaini/Zotero/storage/H6D8D737/1906.html}
}

@article{lee-thorp_fnet_2021,
  title = {{{FNet}}: {{Mixing Tokens}} with {{Fourier Transforms}}},
  shorttitle = {{{FNet}}},
  author = {{Lee-Thorp}, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  year = {2021},
  month = may,
  abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate "efficient" Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/UZB33DL3/Lee-Thorp et al. - 2021 - FNet Mixing Tokens with Fourier Transforms.pdf;/Users/teetusaini/Zotero/storage/EBHWQTHL/2105.html}
}

@inproceedings{li_bert-attack_2020,
  title = {{{BERT-ATTACK}}: {{Adversarial Attack Against BERT Using BERT}}},
  shorttitle = {{{BERT-ATTACK}}},
  booktitle = {{{EMNLP}}},
  author = {Li, L. and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
  year = {2020},
  doi = {10.18653/v1/2020.emnlp-main.500},
  abstract = {Adversarial attacks for discrete data (such as text) has been proved significantly more challenging than continuous data (such as image), since it is difficult to generate adversarial samples with gradient-based methods. Currently, the successful attack methods for text usually adopt heuristic replacement strategies on character or word level, which remains challenging to find the optimal solution in the massive space of possible combination of replacements, while preserving semantic consistency and language fluency. In this paper, we propose \textbackslash textbf\{BERT-Attack\}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models for downstream tasks. Our method successfully misleads the target models to predict incorrectly, outperforming state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations.},
  file = {/Users/teetusaini/Zotero/storage/H7LCX3UC/Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf}
}

@article{li_contextualized_2021,
  title = {Contextualized {{Perturbation}} for {{Textual Adversarial Attack}}},
  author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
  year = {2021},
  month = mar,
  journal = {arXiv:2009.07502 [cs]},
  eprint = {2009.07502},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/QPU85HX3/Li et al. - 2021 - Contextualized Perturbation for Textual Adversaria.pdf;/Users/teetusaini/Zotero/storage/WC22HCKZ/2009.html}
}

@article{li_contextualized_2021-1,
  title = {Contextualized {{Perturbation}} for {{Textual Adversarial Attack}}},
  author = {Li, Dianqi and Zhang, Yizhe and Peng, Hao and Chen, Liqun and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
  year = {2021},
  month = mar,
  journal = {arXiv:2009.07502 [cs]},
  eprint = {2009.07502},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/D6FH8JSY/Li et al. - 2021 - Contextualized Perturbation for Textual Adversaria.pdf;/Users/teetusaini/Zotero/storage/DXS2SFMY/2009.html}
}

@article{li_data_2021,
  title = {Data {{Augmentation Approaches}} in {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Data {{Augmentation Approaches}} in {{Natural Language Processing}}},
  author = {Li, Bohan and Hou, Yutai and Che, Wanxiang},
  year = {2021},
  month = nov,
  journal = {arXiv:2110.01852 [cs]},
  eprint = {2110.01852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/FWX8GWUA/Li et al. - 2021 - Data Augmentation Approaches in Natural Language P.pdf;/Users/teetusaini/Zotero/storage/7MFA7PD3/2110.html}
}

@article{li_textbugger_2019,
  title = {{{TextBugger}}: {{Generating Adversarial Text Against Real-world Applications}}},
  shorttitle = {{{TextBugger}}},
  author = {Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
  year = {2019},
  journal = {Proceedings 2019 Network and Distributed System Security Symposium},
  eprint = {1812.05271},
  eprinttype = {arxiv},
  doi = {10.14722/ndss.2019.23138},
  abstract = {Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\textbackslash\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\textbackslash\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\textbackslash\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/MYDNKSM6/Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf;/Users/teetusaini/Zotero/storage/4C4B5NMU/1812.html}
}

@inproceedings{li_textshield_2020,
  title = {\{\vphantom\}{{TextShield}}\vphantom\{\}: {{Robust Text Classification Based}} on {{Multimodal Embedding}} and {{Neural Machine Translation}}},
  shorttitle = {\{\vphantom\}{{TextShield}}\vphantom\{\}},
  booktitle = {29th {{USENIX Security Symposium}} ({{USENIX Security}} 20)},
  author = {Li, Jinfeng and Du, Tianyu and Ji, Shouling and Zhang, Rong and Lu, Quan and Yang, Min and Wang, Ting},
  year = {2020},
  pages = {1381--1398},
  isbn = {978-1-939133-17-5},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/CVC9642H/Li et al. - 2020 - TextShield Robust Text Classification Based on .pdf;/Users/teetusaini/Zotero/storage/VRVRKX89/li-jinfeng.html}
}

@article{li_understanding_2017,
  title = {Understanding {{Neural Networks}} through {{Representation Erasure}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2017},
  month = jan,
  journal = {arXiv:1612.08220 [cs]},
  eprint = {1612.08220},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/8RFSYGE7/Li et al. - 2017 - Understanding Neural Networks through Representati.pdf;/Users/teetusaini/Zotero/storage/AD2YRS2Z/1612.html}
}

@inproceedings{li_universal_2019,
  title = {Universal {{Rules}} for {{Fooling Deep Neural Networks}} Based {{Text Classification}}},
  booktitle = {2019 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Li, Di and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
  year = {2019},
  month = jun,
  pages = {2221--2228},
  doi = {10.1109/CEC.2019.8790213},
  abstract = {Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses.},
  keywords = {Adversarial machine learning,Deep learning,Natural language processing,Natural Language processing,Observers,Optimization,Perturbation methods,Text misclassification,Training},
  file = {/Users/teetusaini/Zotero/storage/Z59DVF25/Li et al. - 2019 - Universal Rules for Fooling Deep Neural Networks b.pdf;/Users/teetusaini/Zotero/storage/Z2EQYYMU/8790213.html}
}

@article{liddy_natural_nodate,
  title = {Natural {{Language Processing}}},
  author = {Liddy, Elizabeth D},
  pages = {15},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/HBNECJFI/Liddy - Natural Language Processing.pdf}
}

@article{liu_adversarial_2020,
  title = {Adversarial {{Training}} for {{Large Neural Language Models}}},
  author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08994 [cs]},
  eprint = {2004.08994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/SZEBKH6F/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf;/Users/teetusaini/Zotero/storage/BG5HQ56P/2004.html}
}

@article{liu_adversarial_2020-1,
  title = {Adversarial {{Training}} for {{Large Neural Language Models}}},
  author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08994 [cs]},
  eprint = {2004.08994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/Z8SXZTEF/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf;/Users/teetusaini/Zotero/storage/862WBUPA/2004.html}
}

@article{liu_adversarial_2020-3,
  title = {Adversarial {{Training}} for {{Large Neural Language Models}}},
  author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08994 [cs]},
  eprint = {2004.08994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/9WPDCL7W/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf;/Users/teetusaini/Zotero/storage/5D6AA76B/2004.html}
}

@article{liu_roberta_2019_1,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/FPK626J4/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/teetusaini/Zotero/storage/4NIYRVAE/1907.html}
}

@article{liu_roberta_2019-1,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/ZYKXTIKI/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/teetusaini/Zotero/storage/GKMJMCIG/1907.html}
}

@article{luo_local_2021,
  title = {Local {{Interpretations}} for {{Explainable Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Local {{Interpretations}} for {{Explainable Natural Language Processing}}},
  author = {Luo, Siwen and Ivison, Hamish and Han, Caren and Poon, Josiah},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.11072 [cs]},
  eprint = {2103.11072},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for natural language processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term \textbackslash textit\{interpretability\} and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are divided into three categories: 1) explaining the model's predictions through related input features; 2) explaining through natural language explanation; 3) probing the hidden states of models and word representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/9PRNLNNM/Luo et al. - 2021 - Local Interpretations for Explainable Natural Lang.pdf;/Users/teetusaini/Zotero/storage/QZ4DYA4V/2103.html}
}

@article{luong_effective_2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  month = sep,
  journal = {arXiv:1508.04025 [cs]},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/GZ2B2SVV/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/Users/teetusaini/Zotero/storage/S6VIW2IL/1508.html}
}

@misc{ma_nlpaug_2022,
  title = {Nlpaug},
  author = {Ma, Edward},
  year = {2022},
  month = jan,
  abstract = {Data augmentation for NLP},
  copyright = {MIT},
  keywords = {adversarial-attacks,adversarial-example,ai,artificial-intelligence,augmentation,data-science,machine-learning,ml,natural-language-processing,nlp}
}

@inproceedings{magassouba_multimodal_2020,
  title = {Multimodal {{Attention Branch Network}} for {{Perspective-Free Sentence Generation}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Magassouba, Aly and Sugiura, Komei and Kawai, Hisashi},
  year = {2020},
  month = may,
  pages = {76--85},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In this paper, we address the automatic sentence generation of fetching instructions for domestic service robots. Typical fetching commands such as ``bring me the yellow toy from the upper part of t...},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/QIX5A9NR/Magassouba et al. - 2020 - Multimodal Attention Branch Network for Perspectiv.pdf;/Users/teetusaini/Zotero/storage/UM3MX3K4/magassouba20a.html}
}

@article{maheshwary_context_2020,
  title = {A {{Context Aware Approach}} for {{Generating Natural Language Attacks}}},
  author = {Maheshwary, Rishabh and Maheshwary, Saket and Pudi, Vikram},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.13339 [cs]},
  eprint = {2012.13339},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study an important task of attacking natural language processing models in a black box setting. We propose an attack strategy that crafts semantically similar adversarial examples on text classification and entailment tasks. Our proposed attack finds candidate words by considering the information of both the original word and its surrounding context. It jointly leverages masked language modelling and next sentence prediction for context understanding. In comparison to attacks proposed in prior literature, we are able to generate high quality adversarial examples that do significantly better both in terms of success rate and word perturbation percentage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/BF7VYMQQ/Maheshwary et al. - 2020 - A Context Aware Approach for Generating Natural La.pdf;/Users/teetusaini/Zotero/storage/NCMZS4MN/2012.html}
}

@article{maheshwary_generating_2021,
  title = {Generating {{Natural Language Attacks}} in a {{Hard Label Black Box Setting}}},
  author = {Maheshwary, Rishabh and Maheshwary, Saket and Pudi, Vikram},
  year = {2021},
  month = apr,
  journal = {arXiv:2012.14956 [cs]},
  eprint = {2012.14956},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study an important and challenging task of attacking natural language processing models in a hard label black box setting. We propose a decision-based attack strategy that crafts high quality adversarial examples on text classification and entailment tasks. Our proposed attack strategy leverages population-based optimization algorithm to craft plausible and semantically similar adversarial examples by observing only the top label predicted by the target model. At each iteration, the optimization procedure allow word replacements that maximizes the overall semantic similarity between the original and the adversarial text. Further, our approach does not rely on using substitute models or any kind of training data. We demonstrate the efficacy of our proposed approach through extensive experimentation and ablation studies on five state-of-the-art target models across seven benchmark datasets. In comparison to attacks proposed in prior literature, we are able to achieve a higher success rate with lower word perturbation percentage that too in a highly restricted setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/MWGKG6MI/Maheshwary et al. - 2021 - Generating Natural Language Attacks in a Hard Labe.pdf;/Users/teetusaini/Zotero/storage/B3FKE7HM/2012.html}
}

@inproceedings{meng_geometry-inspired_2020,
  title = {A {{Geometry-Inspired Attack}} for {{Generating Natural Language Adversarial Examples}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Meng, Zhao and Wattenhofer, Roger},
  year = {2020},
  month = dec,
  pages = {6679--6689},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.585},
  abstract = {Generating adversarial examples for natural language is hard, as natural language consists of discrete symbols, and examples are often of variable lengths. In this paper, we propose a geometry-inspired attack for generating natural language adversarial examples. Our attack generates adversarial examples by iteratively approximating the decision boundary of Deep Neural Networks (DNNs). Experiments on two datasets with two different models show that our attack fools natural language models with high success rates, while only replacing a few words. Human evaluation shows that adversarial examples generated by our attack are hard for humans to recognize. Further experiments show that adversarial training can improve model robustness against our attack.},
  file = {/Users/teetusaini/Zotero/storage/2YVNA52V/Meng and Wattenhofer - 2020 - A Geometry-Inspired Attack for Generating Natural .pdf}
}

@inproceedings{merono-penuela_can_2020,
  title = {Can a {{Transformer Assist}} in {{Scientific Writing}}? {{Generating Semantic Web Paper Snippets}} with {{GPT-2}}},
  shorttitle = {Can a {{Transformer Assist}} in {{Scientific Writing}}?},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2020 {{Satellite Events}}},
  author = {{Mero{\~n}o-Pe{\~n}uela}, Albert and Spagnuelo, Dayana},
  editor = {Harth, Andreas and Presutti, Valentina and Troncy, Rapha{\"e}l and Acosta, Maribel and Polleres, Axel and Fern{\'a}ndez, Javier D. and Xavier Parreira, Josiane and Hartig, Olaf and Hose, Katja and Cochez, Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {158--163},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-62327-2_27},
  abstract = {The Semantic Web community has produced a large body of literature that is becoming increasingly difficult to manage, browse, and use. Recent work on attention-based, sequence-to-sequence Transformer neural architecture has produced language models that generate surprisingly convincing synthetic conditional text samples. In this demonstration, we re-train the GPT-2 architecture using the complete corpus of proceedings of the International Semantic Web Conference since 2002 until 2019. We use user-provided sentences to conditionally sample paper snippets, therefore illustrating cases where this model can help at addressing challenges in scientific paper writing, such as navigating extensive literature, explaining the Semantic Web core concepts, providing definitions, and even inspiring new research ideas.},
  isbn = {978-3-030-62327-2},
  langid = {english},
  keywords = {Natural language generation,Scholarly communication,Semantic Web papers},
  file = {/Users/teetusaini/Zotero/storage/E8RM23G3/Mero√±o-Pe√±uela and Spagnuelo - 2020 - Can a Transformer Assist in Scientific Writing Ge.pdf}
}

@article{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  journal = {arXiv:1301.3781 [cs]},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/M9HIIZIU/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/teetusaini/Zotero/storage/4IXDPT6P/1301.html}
}

@article{miyato_adversarial_2017,
  title = {Adversarial {{Training Methods}} for {{Semi-Supervised Text Classification}}},
  author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
  year = {2017},
  month = may,
  journal = {arXiv:1605.07725 [cs, stat]},
  eprint = {1605.07725},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/JS2XZ39B/Miyato et al. - 2017 - Adversarial Training Methods for Semi-Supervised T.pdf;/Users/teetusaini/Zotero/storage/BI9FJVAE/1605.html}
}

@article{miyato_virtual_2018,
  title = {Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi-Supervised Learning}}},
  shorttitle = {Virtual {{Adversarial Training}}},
  author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  year = {2018},
  month = jun,
  journal = {arXiv:1704.03976 [cs, stat]},
  eprint = {1704.03976},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/W9ETVEI8/Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf;/Users/teetusaini/Zotero/storage/7PTPI7YX/1704.html}
}

@article{moradi_evaluating_2021,
  title = {Evaluating the {{Robustness}} of {{Neural Language Models}} to {{Input Perturbations}}},
  author = {Moradi, Milad and Samwald, Matthias},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.12237 [cs]},
  eprint = {2108.12237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/CNZK233H/Moradi and Samwald - 2021 - Evaluating the Robustness of Neural Language Model.pdf;/Users/teetusaini/Zotero/storage/E9RV5X2S/2108.html}
}

@article{morris_reevaluating_2020,
  title = {Reevaluating {{Adversarial Examples}} in {{Natural Language}}},
  author = {Morris, John X. and Lifland, Eli and Lanchantin, Jack and Ji, Yangfeng and Qi, Yanjun},
  year = {2020},
  month = apr,
  abstract = {State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38\% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/79PHR3GG/Morris et al. - 2020 - Reevaluating Adversarial Examples in Natural Langu.pdf;/Users/teetusaini/Zotero/storage/BGWPSGAV/2004.html}
}

@article{morris_second-order_2020,
  title = {Second-{{Order NLP Adversarial Examples}}},
  author = {Morris, John X.},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.01770 [cs]},
  eprint = {2010.01770},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at https://github.com/jxmorris12/second-order-adversarial-examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/M6ZMBBWB/Morris - 2020 - Second-Order NLP Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/B3A85FRF/2010.html}
}

@article{morris_textattack_2020,
  title = {{{TextAttack}}: {{A Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}},
  shorttitle = {{{TextAttack}}},
  author = {Morris, John X. and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  year = {2020},
  month = oct,
  journal = {arXiv:2005.05909 [cs]},
  eprint = {2005.05909},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/2LDHB8U7/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf;/Users/teetusaini/Zotero/storage/IJBNNZEH/2005.html}
}

@inproceedings{mozes_frequency-guided_2021,
  title = {Frequency-{{Guided Word Substitutions}} for {{Detecting Textual Adversarial Examples}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Mozes, Maximilian and Stenetorp, Pontus and Kleinberg, Bennett and Griffin, Lewis},
  year = {2021},
  month = apr,
  pages = {171--186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.13},
  abstract = {Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4\% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0\% F1.},
  file = {/Users/teetusaini/Zotero/storage/4LRMQN94/Mozes et al. - 2021 - Frequency-Guided Word Substitutions for Detecting .pdf}
}

@article{mrksic_counter-fitting_2016,
  title = {Counter-Fitting {{Word Vectors}} to {{Linguistic Constraints}}},
  author = {Mrk{\v s}i{\'c}, Nikola and S{\'e}aghdha, Diarmuid {\'O} and Thomson, Blaise and Ga{\v s}i{\'c}, Milica and {Rojas-Barahona}, Lina and Su, Pei-Hao and Vandyke, David and Wen, Tsung-Hsien and Young, Steve},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.00892 [cs]},
  eprint = {1603.00892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/KW85WNTD/Mrk≈°iƒá et al. - 2016 - Counter-fitting Word Vectors to Linguistic Constra.pdf;/Users/teetusaini/Zotero/storage/D2DIX7BT/1603.html}
}

@article{nadaraya_estimating_1964,
  title = {On {{Estimating Regression}}},
  author = {Nadaraya, E. A.},
  year = {1964},
  month = jan,
  journal = {Theory of Probability \& Its Applications},
  volume = {9},
  number = {1},
  pages = {141--142},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/1109020},
  abstract = {A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly.}
}

@article{narang_deepnetdevanagari_2021,
  title = {{{DeepNetDevanagari}}: A Deep Learning Model for {{Devanagari}} Ancient Character Recognition},
  shorttitle = {{{DeepNetDevanagari}}},
  author = {Narang, Sonika Rani and Kumar, Munish and Jindal, M. K.},
  year = {2021},
  month = mar,
  journal = {Multimedia Tools and Applications},
  issn = {1573-7721},
  doi = {10.1007/s11042-021-10775-6},
  abstract = {Devanagari script is the most widely used script in India and other Asian countries. There is a rich collection of ancient Devanagari manuscripts, which is a wealth of knowledge. To make these manuscripts available to people, efforts are being done to digitize these documents. Optical Character Recognition (OCR) plays an important role in recognizing these documents. Convolutional Neural Network (CNN) is a powerful model that is giving very promising results in the field of character recognition, pattern recognition etc. CNN has never been used for the recognition of the Devanagari ancient manuscripts. Our aim in the proposed work is to use the power of CNN for extracting the wealth of knowledge from Devanagari handwritten ancient manuscripts. In addition, we aim is to experiment with various design options like number of layes, stride size, number of filters, kenel size and different functions in various layers and to select the best of these. In this paper, the authors have proposed to use deep learning model as a feature extractor as well as a classifier for the recognition of 33 classes of basic characters of Devanagari ancient manuscripts. A dataset containing 5484 characters has been used for the experimental work. Various experiments show that the accuracy achieved using CNN as a feature extractor is better than other state-of-the-art techniques. The recognition accuracy of 93.73\% has been achieved by using the model proposed in this paper for Devanagari ancient character recognition.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/BEXTT63Z/Narang et al. - 2021 - DeepNetDevanagari a deep learning model for Devan.pdf}
}

@article{naseem_comprehensive_2020,
  title = {A {{Comprehensive Survey}} on {{Word Representation Models}}: {{From Classical}} to {{State-Of-The-Art Word Representation Language Models}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Word Representation Models}}},
  author = {Naseem, Usman and Razzak, Imran and Khan, Shah Khalid and Prasad, Mukesh},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.15036 [cs]},
  eprint = {2010.15036},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/HGTIW5KJ/Naseem et al. - 2020 - A Comprehensive Survey on Word Representation Mode.pdf;/Users/teetusaini/Zotero/storage/AKT3P9RD/2010.html}
}

@article{ng_ssmba_2020,
  title = {{{SSMBA}}: {{Self-Supervised Manifold Based Data Augmentation}} for {{Improving Out-of-Domain Robustness}}},
  shorttitle = {{{SSMBA}}},
  author = {Ng, Nathan and Cho, Kyunghyun and Ghassemi, Marzyeh},
  year = {2020},
  month = oct,
  journal = {arXiv:2009.10195 [cs, stat]},
  eprint = {2009.10195},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8\% accuracy on OOD Amazon reviews, 1.8\% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/Y9MREK67/Ng et al. - 2020 - SSMBA Self-Supervised Manifold Based Data Augment.pdf;/Users/teetusaini/Zotero/storage/2BY5VDVW/2009.html}
}

@article{nicolae_adversarial_2019,
  title = {Adversarial {{Robustness Toolbox}} v1.0.0},
  author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
  year = {2019},
  month = nov,
  journal = {arXiv:1807.01069 [cs, stat]},
  eprint = {1807.01069},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/2RZSIXT4/Nicolae et al. - 2019 - Adversarial Robustness Toolbox v1.0.0.pdf;/Users/teetusaini/Zotero/storage/I9FMDFWC/1807.html}
}

@misc{noauthor_connected_nodate,
  title = {Connected {{Papers}} | {{Find}} and Explore Academic Papers},
  abstract = {A unique, visual tool to help researchers and applied scientists find and explore papers relevant to their field of work.},
  howpublished = {https://www.connectedpapers.com/main/32c8884a95101ae9b854399b69284b28dd062fce/Machine-Learning-Threatens-5G-Security/prior},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/3XHAAWEH/prior.html}
}

@misc{noauthor_hugging_nodate,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/},
  file = {/Users/teetusaini/Zotero/storage/XIZ82XT3/huggingface.co.html}
}

@misc{noauthor_maifshapash_2021,
  title = {{{MAIF}}/Shapash},
  year = {2021},
  month = apr,
  abstract = {Shapash makes Machine Learning models transparent and understandable by everyone},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  howpublished = {MAIF},
  keywords = {ethical-artificial-intelligence,explainability,explainable-ml,lime,machine-learning,python,shap,transparency}
}

@misc{noauthor_pdf_nodate,
  title = {[{{PDF}}] {{HotFlip}}: {{White-Box Adversarial Examples}} for {{Text Classification}} | {{Semantic Scholar}}},
  howpublished = {https://www.semanticscholar.org/paper/HotFlip\%3A-White-Box-Adversarial-Examples-for-Text-Ebrahimi-Rao/514e7fb769950dbe96eb519c88ca17e04dc829f6},
  file = {/Users/teetusaini/Zotero/storage/I5XETQ6L/514e7fb769950dbe96eb519c88ca17e04dc829f6.html}
}

@misc{noauthor_pdf_nodate-3,
  title = {[{{PDF}}] {{TextBugger}}: {{Generating Adversarial Text Against Real-world Applications}} | {{Semantic Scholar}}},
  howpublished = {https://www.semanticscholar.org/paper/TextBugger\%3A-Generating-Adversarial-Text-Against-Li-Ji/1e7ea2465753c5186e5fcc9e1a64cdc522cf4de4},
  file = {/Users/teetusaini/Zotero/storage/QQ524299/1e7ea2465753c5186e5fcc9e1a64cdc522cf4de4.html}
}

@misc{noauthor_thunlpopenattack_2021,
  title = {Thunlp/{{OpenAttack}}},
  year = {2021},
  month = apr,
  abstract = {An Open-Source Package for Textual Adversarial Attack.},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {THUNLP},
  keywords = {adversarial-attacks,adversarial-example,nlp}
}

@misc{noauthor_thunlptaadpapers_2021,
  title = {Thunlp/{{TAADpapers}}},
  year = {2021},
  month = apr,
  abstract = {Must-read Papers on Textual Adversarial Attack and Defense},
  howpublished = {THUNLP},
  keywords = {adversarial-attacks,adversarial-defense,adversarial-learning,nlp,paper-list}
}

@article{noever_virus-mnist_2021,
  title = {Virus-{{MNIST}}: {{A Benchmark Malware Dataset}}},
  shorttitle = {Virus-{{MNIST}}},
  author = {Noever, David and Noever, Samantha E. Miller},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.00602 [cs]},
  eprint = {2103.00602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The short note presents an image classification dataset consisting of 10 executable code varieties and approximately 50,000 virus examples. The malicious classes include 9 families of computer viruses and one benign set. The image formatting for the first 1024 bytes of the Portable Executable (PE) mirrors the familiar MNIST handwriting dataset, such that most of the previously explored algorithmic methods can transfer with minor modifications. The designation of 9 virus families for malware derives from unsupervised learning of class labels; we discover the families with KMeans clustering that excludes the non-malicious examples. As a benchmark using deep learning methods (MobileNetV2), we find an overall 80\% accuracy for virus identification by families when beneware is included. We also find that once a positive malware detection occurs (by signature or heuristics), the projection of the first 1024 bytes into a thumbnail image can classify with 87\% accuracy the type of virus. The work generalizes what other malware investigators have demonstrated as promising convolutional neural networks originally developed to solve image problems but applied to a new abstract domain in pixel bytes from executable files. The dataset is available on Kaggle and Github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/6EQMU4W9/Noever and Noever - 2021 - Virus-MNIST A Benchmark Malware Dataset.pdf;/Users/teetusaini/Zotero/storage/8GSKJRE5/2103.html}
}

@article{ovadia_can_2019,
  title = {Can {{You Trust Your Model}}'s {{Uncertainty}}? {{Evaluating Predictive Uncertainty Under Dataset Shift}}},
  shorttitle = {Can {{You Trust Your Model}}'s {{Uncertainty}}?},
  author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
  year = {2019},
  month = dec,
  journal = {arXiv:1906.02530 [cs, stat]},
  eprint = {1906.02530},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive \{\textbackslash em uncertainty\}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/Y239PSZ3/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf;/Users/teetusaini/Zotero/storage/5F3LB7TG/1906.html}
}

@article{papernot_crafting_2016,
  title = {Crafting {{Adversarial Input Sequences}} for {{Recurrent Neural Networks}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Swami, Ananthram and Harang, Richard},
  year = {2016},
  month = apr,
  journal = {arXiv:1604.08275 [cs]},
  eprint = {1604.08275},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/teetusaini/Zotero/storage/KRYTNPGN/Papernot et al. - 2016 - Crafting Adversarial Input Sequences for Recurrent.pdf;/Users/teetusaini/Zotero/storage/M2EQRRC4/1604.html}
}

@article{patwa_fighting_2021,
  title = {Fighting an {{Infodemic}}: {{COVID-19 Fake News Dataset}}},
  shorttitle = {Fighting an {{Infodemic}}},
  author = {Patwa, Parth and Sharma, Shivam and Pykl, Srinivas and Guptha, Vineeth and Kumari, Gitanjali and Akhtar, Md Shad and Ekbal, Asif and Das, Amitava and Chakraborty, Tanmoy},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.03327 [cs]},
  eprint = {2011.03327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\textbackslash\% F1-score with SVM. The data and code is available at: https://github.com/parthpatwa/covid19-fake-news-dectection},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  file = {/Users/teetusaini/Zotero/storage/D2R6R9G4/Patwa et al. - 2021 - Fighting an Infodemic COVID-19 Fake News Dataset.pdf;/Users/teetusaini/Zotero/storage/D45KXKU8/2011.html}
}

@misc{paul_very_nodate,
  title = {A Very Tiny Alteration Can Help Deepfakes Escape Detection},
  author = {Paul, Ben and California, University of Southern},
  abstract = {Last month, Sophie Wilm\`es, the prime minister of Belgium, appeared in an online video to tell her audience that the COVID-19 pandemic was linked to the "exploitation and destruction by humans of our natural environment." Whether or not these two existential crises are connected, the fact is that Wilm\`es said no such thing. Produced by an organization of climate change activists, the video was actually a deepfake, or a form of fake media created using deep learning. Deepfakes are yet another way to spread misinformation\textemdash as if there wasn't enough fake news about the pandemic already.},
  howpublished = {https://techxplore.com/news/2020-10-tiny-deepfakes.html},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/WCBUKGF7/2020-10-tiny-deepfakes.html}
}

@inproceedings{pennington_glove_2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  file = {/Users/teetusaini/Zotero/storage/4S7E58E5/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{peters_deep_2018-3,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = jun,
  pages = {2227--2237},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1202},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  file = {/Users/teetusaini/Zotero/storage/76V3T6CZ/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of Stochastic Approximation by Averaging},
  author = {Polyak, B. T. and Juditsky, A. B.},
  year = {1992},
  month = jul,
  journal = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  issn = {0363-0129},
  doi = {10.1137/0330046},
  keywords = {optimal algorithms,recursive estimation,stochastic approximation,stochastic optimization}
}

@article{radford_improving_nodate,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/E9B7IXYG/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radford_improving_nodate-1,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/3YDCNVSS/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{raghunathan_certified_2020,
  title = {Certified {{Defenses}} against {{Adversarial Examples}}},
  author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
  year = {2020},
  month = oct,
  journal = {arXiv:1801.09344 [cs]},
  eprint = {1801.09344},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \textbackslash epsilon = 0.1 can cause more than 35\% test error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/AGMPMEIJ/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf;/Users/teetusaini/Zotero/storage/FQIHBKUX/1801.html}
}

@article{rauber_foolbox_2018,
  title = {Foolbox: {{A Python}} Toolbox to Benchmark the Robustness of Machine Learning Models},
  shorttitle = {Foolbox},
  author = {Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
  year = {2018},
  month = mar,
  journal = {arXiv:1707.04131 [cs, stat]},
  eprint = {1707.04131},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox . The most up-to-date documentation can be found at http://foolbox.readthedocs.io .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/6FHMC639/Rauber et al. - 2018 - Foolbox A Python toolbox to benchmark the robustn.pdf;/Users/teetusaini/Zotero/storage/DH6R97E7/1707.html}
}

@book{ravichandiran_getting_2021,
  title = {Getting {{Started}} with {{Google BERT}}},
  author = {Ravichandiran, Sudharsan and Safari, an O'Reilly Media Company},
  year = {2021},
  abstract = {Kickstart your NLP journey by exploring BERT and its variants such as ALBERT, RoBERTa, DistilBERT, VideoBERT, and more with Hugging Face's transformers library Key Features Explore the encoder and decoder of the transformer model Become well-versed with BERT along with ALBERT, RoBERTa, and DistilBERT Discover how to pre-train and fine-tune BERT models for several NLP tasks Book Description BERT (bidirectional encoder representations from transformer) has revolutionized the world of natural language processing (NLP) with promising results. This book is an introductory guide that will help you get to grips with Google's BERT architecture. With a detailed explanation of the transformer architecture, this book will help you understand how the transformer's encoder and decoder work. You'll explore the BERT architecture by learning how the BERT model is pre-trained and how to use pre-trained BERT for downstream tasks by fine-tuning it for NLP tasks such as sentiment analysis and text summarization with the Hugging Face transformers library. As you advance, you'll learn about different variants of BERT such as ALBERT, RoBERTa, and ELECTRA, and look at SpanBERT, which is used for NLP tasks like question answering. You'll also cover simpler and faster BERT variants based on knowledge distillation such as DistilBERT and TinyBERT. The book takes you through MBERT, XLM, and XLM-R in detail and then introduces you to sentence-BERT, which is used for obtaining sentence representation. Finally, you'll discover domain-specific BERT models such as BioBERT and ClinicalBERT, and discover an interesting variant called VideoBERT. By the end of this BERT book, you'll be well-versed with using BERT and its variants for performing practical NLP tasks. What you will learn Understand the transformer model from the ground up Find out how BERT works and pre-train it using masked language model (MLM) and next sentence prediction (NSP) tasks Get hands-on with BERT by learning to generate contextual word and sentence embeddings Fine-tune BERT for downstream tasks Get to grips with ALBERT, RoBERTa, ELECTRA, and SpanBERT models Get the hang of the BERT models based on knowledge distillation Understand cross-lingual models such as XLM and XLM-R Explore Sentence-BERT, VideoBERT, and BART Who this book is for This book is for NLP professionals and data scientists looking to simplify NLP tasks to enable efficient language understanding using BERT. A basic understanding of NLP con...},
  isbn = {978-1-83882-159-3},
  langid = {english},
  annotation = {OCLC: 1241738046},
  file = {/Users/teetusaini/Zotero/storage/NR3SGRR9/Ravichandiran and Safari - 2021 - Getting Started with Google BERT.pdf}
}

@article{rawlinson_significance_2007,
  title = {The {{Significance}} of {{Letter Position}} in {{Word Recognition}}},
  author = {Rawlinson, Graham},
  year = {2007},
  month = jan,
  journal = {IEEE Aerospace and Electronic Systems Magazine},
  volume = {22},
  number = {1},
  pages = {26--27},
  issn = {1557-959X},
  doi = {10.1109/MAES.2007.327521},
  abstract = {Presents the summary of a PhD thesis on letter position in word recognition, which was written in 1976. The thesis was the source of the text used on the cover of the October 2006 issue of the IEEE Aerospace and Electronic Systems Magazine.},
  keywords = {Books,Copper,Electromagnetic scattering,Kirk field collapse effect,Marine vehicles,Metamaterials,Microwave frequencies,Roads,Shape,Testing},
  file = {/Users/teetusaini/Zotero/storage/P2WWZJYI/4126417.html}
}

@inproceedings{ren_generating_2019,
  title = {Generating {{Natural Language Adversarial Examples}} through {{Probability Weighted Word Saliency}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
  year = {2019},
  month = jul,
  pages = {1085--1097},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1103},
  abstract = {We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.},
  file = {/Users/teetusaini/Zotero/storage/DC366H9X/Ren et al. - 2019 - Generating Natural Language Adversarial Examples t.pdf}
}

@article{ren_generating_2020,
  title = {Generating {{Natural Language Adversarial Examples}} on a {{Large Scale}} with {{Generative Models}}},
  author = {Ren, Yankun and Lin, Jianbin and Tang, Siliang and Zhou, Jun and Yang, Shuang and Qi, Yuan and Ren, Xiang},
  year = {2020},
  month = mar,
  abstract = {Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/GE62AIGN/Ren et al. - 2020 - Generating Natural Language Adversarial Examples o.pdf;/Users/teetusaini/Zotero/storage/MKHRYVBE/2003.html}
}

@article{ribeiro_beyond_2020,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP}} Models with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  year = {2020},
  month = may,
  journal = {arXiv:2005.04118 [cs]},
  eprint = {2005.04118},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/NXCRGQH5/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP models .pdf;/Users/teetusaini/Zotero/storage/32ZI897H/2005.html}
}

@inproceedings{ribeiro_semantically_2018,
  title = {Semantically {{Equivalent Adversarial Rules}} for {{Debugging NLP}} Models},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  month = jul,
  pages = {856--865},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1079},
  abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) \textendash{} semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) \textendash{} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
  file = {/Users/teetusaini/Zotero/storage/ZTWB68B3/Ribeiro et al. - 2018 - Semantically Equivalent Adversarial Rules for Debu.pdf}
}

@article{rogers_primer_2021,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2021},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00349},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  file = {/Users/teetusaini/Zotero/storage/DBHDPUQ8/Rogers et al. - 2021 - A Primer in BERTology What We Know About How BERT.pdf;/Users/teetusaini/Zotero/storage/5R3XIMLI/A-Primer-in-BERTology-What-We-Know-About-How-BERT.html}
}

@article{roth_token-modification_2021,
  title = {Token-{{Modification Adversarial Attacks}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Token-{{Modification Adversarial Attacks}} for {{Natural Language Processing}}},
  author = {Roth, Tom and Gao, Yansong and Abuadbba, Alsharif and Nepal, Surya and Liu, Wei},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.00676 [cs]},
  eprint = {2103.00676},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a \textbackslash textit\{token-modification\} attack. Each token-modification attack is defined by a specific combination of fundamental \textbackslash textit\{components\}, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. We hope this survey will guide new researchers to this field and spark further research into the individual attack components.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/LZ6QTFH4/Roth et al. - 2021 - Token-Modification Adversarial Attacks for Natural.pdf;/Users/teetusaini/Zotero/storage/2VXCSR8E/2103.html}
}

@article{sanh_distilbert_2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.01108 [cs]},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/6R55MCXP/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf;/Users/teetusaini/Zotero/storage/43KMCF84/1910.html}
}

@article{saxena_textdecepter_2020,
  title = {{{TextDecepter}}: {{Hard Label Black Box Attack}} on {{Text Classifiers}}},
  shorttitle = {{{TextDecepter}}},
  author = {Saxena, Sachin},
  year = {2020},
  month = dec,
  journal = {arXiv:2008.06860 [cs]},
  eprint = {2008.06860},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine learning has been proven to be susceptible to carefully crafted samples, known as adversarial examples. The generation of these adversarial examples helps to make the models more robust and gives us an insight into the underlying decision-making of these models. Over the years, researchers have successfully attacked image classifiers in both, white and black-box settings. However, these methods are not directly applicable to texts as text data is discrete. In recent years, research on crafting adversarial examples against textual applications has been on the rise. In this paper, we present a novel approach for hard-label black-box attacks against Natural Language Processing (NLP) classifiers, where no model information is disclosed, and an attacker can only query the model to get a final decision of the classifier, without confidence scores of the classes involved. Such an attack scenario applies to real-world black-box models being used for security-sensitive applications such as sentiment analysis and toxic content detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/EVZ9AF6W/Saxena - 2020 - TextDecepter Hard Label Black Box Attack on Text .pdf;/Users/teetusaini/Zotero/storage/6T34RZIQ/2008.html}
}

@article{sensoy_evidential_2018,
  title = {Evidential {{Deep Learning}} to {{Quantify Classification Uncertainty}}},
  author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
  year = {2018},
  month = oct,
  journal = {arXiv:1806.01768 [cs, stat]},
  eprint = {1806.01768},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/N96Y7WG5/Sensoy et al. - 2018 - Evidential Deep Learning to Quantify Classificatio.pdf;/Users/teetusaini/Zotero/storage/TDDLKP6D/1806.html}
}

@inproceedings{shi_robustness_2020,
  title = {Robustness to {{Modification}} with {{Shared Words}} in {{Paraphrase Identification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Shi, Zhouxing and Huang, Minlie},
  year = {2020},
  month = nov,
  pages = {164--171},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.16},
  abstract = {Revealing the robustness issues of natural language processing models and improving their robustness is important to their performance under difficult situations. In this paper, we study the robustness of paraphrase identification models from a new perspective \textendash{} via modification with shared words, and we show that the models have significant robustness issues when facing such modifications. To modify an example consisting of a sentence pair, we either replace some words shared by both sentences or introduce new shared words. We aim to construct a valid new example such that a target model makes a wrong prediction. To find a modification solution, we use beam search constrained by heuristic rules, and we leverage a BERT masked language model for generating substitution words compatible with the context. Experiments show that the performance of the target models has a dramatic drop on the modified examples, thereby revealing the robustness issue. We also show that adversarial training can mitigate this issue.},
  file = {/Users/teetusaini/Zotero/storage/X4YHYZLW/Shi and Huang - 2020 - Robustness to Modification with Shared Words in Pa.pdf}
}

@article{shi_robustness_2020-1,
  title = {Robustness {{Verification}} for {{Transformers}}},
  author = {Shi, Zhouxing and Zhang, Huan and Chang, Kai-Wei and Huang, Minlie and Hsieh, Cho-Jui},
  year = {2020},
  month = dec,
  journal = {arXiv:2002.06622 [cs, stat]},
  eprint = {2002.06622},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/DPGUN44D/Shi et al. - 2020 - Robustness Verification for Transformers.pdf;/Users/teetusaini/Zotero/storage/B8JLTBC8/2002.html}
}

@inproceedings{si_better_2021,
  title = {Better {{Robustness}} by {{More Coverage}}: {{Adversarial}} and {{Mixup Data Augmentation}} for {{Robust Finetuning}}},
  shorttitle = {Better {{Robustness}} by {{More Coverage}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Si, Chenglei and Zhang, Zhengyan and Qi, Fanchao and Liu, Zhiyuan and Wang, Yasheng and Liu, Qun and Sun, Maosong},
  year = {2021},
  month = aug,
  pages = {1569--1576},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.137},
  file = {/Users/teetusaini/Zotero/storage/99C258HW/Si et al. - 2021 - Better Robustness by More Coverage Adversarial an.pdf}
}

@article{su_css-lm_2021,
  title = {{{CSS-LM}}: {{A Contrastive Framework}} for {{Semi-supervised Fine-tuning}} of {{Pre-trained Language Models}}},
  shorttitle = {{{CSS-LM}}},
  author = {Su, Yusheng and Han, Xu and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Li, Peng and Zhou, Jie and Sun, Maosong},
  year = {2021},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  eprint = {2102.03752},
  eprinttype = {arxiv},
  pages = {2930--2941},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3105013},
  abstract = {Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many low-resource scenarios, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named "CSS-LM") to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings, and outperforms the latest supervised contrastive fine-tuning strategies. Our datasets and source code will be available to provide more details.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/D5JAI8N7/Su et al. - 2021 - CSS-LM A Contrastive Framework for Semi-supervise.pdf;/Users/teetusaini/Zotero/storage/8PCSCAV5/2102.html}
}

@article{sun_adv-bert_2020,
  title = {Adv-{{BERT}}: {{BERT}} Is Not Robust on Misspellings! {{Generating}} Nature Adversarial Samples on {{BERT}}},
  shorttitle = {Adv-{{BERT}}},
  author = {Sun, Lichao and Hashimoto, Kazuma and Yin, Wenpeng and Asai, Akari and Li, Jia and Yu, Philip and Xiong, Caiming},
  year = {2020},
  month = feb,
  journal = {arXiv:2003.04985 [cs]},
  eprint = {2003.04985},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There is an increasing amount of literature that claims the brittleness of deep neural networks in dealing with adversarial examples that are created maliciously. It is unclear, however, how the models will perform in realistic scenarios where \textbackslash textit\{natural rather than malicious\} adversarial instances often exist. This work systematically explores the robustness of BERT, the state-of-the-art Transformer-style model in NLP, in dealing with noisy data, particularly mistakes in typing the keyboard, that occur inadvertently. Intensive experiments on sentiment analysis and question answering benchmarks indicate that: (i) Typos in various words of a sentence do not influence equally. The typos in informative words make severer damages; (ii) Mistype is the most damaging factor, compared with inserting, deleting, etc.; (iii) Humans and machines have different focuses on recognizing adversarial attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/TDW6TXYR/Sun et al. - 2020 - Adv-BERT BERT is not robust on misspellings! Gene.pdf;/Users/teetusaini/Zotero/storage/ETS5A5G9/2003.html}
}

@inproceedings{sun_understanding_2020,
  title = {Understanding {{Attention}} for {{Text Classification}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Sun, Xiaobing and Lu, Wei},
  year = {2020},
  month = jul,
  pages = {3418--3428},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.312},
  abstract = {Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token's significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.},
  file = {/Users/teetusaini/Zotero/storage/KH79QXJ4/Sun and Lu - 2020 - Understanding Attention for Text Classification.pdf}
}

@article{sutskever_sequence_2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  journal = {arXiv:1409.3215 [cs]},
  eprint = {1409.3215},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/MYZ7NFA6/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;/Users/teetusaini/Zotero/storage/P8DMYQT5/1409.html}
}

@article{szegedy_intriguing_2014,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2014},
  month = feb,
  journal = {arXiv:1312.6199 [cs]},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/teetusaini/Zotero/storage/M2CHLT8E/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf;/Users/teetusaini/Zotero/storage/6PBELWQN/1312.html}
}

@inproceedings{tan_its_2020,
  title = {It's {{Morphin}}' {{Time}}! {{Combating Linguistic Discrimination}} with {{Inflectional Perturbations}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tan, Samson and Joty, Shafiq and Kan, Min-Yen and Socher, Richard},
  year = {2020},
  month = jul,
  pages = {2920--2935},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.263},
  abstract = {Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.},
  file = {/Users/teetusaini/Zotero/storage/WRULE6FD/Tan et al. - 2020 - It's Morphin' Time! Combating Linguistic Discrimin.pdf}
}

@inproceedings{tan_its_2020-1,
  title = {It's {{Morphin}}' {{Time}}! {{Combating Linguistic Discrimination}} with {{Inflectional Perturbations}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tan, Samson and Joty, Shafiq and Kan, Min-Yen and Socher, Richard},
  year = {2020},
  month = jul,
  pages = {2920--2935},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.263},
  abstract = {Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.},
  file = {/Users/teetusaini/Zotero/storage/JHP8WKZK/Tan et al. - 2020 - It's Morphin' Time! Combating Linguistic Discrimin.pdf}
}

@article{tarvainen_mean_2018,
  title = {Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results},
  shorttitle = {Mean Teachers Are Better Role Models},
  author = {Tarvainen, Antti and Valpola, Harri},
  year = {2018},
  month = apr,
  journal = {arXiv:1703.01780 [cs, stat]},
  eprint = {1703.01780},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/QHT4R9TD/Tarvainen and Valpola - 2018 - Mean teachers are better role models Weight-avera.pdf;/Users/teetusaini/Zotero/storage/VJB9JEHF/1703.html}
}

@misc{tum_survey_2020,
  title = {A {{Survey}} of the {{State-of-the-Art Language Models}} up to {{Early}} 2020},
  author = {Tum, Phylypo},
  year = {2020},
  month = nov,
  journal = {Medium},
  abstract = {This is a survey of the different approaches in natural language processing (NLP) from an early day to the most recent state-of-the-art\ldots},
  howpublished = {https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/WG6QTWSU/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6.html}
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/AHTHPI6X/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/teetusaini/Zotero/storage/ISN3SHEF/1706.html}
}

@article{vaswani_tensor2tensor_2018,
  title = {{{Tensor2Tensor}} for {{Neural Machine Translation}}},
  author = {Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N. and Gouws, Stephan and Jones, Llion and Kaiser, {\L}ukasz and Kalchbrenner, Nal and Parmar, Niki and Sepassi, Ryan and Shazeer, Noam and Uszkoreit, Jakob},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.07416 [cs, stat]},
  eprint = {1803.07416},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/JB9IUDTR/Vaswani et al. - 2018 - Tensor2Tensor for Neural Machine Translation.pdf;/Users/teetusaini/Zotero/storage/ADUJV7D5/1803.html}
}

@article{wahle_are_2021,
  title = {Are {{Neural Language Models Good Plagiarists}}? {{A Benchmark}} for {{Neural Paraphrase Detection}}},
  shorttitle = {Are {{Neural Language Models Good Plagiarists}}?},
  author = {Wahle, Jan Philip and Ruas, Terry and Meuschke, Norman and Gipp, Bela},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.12450 [cs]},
  eprint = {2103.12450},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The rise of language models such as BERT allows for high-quality text paraphrasing. This is a problem to academic integrity, as it is difficult to differentiate between original and machine-generated content. We propose a benchmark consisting of paraphrased articles using recent language models relying on the Transformer architecture. Our contribution fosters future research of paraphrase detection systems as it offers a large collection of aligned original and paraphrased documents, a study regarding its structure, classification experiments with state-of-the-art systems, and we make our findings publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Digital Libraries},
  file = {/Users/teetusaini/Zotero/storage/C7UWFU37/Wahle et al. - 2021 - Are Neural Language Models Good Plagiarists A Ben.pdf;/Users/teetusaini/Zotero/storage/4RLI7B3D/2103.html}
}

@article{wahle_identifying_2021,
  title = {Identifying {{Machine-Paraphrased Plagiarism}}},
  author = {Wahle, Jan Philip and Ruas, Terry and Folt'ynek, Tom'avs and Meuschke, Norman and Gipp, Bela},
  year = {2021},
  journal = {ArXiv},
  abstract = {Employing paraphrasing tools to conceal plagiarized text is a severe threat to academic integrity. To enable the detection of machine-paraphrased text, we evaluate the effectiveness of five pre-trained word embedding models combined with machine learning classifiers and state-of-the-art neural language models. We analyze preprints of research papers, graduation theses, and Wikipedia articles, which we paraphrased using different configurations of the tools SpinBot and SpinnerChief. The best performing technique, Longformer, achieved an average F1 score of 80.99\% (F1=99.68\% for SpinBot and F1=71.64\% for SpinnerChief cases), while human evaluators achieved F1=78.4\% for SpinBot and F1=65.6\% for SpinnerChief cases. We show that the automated classification alleviates shortcomings of widelyused text-matching systems, such as Turnitin and PlagScan. To facilitate future research, all data, code, and two web applications showcasing our contributions are openly available.},
  file = {/Users/teetusaini/Zotero/storage/69SQWXCC/Wahle et al. - 2021 - Identifying Machine-Paraphrased Plagiarism.pdf}
}

@article{wallace_trick_2019,
  title = {Trick {{Me If You Can}}: {{Human-in-the-Loop Generation}} of {{Adversarial Examples}} for {{Question Answering}}},
  shorttitle = {Trick {{Me If You Can}}},
  author = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and {Boyd-Graber}, Jordan},
  year = {2019},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {387--401},
  doi = {10.1162/tacl_a_00279},
  abstract = {Adversarial evaluation stress-tests a model's understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human\textendash computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.},
  file = {/Users/teetusaini/Zotero/storage/FNHKGK82/Wallace et al. - 2019 - Trick Me If You Can Human-in-the-Loop Generation .pdf}
}

@article{wang_adversarial_2021-1,
  title = {Adversarial {{Training}} with {{Fast Gradient Projection Method}} against {{Synonym Substitution Based Text Attacks}}},
  author = {Wang, Xiaosen and Yang, Yichen and Deng, Yihe and He, Kun},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {16},
  pages = {13997--14005},
  issn = {2374-3468},
  abstract = {Adversarial training is the most empirically successful approach in improving the robustness of deep neural networks for image classification. For text classification, however, existing synonym substitution based adversarial attacks are effective but not very efficient to be incorporated into practical text adversarial training. Gradient-based attacks, which are very efficient for images, are hard to be implemented for synonym substitution based text attacks due to the lexical, grammatical and semantic constraints and the discrete text input space. Thereby, we propose a fast text adversarial attack method called Fast Gradient Projection Method (FGPM) based on synonym substitution, which is about 20 times faster than existing text attack methods and could achieve similar attack performance. We then incorporate FGPM with adversarial training and propose a text defense method called Adversarial Training with FGPM enhanced by Logit pairing (ATFL). Experiments show that ATFL could significantly improve the model robustness and block the transferability of adversarial examples.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Adversarial Attacks \& Robustness},
  file = {/Users/teetusaini/Zotero/storage/C8V9ZTTS/Wang et al. - 2021 - Adversarial Training with Fast Gradient Projection.pdf}
}

@article{wang_cat-gen_2020,
  title = {{{CAT-Gen}}: {{Improving Robustness}} in {{NLP Models}} via {{Controlled Adversarial Text Generation}}},
  shorttitle = {{{CAT-Gen}}},
  author = {Wang, Tianlu and Wang, Xuezhi and Qin, Yao and Packer, Ben and Li, Kang and Chen, Jilin and Beutel, Alex and Chi, Ed},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.02338 [cs]},
  eprint = {2010.02338},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/LR5BEYI6/Wang et al. - 2020 - CAT-Gen Improving Robustness in NLP Models via Co.pdf;/Users/teetusaini/Zotero/storage/LFKDN9QU/2010.html}
}

@inproceedings{wang_cat-gen_2020-1,
  title = {{{CAT-Gen}}: {{Improving Robustness}} in {{NLP Models}} via {{Controlled Adversarial Text Generation}}},
  shorttitle = {{{CAT-Gen}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wang, Tianlu and Wang, Xuezhi and Qin, Yao and Packer, Ben and Li, Kang and Chen, Jilin and Beutel, Alex and Chi, Ed},
  year = {2020},
  month = nov,
  pages = {5141--5146},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.417},
  abstract = {NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.},
  file = {/Users/teetusaini/Zotero/storage/UITXTNI7/Wang et al. - 2020 - CAT-Gen Improving Robustness in NLP Models via Co.pdf}
}

@inproceedings{wang_infobert_2020,
  title = {{{InfoBERT}}: {{Improving Robustness}} of {{Language Models}} from {{An Information Theoretic Perspective}}},
  shorttitle = {{{InfoBERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
  year = {2020},
  month = sep,
  abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing...},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/WPVNHUXV/Wang et al. - 2020 - InfoBERT Improving Robustness of Language Models .pdf;/Users/teetusaini/Zotero/storage/7FAETCG7/forum.html}
}

@inproceedings{wang_infobert_2020-1,
  title = {{{InfoBERT}}: {{Improving Robustness}} of {{Language Models}} from {{An Information Theoretic Perspective}}},
  shorttitle = {{{InfoBERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
  year = {2020},
  month = sep,
  abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing...},
  langid = {english}
}

@article{wang_infobert_2021-2,
  title = {{{InfoBERT}}: {{Improving Robustness}} of {{Language Models}} from {{An Information Theoretic Perspective}}},
  shorttitle = {{{InfoBERT}}},
  author = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
  year = {2021},
  month = mar,
  journal = {arXiv:2010.02329 [cs]},
  eprint = {2010.02329},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/XTX9GJJS/Wang et al. - 2021 - InfoBERT Improving Robustness of Language Models .pdf;/Users/teetusaini/Zotero/storage/PSFFFC79/2010.html}
}

@article{wang_natural_2020,
  title = {Natural {{Language Adversarial Attacks}} and {{Defenses}} in {{Word Level}}},
  author = {Wang, Xiaosen and Jin, Hao and He, Kun},
  year = {2020},
  month = apr,
  journal = {arXiv:1909.06723 [cs]},
  eprint = {1909.06723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, inspired by a mass of researches on adversarial examples for computer vision, there has been a growing interest in designing adversarial attacks for Natural Language Processing (NLP) tasks, followed by very few works of adversarial defenses for NLP. To our knowledge, there exists no defense method against the successful synonym substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to be perceived by humans. We contribute to fill this gap and propose a novel adversarial defense method called \textbackslash textit\{Synonym Encoding Method\} (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with little decay on the accuracy for benign examples. To better evaluate SEM, we also design a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic for synonym substitution based attacks. Compared with the first genetic based adversarial attack proposed in 2018, IGA can achieve higher attack success rate with lower word substitution rate, at the same time maintain the transferability of adversarial examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/ME2TYR5G/Wang et al. - 2020 - Natural Language Adversarial Attacks and Defenses .pdf;/Users/teetusaini/Zotero/storage/G2TQYJ57/1909.html}
}

@article{wang_natural_2020-1,
  title = {Natural {{Language Adversarial Attacks}} and {{Defenses}} in {{Word Level}}},
  author = {Wang, Xiaosen and Jin, Hao and He, Kun},
  year = {2020},
  month = apr,
  journal = {arXiv:1909.06723 [cs]},
  eprint = {1909.06723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, inspired by a mass of researches on adversarial examples for computer vision, there has been a growing interest in designing adversarial attacks for Natural Language Processing (NLP) tasks, followed by very few works of adversarial defenses for NLP. To our knowledge, there exists no defense method against the successful synonym substitution based attacks that aim to satisfy all the lexical, grammatical, semantic constraints and thus are hard to be perceived by humans. We contribute to fill this gap and propose a novel adversarial defense method called \textbackslash textit\{Synonym Encoding Method\} (SEM), which inserts an encoder before the input layer of the model and then trains the model to eliminate adversarial perturbations. Extensive experiments demonstrate that SEM can efficiently defend current best synonym substitution based adversarial attacks with little decay on the accuracy for benign examples. To better evaluate SEM, we also design a strong attack method called Improved Genetic Algorithm (IGA) that adopts the genetic metaheuristic for synonym substitution based attacks. Compared with the first genetic based adversarial attack proposed in 2018, IGA can achieve higher attack success rate with lower word substitution rate, at the same time maintain the transferability of adversarial examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/6I52DNUL/Wang et al. - 2020 - Natural Language Adversarial Attacks and Defenses .pdf;/Users/teetusaini/Zotero/storage/RUKARYWE/1909.html}
}

@article{wang_natural_2021,
  title = {Natural {{Language Adversarial Defense}} through {{Synonym Encoding}}},
  author = {Wang, Xiaosen and Jin, Hao and Yang, Yichen and He, Kun},
  year = {2021},
  month = jun,
  journal = {arXiv:1909.06723 [cs]},
  eprint = {1909.06723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/8RXRMH5N/Wang et al. - 2021 - Natural Language Adversarial Defense through Synon.pdf;/Users/teetusaini/Zotero/storage/J35J9MJ9/1909.html}
}

@article{wang_natural_2021-1,
  title = {Natural {{Language Adversarial Defense}} through {{Synonym Encoding}}},
  author = {Wang, Xiaosen and Jin, Hao and Yang, Yichen and He, Kun},
  year = {2021},
  month = jun,
  journal = {arXiv:1909.06723 [cs]},
  eprint = {1909.06723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/P6TVSKQX/Wang et al. - 2021 - Natural Language Adversarial Defense through Synon.pdf;/Users/teetusaini/Zotero/storage/BTP58EBV/1909.html}
}

@article{wang_structbert_2019,
  title = {{{StructBERT}}: {{Incorporating Language Structures}} into {{Pre-training}} for {{Deep Language Understanding}}},
  shorttitle = {{{StructBERT}}},
  author = {Wang, Wei and Bi, Bin and Yan, Ming and Wu, Chen and Bao, Zuyi and Xia, Jiangnan and Peng, Liwei and Si, Luo},
  year = {2019},
  month = sep,
  journal = {arXiv:1908.04577 [cs]},
  eprint = {1908.04577},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/YNBN3HYL/Wang et al. - 2019 - StructBERT Incorporating Language Structures into.pdf;/Users/teetusaini/Zotero/storage/NK9HBTPP/1908.html}
}

@article{wang_survey_2019,
  title = {A Survey on {{Adversarial Attacks}} and {{Defenses}} in {{Text}}},
  author = {Wang, Wenqi and Wang, Lina and Tang, Benxiao and Wang, Run and Ye, Aoshuang},
  year = {2019},
  month = feb,
  abstract = {Deep neural networks (DNNs) have shown an inherent vulnerability to adversarial examples which are maliciously crafted on real examples by attackers, aiming at making target DNNs misbehave. The threats of adversarial examples are widely existed in image, voice, speech, and text recognition and classification. Inspired by the previous work, researches on adversarial attacks and defenses in text domain develop rapidly. In order to make people have a general understanding about the field, this article presents a comprehensive review on adversarial examples in text. We analyze the advantages and shortcomings of recent adversarial examples generation methods and elaborate the efficiency and limitations on countermeasures. Finally, we discuss the challenges in adversarial texts and provide a research direction of this aspect.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/Q2VBU8LL/Wang et al. - 2019 - A survey on Adversarial Attacks and Defenses in Te.pdf;/Users/teetusaini/Zotero/storage/MYFQEL2L/1902.html}
}

@article{wang_textfirewall_2021,
  title = {{{TextFirewall}}: {{Omni-Defending Against Adversarial Texts}} in {{Sentiment Classification}}},
  shorttitle = {{{TextFirewall}}},
  author = {Wang, Wenqi and Wang, Run and Ke, Jianpeng and Wang, Lina},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {27467--27475},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3058278},
  abstract = {Sentiment classification has been broadly applied in real life, such as product recommendation and opinion-oriented analysis. Unfortunately, the widely employed sentiment classification systems based on deep neural networks (DNNs) are susceptible to adversarial attacks with imperceptible perturbations into the legitimate texts (also called adversarial texts). Adversarial texts could cause erroneous outputs even without access to the target model, bringing security concerns to systems deployed in safety-critical applications. However, studies on defending against adversarial texts are still in the early stage and not ready for tackling the emerging threats, especially in dealing with unknown attacks. Investigating the minor differences between adversarial texts and legitimate texts and enhancing the robustness of target models are two mainstream ideas for defending against adversarial texts. However, both of them suffer the generalization issue in dealing with unknown adversarial attacks. In this paper, we proposed a general method, called TextFirewall, for defending against adversarial texts crafted by various adversarial attacks, which shows the potential in identifying new developed adversarial attacks in the future. Given a piece of text, our TextFirewall identifies the adversarial text by investigating the inconsistency between the target model's output and the impact value calculated by important words in the text. TextFirewall could be deployed as a third-party tool without modifying the target model and agnostic to the specific type of adversarial texts. Experimental results demonstrate that our proposed TextFirewall effectively identifies adversarial texts generated by the three state-of-the-art (SOTA) attacks and outperforms previous defense techniques. Specifically, TextFirewall achieves an average accuracy of 90.7\% on IMDB and 96.9\% on Yelp in defending the three SOTA attacks.},
  keywords = {Adversarial texts,Genetic algorithms,Motion pictures,Perturbation methods,Robustness,Sentiment analysis,sentiment classification,sentiment polarity,Task analysis,Training,valuable words},
  file = {/Users/teetusaini/Zotero/storage/H3EKU2J9/Wang et al. - 2021 - TextFirewall Omni-Defending Against Adversarial T.pdf;/Users/teetusaini/Zotero/storage/SAIFIXX6/9350600.html}
}

@article{wang_towards_2021,
  title = {Towards a {{Robust Deep Neural Network}} in {{Texts}}: {{A Survey}}},
  shorttitle = {Towards a {{Robust Deep Neural Network}} in {{Texts}}},
  author = {Wang, Wenqi and Wang, Run and Wang, Lina and Wang, Zhibo and Ye, Aoshuang},
  year = {2021},
  month = apr,
  journal = {arXiv:1902.07285 [cs]},
  eprint = {1902.07285},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques. We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/P2NHSKAZ/Wang et al. - 2021 - Towards a Robust Deep Neural Network in Texts A S.pdf;/Users/teetusaini/Zotero/storage/PP3DSKQC/1902.html}
}

@article{wolf_huggingfaces_2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.03771 [cs]},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash url\{https://github.com/huggingface/transformers\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/Y3WVKMCG/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf;/Users/teetusaini/Zotero/storage/TDJ7DQMM/1910.html}
}

@article{wortsman_robust_2021,
  title = {Robust Fine-Tuning of Zero-Shot Models},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Li, Mike and Kim, Jong Wook and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01903 [cs]},
  eprint = {2109.01903},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large pre-trained models such as CLIP offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning approaches substantially improve accuracy in-distribution, they also reduce out-of-distribution robustness. We address this tension by introducing a simple and effective method for improving robustness: ensembling the weights of the zero-shot and fine-tuned models. Compared to standard fine-tuning, the resulting weight-space ensembles provide large accuracy improvements out-of-distribution, while matching or improving in-distribution accuracy. On ImageNet and five derived distribution shifts, weight-space ensembles improve out-of-distribution accuracy by 2 to 10 percentage points while increasing in-distribution accuracy by nearly 1 percentage point relative to standard fine-tuning. These improvements come at no additional computational cost during fine-tuning or inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/48SWLLIK/Wortsman et al. - 2021 - Robust fine-tuning of zero-shot models.pdf;/Users/teetusaini/Zotero/storage/JKKRQHGI/2109.html}
}

@article{wu_novel_2021,
  title = {A Novel Framework for Detecting Social Bots with Deep Neural Networks and Active Learning},
  author = {Wu, Yuhao and Fang, Yuzhou and Shang, Shuaikang and Jin, Jing and Wei, Lai and Wang, Haizhou},
  year = {2021},
  journal = {Knowl. Based Syst.},
  doi = {10.1016/j.knosys.2020.106525},
  abstract = {Abstract Microblogging is a popular online social network (OSN), which facilitates users to obtain and share news and information. Nevertheless, it is filled with a huge number of social bots that significantly disrupt the normal order of OSNs. Sina Weibo, one of the most popular Chinese OSNs in the world, is also seriously affected by social bots. With the growing development of social bots in Sina Weibo, they are increasingly indistinguishable from normal users, which presents more huge challenges in detecting social bots. Firstly, it is difficult to extract the features of social bots completely. Secondly, large-scale data collection and labeling of user data are extremely hard. Thirdly, the performance of classical classification approaches applied to social bot detection is not good enough. Therefore, this paper proposes a novel framework for detecting social bots in Sina Weibo based on deep neural networks and active learning (DABot). Specifically, 30 features from four categories, namely metadata-based, interaction-based, content-based, and timing-based are extracted to distinguish between social bots and normal users. Nine of these features are completely new features proposed in this paper. Moreover, active learning is employed to efficiently expand the labeled data. Then, a new deep neural network model called RGA is built to implement the detection of social bots, which makes use of a residual network (ResNet), a bidirectional gated recurrent unit (BiGRU), and an attention mechanism. After performance evaluation, the results show that DABot is more effective than the state-of-the-art baselines with the accuracy of 0.9887.}
}

@article{xiong_towards_2021,
  title = {Towards a {{Robust}} and {{Trustworthy Machine Learning System Development}}},
  author = {Xiong, Pulei and Buffett, Scott and Iqbal, Shahrear and Lamontagne, Philippe and Mamun, Mohammad and Molyneaux, Heather},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03042 [cs]},
  eprint = {2101.03042},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine Learning (ML) technologies have been widely adopted in many mission critical fields, such as cyber security, autonomous vehicle control, healthcare, etc. to support intelligent decision-making. While ML has demonstrated impressive performance over conventional methods in these applications, concerns arose with respect to system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, firstly we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness technologies from a security engineering perspective, which covers all aspects of secure ML system development including threat modeling, common offensive and defensive technologies, privacy-preserving machine learning, user trust in the context of machine learning, and empirical evaluation for ML model robustness. Secondly, we then push our studies forward above and beyond a survey by describing a metamodel we created that represents the body of knowledge in a standard and visualized way for ML practitioners. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process in a context of generic ML system development, which extends and scales up the classic process. Thirdly, we propose future research directions motivated by our findings to advance the development of robust and trustworthy ML systems. Our work differs from existing surveys in this area in that, to the best of our knowledge, it is the first of its kind of engineering effort to (i) explore the fundamental principles and best practices to support robust and trustworthy ML system development; and (ii) study the interplay of robustness and user trust in the context of ML systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,D.2,H.1,I.2},
  file = {/Users/teetusaini/Zotero/storage/I9F999K6/Xiong et al. - 2021 - Towards a Robust and Trustworthy Machine Learning .pdf;/Users/teetusaini/Zotero/storage/NMGBQLW3/2101.html}
}

@inproceedings{xu_lexicalat_2019,
  title = {{{LexicalAT}}: {{Lexical-Based Adversarial Reinforcement Training}} for {{Robust Sentiment Classification}}},
  shorttitle = {{{LexicalAT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Xu, Jingjing and Zhao, Liang and Yan, Hanqi and Zeng, Qi and Liang, Yun and Sun, Xu},
  year = {2019},
  month = nov,
  pages = {5518--5527},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1554},
  abstract = {Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.},
  file = {/Users/teetusaini/Zotero/storage/ETXQ8E5Z/Xu et al. - 2019 - LexicalAT Lexical-Based Adversarial Reinforcement.pdf}
}

@article{yang_greedy_nodate,
  title = {Greedy {{Attack}} and {{Gumbel Attack}}: {{Generating Adversarial Examples}} for {{Discrete Data}}},
  author = {Yang, Puyudi and Chen, Jianbo and Hsieh, Cho-Jui and Wang, Jane-Ling and Jordan, Michael I},
  pages = {36},
  abstract = {We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various stateof-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/FCP4T3W9/Yang et al. - Greedy Attack and Gumbel Attack Generating Advers.pdf}
}

@article{yang_xlnet_2020,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  year = {2020},
  month = jan,
  journal = {arXiv:1906.08237 [cs]},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/ISS323V8/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf;/Users/teetusaini/Zotero/storage/RHNQZGPW/1906.html}
}

@inproceedings{yin_robustness_2020,
  title = {On the {{Robustness}} of {{Language Encoders}} against {{Grammatical Errors}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yin, Fan and Long, Quanyu and Meng, Tao and Chang, Kai-Wei},
  year = {2020},
  month = jul,
  pages = {3386--3403},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.310},
  abstract = {We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.},
  file = {/Users/teetusaini/Zotero/storage/ZDENM6X5/Yin et al. - 2020 - On the Robustness of Language Encoders against Gra.pdf}
}

@article{yuan_adversarial_2018,
  title = {Adversarial {{Examples}}: {{Attacks}} and {{Defenses}} for {{Deep Learning}}},
  shorttitle = {Adversarial {{Examples}}},
  author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
  year = {2018},
  month = jul,
  journal = {arXiv:1712.07107 [cs, stat]},
  eprint = {1712.07107},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/WH2XII8L/Yuan et al. - 2018 - Adversarial Examples Attacks and Defenses for Dee.pdf;/Users/teetusaini/Zotero/storage/8XZTLDVI/1712.html}
}

@article{zang_learning_2020,
  title = {Learning to {{Attack}}: {{Towards Textual Adversarial Attacking}} in {{Real-world Situations}}},
  shorttitle = {Learning to {{Attack}}},
  author = {Zang, Yuan and Hou, Bairu and Qi, Fanchao and Liu, Zhiyuan and Meng, Xiaojun and Sun, Maosong},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.09192 [cs]},
  eprint = {2009.09192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/Users/teetusaini/Zotero/storage/99N5JB8S/Zang et al. - 2020 - Learning to Attack Towards Textual Adversarial At.pdf;/Users/teetusaini/Zotero/storage/MGU5E88K/2009.html}
}

@article{zang_word-level_2019,
  title = {Word-Level {{Textual Adversarial Attacking}} as {{Combinatorial Optimization}}},
  author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
  year = {2019},
  month = oct,
  doi = {10.18653/v1/2020.acl-main.540},
  abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/2VYWRSDW/Zang et al. - 2019 - Word-level Textual Adversarial Attacking as Combin.pdf;/Users/teetusaini/Zotero/storage/WG9GYFZW/1910.html}
}

@article{zeng_openattack_2020,
  title = {{{OpenAttack}}: {{An Open-source Textual Adversarial Attack Toolkit}}},
  shorttitle = {{{OpenAttack}}},
  author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.09191 [cs]},
  eprint = {2009.09191},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and apt comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack. It currently builds in 12 typical attack models that cover all the attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a victim model, assisting in developing new attack models, and adversarial training. Source code, built-in models and documentation can be obtained at https://github.com/thunlp/OpenAttack.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/Users/teetusaini/Zotero/storage/5ZUEU54S/Zeng et al. - 2020 - OpenAttack An Open-source Textual Adversarial Att.pdf;/Users/teetusaini/Zotero/storage/BIWTTJC5/2009.html}
}

@article{zhang_adversarial_2019,
  title = {Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}},
  author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
  year = {2019},
  month = apr,
  journal = {arXiv:1901.06796 [cs]},
  eprint = {1901.06796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/MQIWCTZQ/Zhang et al. - 2019 - Adversarial Attacks on Deep Learning Models in Nat.pdf;/Users/teetusaini/Zotero/storage/MVREFBFT/1901.html}
}

@article{zhang_character-level_2016,
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  year = {2016},
  month = apr,
  journal = {arXiv:1509.01626 [cs]},
  eprint = {1509.01626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/A2AJSS8X/Zhang et al. - 2016 - Character-level Convolutional Networks for Text Cl.pdf;/Users/teetusaini/Zotero/storage/DUCF75RD/1509.html}
}

@article{zhang_generating_2020,
  title = {Generating {{Fluent Adversarial Examples}} for {{Natural Languages}}},
  author = {Zhang, Huangzhao and Zhou, Hao and Miao, Ning and Li, Lei},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.06174 [cs]},
  eprint = {2007.06174},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms the baseline model on attacking capability. Adversarial training with MAH also leads to better robustness and performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/WXR4EEMZ/Zhang et al. - 2020 - Generating Fluent Adversarial Examples for Natural.pdf;/Users/teetusaini/Zotero/storage/2WLPTGUH/2007.html}
}

@inproceedings{zhang_paws_2019,
  title = {{{PAWS}}: {{Paraphrase Adversaries}} from {{Word Scrambling}}},
  shorttitle = {{{PAWS}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  year = {2019},
  month = jun,
  pages = {1298--1308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1131},
  abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (\textbackslash textless40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
  file = {/Users/teetusaini/Zotero/storage/KKXRKSFV/Zhang et al. - 2019 - PAWS Paraphrase Adversaries from Word Scrambling.pdf}
}

@inproceedings{zhang_paws_2019-1,
  title = {{{PAWS}}: {{Paraphrase Adversaries}} from {{Word Scrambling}}},
  shorttitle = {{{PAWS}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  year = {2019},
  month = jun,
  pages = {1298--1308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1131},
  abstract = {Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (\textbackslash textless40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
  file = {/Users/teetusaini/Zotero/storage/9845A3YK/Zhang et al. - 2019 - PAWS Paraphrase Adversaries from Word Scrambling.pdf}
}

@article{zhang_revisiting_2021,
  title = {Revisiting {{Few-sample BERT Fine-tuning}}},
  author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2021},
  month = mar,
  journal = {arXiv:2006.05987 [cs]},
  eprint = {2006.05987},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/MUZ83PHM/Zhang et al. - 2021 - Revisiting Few-sample BERT Fine-tuning.pdf;/Users/teetusaini/Zotero/storage/63J5XAQY/2006.html}
}

@inproceedings{zheng_evaluating_2020,
  title = {Evaluating and {{Enhancing}} the {{Robustness}} of {{Neural Network-based Dependency Parsing Models}} with {{Adversarial Examples}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zheng, Xiaoqing and Zeng, Jiehang and Zhou, Yi and Hsieh, Cho-Jui and Cheng, Minhao and Huang, Xuanjing},
  year = {2020},
  month = jul,
  pages = {6600--6610},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.590},
  abstract = {Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77\% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.},
  file = {/Users/teetusaini/Zotero/storage/YXJZ4E2M/Zheng et al. - 2020 - Evaluating and Enhancing the Robustness of Neural .pdf}
}

@article{zhou_defense_2020,
  title = {Defense against {{Adversarial Attacks}} in {{NLP}} via {{Dirichlet Neighborhood Ensemble}}},
  author = {Zhou, Yi and Zheng, Xiaoqing and Hsieh, Cho-Jui and Chang, Kai-wei and Huang, Xuanjing},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.11627 [cs]},
  eprint = {2006.11627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Despite neural networks have achieved prominent performance on many natural language processing (NLP) tasks, they are vulnerable to adversarial examples. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to defense substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models for NLP applications. We demonstrate through extensive experimentation that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/teetusaini/Zotero/storage/Z7AUENF5/Zhou et al. - 2020 - Defense against Adversarial Attacks in NLP via Dir.pdf;/Users/teetusaini/Zotero/storage/ILBBVL3H/2006.html}
}

@article{zhou_fake_2019,
  title = {Fake {{News Detection}} via {{NLP}} Is {{Vulnerable}} to {{Adversarial Attacks}}},
  author = {Zhou, Zhixuan and Guan, Huankang and Bhat, Meghana Moorthy and Hsu, Justin},
  year = {2019},
  journal = {Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
  eprint = {1901.09657},
  eprinttype = {arxiv},
  pages = {794--800},
  doi = {10.5220/0007566307940800},
  abstract = {News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/teetusaini/Zotero/storage/WJSEKDHH/Zhou et al. - 2019 - Fake News Detection via NLP is Vulnerable to Adver.pdf;/Users/teetusaini/Zotero/storage/UWJ6Z4RD/1901.html}
}

@article{zhou_learning_2019,
  title = {Learning to {{Discriminate Perturbations}} for {{Blocking Adversarial Attacks}} in {{Text Classification}}},
  author = {Zhou, Yichao and Jiang, Jyun-Yu and Chang, Kai-Wei and Wang, Wei},
  year = {2019},
  month = sep,
  abstract = {Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis. In this paper, we propose a novel framework, learning to DIScriminate Perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations. For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.},
  langid = {english},
  file = {/Users/teetusaini/Zotero/storage/DWTX4D95/Zhou et al. - 2019 - Learning to Discriminate Perturbations for Blockin.pdf;/Users/teetusaini/Zotero/storage/5U69NUPT/1909.html}
}

@article{zhu_at-bert_2021,
  title = {{{AT-BERT}}: {{Adversarial Training BERT}} for {{Acronym Identification Winning Solution}} for {{SDU}}@{{AAAI-21}}},
  shorttitle = {{{AT-BERT}}},
  author = {Zhu, Danqing and Lin, Wangli and Zhang, Yang and Zhong, Qiwei and Zeng, Guanxiong and Wu, Weilin and Tang, Jiayu},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03700 [cs]},
  eprint = {2101.03700},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Acronym identification focuses on finding the acronyms and the phrases that have been abbreviated, which is crucial for scientific document understanding tasks. However, the limited size of manually annotated datasets hinders further improvement for the problem. Recent breakthroughs of language models pre-trained on large corpora clearly show that unsupervised pre-training can vastly improve the performance of downstream tasks. In this paper, we present an Adversarial Training BERT method named AT-BERT, our winning solution to acronym identification task for Scientific Document Understanding (SDU) Challenge of AAAI 2021. Specifically, the pre-trained BERT is adopted to capture better semantic representation. Then we incorporate the FGM adversarial training strategy into the fine-tuning of BERT, which makes the model more robust and generalized. Furthermore, an ensemble mechanism is devised to involve the representations learned from multiple BERT variants. Assembling all these components together, the experimental results on the SciAI dataset show that our proposed approach outperforms all other competitive state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/teetusaini/Zotero/storage/FJ4YLD9I/Zhu et al. - 2021 - AT-BERT Adversarial Training BERT for Acronym Ide.pdf;/Users/teetusaini/Zotero/storage/I3B3CFFX/2101.html}
}


